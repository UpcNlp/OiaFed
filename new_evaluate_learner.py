    def evaluate_learner(self, learner_id: str, learner: Any) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªå­¦ä¹ å™¨
        
        Args:
            learner_id: å­¦ä¹ å™¨ID
            learner: å­¦ä¹ å™¨å®ä¾‹
            
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        try:
            if not self.evaluation_engine:
                self.logger.warning("è¯„ä¼°å¼•æ“æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯„ä¼°")
                return {}
            
            evaluation_config = self.config_manager.get_evaluation_config()
            self.logger.debug(f"è¯„ä¼°é…ç½®ç»“æ„: {evaluation_config}")
            
            # æ”¯æŒæ–°æ ¼å¼: evaluation.tasks åˆ—è¡¨
            evaluation_tasks = []
            
            if "tasks" in evaluation_config:
                # è¿‡æ»¤å‡ºå½“å‰learnerçš„ä»»åŠ¡
                all_tasks = evaluation_config["tasks"]
                learner_tasks = [task for task in all_tasks if task.get("learner") == learner_id]
                
                if learner_tasks:
                    self.logger.debug(f"æ‰¾åˆ° {len(learner_tasks)} ä¸ªé’ˆå¯¹learner {learner_id} çš„è¯„ä¼°ä»»åŠ¡")
                    for task in learner_tasks:
                        evaluation_tasks.append({
                            "evaluator": task.get("evaluator"),
                            "test_dataset": task.get("test_data"),  # æ³¨æ„å­—æ®µåæ˜ å°„
                            "name": f"{task.get('evaluator')}_{task.get('test_data')}"
                        })
                else:
                    self.logger.debug(f"åœ¨tasksåˆ—è¡¨ä¸­æœªæ‰¾åˆ°é’ˆå¯¹learner {learner_id} çš„è¯„ä¼°ä»»åŠ¡")
            
            if not evaluation_tasks:
                self.logger.debug(f"æ²¡æœ‰ä¸ºlearner {learner_id} æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ä»»åŠ¡")
                return {}
                
            results = {}
            
            # æ‰§è¡Œè¯„ä¼°ä»»åŠ¡
            for task in evaluation_tasks:
                evaluator_id = task["evaluator"]
                test_dataset_id = task["test_dataset"]
                task_name = task.get("name", f"{evaluator_id}_{test_dataset_id}")
                
                self.logger.debug(f"æ‰§è¡Œè¯„ä¼°ä»»åŠ¡: {task_name}")
                
                if evaluator_id not in self.evaluators:
                    self.logger.warning(f"è¯„ä¼°å™¨ {evaluator_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä»»åŠ¡ {task_name}")
                    continue
                    
                if test_dataset_id not in self.test_dataloaders:
                    self.logger.warning(f"æµ‹è¯•æ•°æ®é›† {test_dataset_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä»»åŠ¡ {task_name}")
                    continue
                
                evaluator = self.evaluators[evaluator_id]
                test_dataloader = self.test_dataloaders[test_dataset_id]
                
                try:
                    # æ‰§è¡Œè¯„ä¼°
                    eval_result = evaluator.evaluate(
                        model=learner.get_model() if hasattr(learner, 'get_model') else learner,
                        dataloader=test_dataloader,
                        learner_id=learner_id
                    )
                    
                    # ä¿å­˜ç»“æœ
                    results[task_name] = eval_result
                    
                    self.logger.info(f"âœ… è¯„ä¼°ä»»åŠ¡å®Œæˆ: {task_name} - {eval_result}")
                    
                except Exception as e:
                    self.logger.error(f"è¯„ä¼°ä»»åŠ¡ {task_name} æ‰§è¡Œå¤±è´¥: {e}")
                    results[task_name] = {"error": str(e)}
            
            if results:
                self.logger.info(f"ğŸ¯ learner {learner_id} è¯„ä¼°å®Œæˆï¼Œå…±æ‰§è¡Œ {len(results)} ä¸ªä»»åŠ¡")
            return results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°learner {learner_id} å¤±è´¥: {e}")
            return {}
