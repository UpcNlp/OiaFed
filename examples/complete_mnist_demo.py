"""
å®Œæ•´çš„MNISTè”é‚¦å­¦ä¹ æ¼”ç¤º - çœŸå®è®­ç»ƒç‰ˆæœ¬ï¼ˆç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼‰
ä½¿ç”¨æ–°æ¶æ„å®ç°çœŸå®çš„MNISTè”é‚¦å­¦ä¹ è®­ç»ƒ
examples/complete_mnist_demo.py

ä½¿ç”¨ç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼š
- æ‰€æœ‰ç»„ä»¶ï¼ˆDataset, Model, Aggregatorï¼‰åœ¨Trainer/Learnerå†…éƒ¨åˆå§‹åŒ–
- æ”¯æŒå»¶è¿ŸåŠ è½½ï¼ˆlazy_init=Trueï¼‰
- ComponentBuilder.parse_config() è§£æé…ç½®ï¼Œè¿”å›ç±»å¼•ç”¨å’Œå‚æ•°
- é…ç½®æ ¼å¼ï¼štraining: {trainer: {name: ..., params: ...}}
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import torchvision
import torchvision.transforms as transforms

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from fedcl.learner.base_learner import BaseLearner
from fedcl.trainer.trainer import BaseTrainer
from fedcl.types import TrainingRequest, TrainingResponse
from fedcl import FederatedLearning

# å¯¼å…¥æ–°å®ç°çš„æ•°æ®é›†å’Œæ¨¡å‹ç®¡ç†
from fedcl.api.decorators import dataset, model
from fedcl.api.registry import registry
from fedcl.methods.datasets.base import FederatedDataset
from fedcl.methods.models.base import FederatedModel

# å¯¼å…¥è£…é¥°å™¨
from fedcl.api import learner, trainer

# ==================== 1. æ³¨å†ŒçœŸå®çš„MNISTæ•°æ®é›† ====================

@dataset(
    name='MNIST',
    description='MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†',
    dataset_type='image_classification',
    num_classes=10
)
class MNISTFederatedDataset(FederatedDataset):
    """MNISTè”é‚¦æ•°æ®é›†å®ç°"""

    def __init__(self, root: str = './data', train: bool = True, download: bool = True):
        super().__init__(root, train, download)

        # æ•°æ®è½¬æ¢
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        # åŠ è½½MNISTæ•°æ®é›†
        self.dataset = torchvision.datasets.MNIST(
            root=root,
            train=train,
            download=download,
            transform=transform
        )

        # è®¾ç½®å±æ€§
        self.num_classes = 10
        self.input_shape = (1, 28, 28)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'dataset_name': 'MNIST',
            'num_samples': len(self.dataset),
            'num_classes': self.num_classes,
            'input_shape': self.input_shape,
            'train': self.train,
        }

# ==================== 2. æ³¨å†ŒçœŸå®çš„CNNæ¨¡å‹ ====================

@model(
    name='MNIST_CNN',
    description='MNIST CNNåˆ†ç±»æ¨¡å‹',
    task='classification',
    input_shape=(1, 28, 28),
    output_shape=(10,)
)
class MNISTCNNModel(FederatedModel):
    """MNIST CNNæ¨¡å‹"""

    def __init__(self, num_classes: int = 10):
        super().__init__()

        # è®¾ç½®å…ƒæ•°æ®
        self.set_metadata(
            task_type='classification',
            input_shape=(1, 28, 28),
            output_shape=(num_classes,)
        )

        # å®šä¹‰ç½‘ç»œç»“æ„
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(64 * 14 * 14, 128)
        self.fc2 = nn.Linear(128, num_classes)

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = self.dropout1(x)
        x = x.view(-1, 64 * 14 * 14)
        x = torch.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

    def get_weights_as_dict(self) -> Dict[str, torch.Tensor]:
        """è·å–æ¨¡å‹æƒé‡"""
        return {k: v.cpu().clone() for k, v in self.state_dict().items()}

    def set_weights_from_dict(self, weights: Dict[str, torch.Tensor], strict: bool = True):
        """è®¾ç½®æ¨¡å‹æƒé‡"""
        self.load_state_dict(weights)

    def get_param_count(self) -> int:
        """è·å–å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters())

# ==================== 3. å®ç°çœŸå®çš„Learner ====================

@learner('MNISTLearner',
         description='MNISTæ•°æ®é›†å­¦ä¹ å™¨',
         version='1.0',
         author='MOE-FedCL',
         dataset='MNIST')
class MNISTLearner(BaseLearner):
    """MNISTå­¦ä¹ å™¨ - å®ç°çœŸå®çš„è®­ç»ƒï¼ˆä½¿ç”¨ç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼‰"""

    def __init__(self, client_id: str, config: Dict[str, Any] = None, lazy_init: bool = True):
        """åˆå§‹åŒ–MNISTå­¦ä¹ å™¨

        Args:
            client_id: å®¢æˆ·ç«¯ID
            config: é…ç½®å­—å…¸ï¼ˆç”±ComponentBuilder.parse_config()ç”Ÿæˆï¼‰
            lazy_init: æ˜¯å¦å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶
        """
        super().__init__(client_id, config, lazy_init)

        # æå–è®­ç»ƒå‚æ•°ï¼ˆä»config.learner.paramsï¼‰
        if not hasattr(self, 'learning_rate'):
            self.learning_rate = 0.01
        if not hasattr(self, 'batch_size'):
            self.batch_size = 32
        if not hasattr(self, 'local_epochs'):
            self.local_epochs = 1

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç»„ä»¶å ä½ç¬¦ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._model = None
        self._optimizer = None
        self._criterion = None
        self._train_loader = None

        self.logger.info(f"MNISTLearner {client_id} åˆå§‹åŒ–å®Œæˆ (lazy_init={lazy_init})")

    def _create_default_dataset(self):
        """åˆ›å»ºé»˜è®¤æ•°æ®é›†ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        self.logger.info(f"Client {self.client_id}: åŠ è½½MNISTæ•°æ®é›†...")
        return self._load_dataset()

    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        # ä»æ³¨å†Œè¡¨è·å–MNISTæ•°æ®é›†
        mnist_dataset_cls = registry.get_dataset('MNIST')
        mnist_dataset = mnist_dataset_cls(root='./data', train=True, download=True)

        # è·å–åº•å±‚çš„ PyTorch Dataset
        base_dataset = mnist_dataset.dataset  # torchvision.datasets.MNIST

        # ç®€å•çš„IIDåˆ’åˆ†ï¼ˆæ‰‹åŠ¨åˆ’åˆ†ï¼‰
        num_clients = 3
        client_idx = int(self.client_id.split('_')[1])

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®èŒƒå›´
        total_size = len(base_dataset)
        samples_per_client = total_size // num_clients
        start_idx = client_idx * samples_per_client
        end_idx = start_idx + samples_per_client if client_idx < num_clients - 1 else total_size

        # åˆ›å»ºç´¢å¼•åˆ—è¡¨
        indices = list(range(start_idx, end_idx))

        # åˆ›å»º Subset
        train_dataset = Subset(base_dataset, indices)

        self.logger.info(f"Client {self.client_id}: æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°={len(train_dataset)}")
        return train_dataset

    @property
    def model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model is None:
            self._model = MNISTCNNModel(num_classes=10).to(self.device)
            self.logger.debug(f"Client {self.client_id}: æ¨¡å‹åˆ›å»ºå®Œæˆ")
        return self._model

    @property
    def optimizer(self):
        """å»¶è¿ŸåŠ è½½ä¼˜åŒ–å™¨"""
        if self._optimizer is None:
            self._optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)
            self.logger.debug(f"Client {self.client_id}: ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
        return self._optimizer

    @property
    def criterion(self):
        """å»¶è¿ŸåŠ è½½æŸå¤±å‡½æ•°"""
        if self._criterion is None:
            self._criterion = nn.CrossEntropyLoss()
        return self._criterion

    @property
    def train_loader(self):
        """å»¶è¿ŸåŠ è½½æ•°æ®åŠ è½½å™¨"""
        if self._train_loader is None:
            # è§¦å‘æ•°æ®é›†åŠ è½½
            dataset = self.dataset

            # æ‰“å°æ•°æ®é›†ç±»å‹ä»¥éªŒè¯
            dataset_type = type(dataset).__name__
            self.logger.info(f"Client {self.client_id}: æ£€æµ‹åˆ°æ•°æ®é›†ç±»å‹ = {dataset_type}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯ FederatedDatasetï¼ˆéœ€è¦è¿›ä¸€æ­¥å¤„ç†ï¼‰
            if hasattr(dataset, 'dataset'):
                # è¿™æ˜¯ä¸€ä¸ª FederatedDataset åŒ…è£…å™¨ï¼ˆå¦‚ MNISTFederatedDatasetï¼‰ï¼Œéœ€è¦è·å–å®é™…çš„æ•°æ®
                self.logger.info(f"Client {self.client_id}: ä½¿ç”¨ {dataset_type}ï¼Œä»ä¸­æå–åº•å±‚æ•°æ®é›†")

                # è·å–åº•å±‚çš„ PyTorch Dataset
                base_dataset = dataset.dataset  # torchvision.datasets.MNIST
                self.logger.debug(f"Client {self.client_id}: åº•å±‚æ•°æ®é›†ç±»å‹ = {type(base_dataset).__name__}")

                # ç®€å•çš„IIDåˆ’åˆ†
                num_clients = 3
                client_idx = int(self.client_id.split('_')[1])

                total_size = len(base_dataset)
                samples_per_client = total_size // num_clients
                start_idx = client_idx * samples_per_client
                end_idx = start_idx + samples_per_client if client_idx < num_clients - 1 else total_size

                indices = list(range(start_idx, end_idx))
                actual_dataset = Subset(base_dataset, indices)

                self.logger.info(f"Client {self.client_id}: ä» {dataset_type} åŠ è½½æ•°æ®é›†ï¼Œæ ·æœ¬æ•°={len(actual_dataset)}")
            else:
                # å·²ç»æ˜¯æ ‡å‡†çš„ PyTorch Datasetï¼ˆä» _create_default_dataset è¿”å›ï¼‰
                self.logger.info(f"Client {self.client_id}: ä½¿ç”¨æ ‡å‡† PyTorch Dataset")
                actual_dataset = dataset

            self._train_loader = DataLoader(
                actual_dataset,
                batch_size=self.batch_size,
                shuffle=True
            )
            self.logger.debug(f"Client {self.client_id}: æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        return self._train_loader

    async def train(self, params: Dict[str, Any]) -> TrainingResponse:
        """è®­ç»ƒæ–¹æ³•"""
        num_epochs = params.get("num_epochs", self.local_epochs)
        round_number = params.get("round_number", 1)

        self.logger.info(f"  [{self.client_id}] Round {round_number}, Training {num_epochs} epochs...")

        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        self.model.train()

        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for epoch in range(num_epochs):
            epoch_loss = 0.0
            epoch_correct = 0
            epoch_samples = 0

            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)

                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                epoch_correct += pred.eq(target.view_as(pred)).sum().item()
                epoch_samples += data.size(0)

            avg_epoch_loss = epoch_loss / epoch_samples
            epoch_accuracy = epoch_correct / epoch_samples
            self.logger.info(f"    [{self.client_id}] Epoch {epoch+1}: Loss={avg_epoch_loss:.4f}, Acc={epoch_accuracy:.4f}")

            total_loss += epoch_loss
            correct_predictions += epoch_correct
            total_samples += epoch_samples

        # è®¡ç®—å¹³å‡å€¼
        avg_loss = total_loss / total_samples
        accuracy = correct_predictions / total_samples

        # è·å–æ¨¡å‹æƒé‡ï¼ˆç›´æ¥è¿”å›torch.Tensorï¼Œåº•å±‚ä¼šè‡ªåŠ¨è½¬æ¢ï¼‰
        model_weights = self.model.get_weights_as_dict()

        # åˆ›å»ºè®­ç»ƒå“åº”
        response = TrainingResponse(
            request_id="",  # ä¼šè¢«stubå¡«å……
            client_id=self.client_id,
            success=True,
            result={
                "epochs_completed": num_epochs,
                "loss": avg_loss,
                "accuracy": accuracy,
                "samples_used": total_samples,
                "model_weights": model_weights  # æ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–tensor
            },
            execution_time=0.0
        )

        self.logger.info(f"  [{self.client_id}] Round {round_number} completed: Loss={avg_loss:.4f}, Acc={accuracy:.4f}")
        return response

    async def evaluate(self, model_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ–¹æ³•"""
        self.model.eval()

        # å¦‚æœæä¾›äº†æ¨¡å‹æƒé‡ï¼Œå…ˆæ›´æ–°æ¨¡å‹
        if model_data and "model_weights" in model_data:
            weights = model_data["model_weights"]
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorch tensor
            torch_weights = {}
            for k, v in weights.items():
                if isinstance(v, np.ndarray):
                    torch_weights[k] = torch.from_numpy(v)
                else:
                    torch_weights[k] = v
            self.model.set_weights_from_dict(torch_weights)

        test_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in self.train_loader:  # ä½¿ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºè¯„ä¼°æ•°æ®
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += data.size(0)

        return {
            "accuracy": correct / total,
            "loss": test_loss / total,
            "samples": total
        }

    async def get_model(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ•°æ®

        ç›´æ¥è¿”å›torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–
        """
        # è·å–æ•°æ®é›†ï¼ˆè§¦å‘å»¶è¿ŸåŠ è½½ï¼‰
        dataset = self.dataset
        return {
            "model_type": "mnist_cnn",
            "parameters": {"weights": self.model.get_weights_as_dict()},
            "metadata": {
                "client_id": self.client_id,
                "samples": len(dataset),
                "param_count": self.model.get_param_count()
            }
        }

    async def set_model(self, model_data: Dict[str, Any]) -> bool:
        """è®¾ç½®æ¨¡å‹æ•°æ®

        æ¥å—torch.Tensoræˆ–numpyæ•°ç»„ï¼Œè‡ªåŠ¨è½¬æ¢
        """
        try:
            if "parameters" in model_data and "weights" in model_data["parameters"]:
                weights = model_data["parameters"]["weights"]
                # æ™ºèƒ½è½¬æ¢ï¼šæ”¯æŒnumpyæ•°ç»„ã€torch.Tensorå’Œdict
                torch_weights = {}
                for k, v in weights.items():
                    if isinstance(v, np.ndarray):
                        torch_weights[k] = torch.from_numpy(v)
                    elif torch.is_tensor(v):
                        torch_weights[k] = v
                    else:
                        torch_weights[k] = v
                self.model.set_weights_from_dict(torch_weights)
                return True
        except Exception as e:
            self.logger.exception(f"  [{self.client_id}] Failed to set model: {e}")
        return False

    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        # è·å–æ•°æ®é›†ï¼ˆè§¦å‘å»¶è¿ŸåŠ è½½ï¼‰
        dataset = self.dataset
        return {
            "total_samples": len(dataset),
            "num_classes": 10,
            "feature_dim": 784,
            "input_shape": (1, 28, 28)
        }

    async def get_local_model(self) -> Dict[str, Any]:
        return await self.get_model()

    async def set_local_model(self, model_data: Dict[str, Any]) -> bool:
        return await self.set_model(model_data)

# ==================== 4. å®ç°çœŸå®çš„Trainer ====================

@trainer('FedAvgMNIST',
         description='MNISTè”é‚¦å¹³å‡è®­ç»ƒå™¨',
         version='1.0',
         author='MOE-FedCL',
         algorithms=['fedavg'])
class FedAvgMNISTTrainer(BaseTrainer):
    """è”é‚¦å¹³å‡è®­ç»ƒå™¨ - å®ç°çœŸå®çš„æ¨¡å‹èšåˆï¼ˆä½¿ç”¨ç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼‰"""

    def __init__(self, config: Dict[str, Any] = None, lazy_init: bool = True, logger=None):
        """åˆå§‹åŒ–FedAvgMNISTè®­ç»ƒå™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ˆç”±ComponentBuilder.parse_config()ç”Ÿæˆï¼‰
            lazy_init: æ˜¯å¦å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶
            logger: æ—¥å¿—è®°å½•å™¨
        """
        super().__init__(config, lazy_init, logger)

        # æå–è®­ç»ƒå‚æ•°ï¼ˆä»config.trainer.paramsï¼‰
        if not hasattr(self, 'local_epochs'):
            self.local_epochs = 1
        if not hasattr(self, 'learning_rate'):
            self.learning_rate = 0.01
        if not hasattr(self, 'batch_size'):
            self.batch_size = 32

        # ç»„ä»¶å ä½ç¬¦ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._global_model_obj = None

        self.logger.info("FedAvgMNISTTraineråˆå§‹åŒ–å®Œæˆ")

    def _create_default_global_model(self):
        """åˆ›å»ºé»˜è®¤å…¨å±€æ¨¡å‹"""
        self.logger.info("åˆ›å»ºé»˜è®¤MNIST CNNå…¨å±€æ¨¡å‹")
        model = MNISTCNNModel(num_classes=10)
        return {
            "model_type": "mnist_cnn",
            "parameters": {"weights": model.get_weights_as_dict()},
            "model_obj": model  # ä¿å­˜æ¨¡å‹å¯¹è±¡ä»¥ä¾¿åç»­ä½¿ç”¨
        }

    @property
    def global_model_obj(self):
        """å»¶è¿ŸåŠ è½½å…¨å±€æ¨¡å‹å¯¹è±¡"""
        if self._global_model_obj is None:
            # è§¦å‘å…¨å±€æ¨¡å‹åŠ è½½
            global_model_data = self.global_model
            if isinstance(global_model_data, dict) and "model_obj" in global_model_data:
                self._global_model_obj = global_model_data["model_obj"]
            else:
                # å¦‚æœæ²¡æœ‰æ¨¡å‹å¯¹è±¡ï¼Œåˆ›å»ºæ–°çš„
                self._global_model_obj = MNISTCNNModel(num_classes=10)
                if isinstance(global_model_data, dict) and "parameters" in global_model_data:
                    weights = global_model_data["parameters"].get("weights", {})
                    torch_weights = {}
                    for k, v in weights.items():
                        if isinstance(v, np.ndarray):
                            torch_weights[k] = torch.from_numpy(v)
                        else:
                            torch_weights[k] = v
                    self._global_model_obj.set_weights_from_dict(torch_weights)
            self.logger.debug(f"å…¨å±€æ¨¡å‹å¯¹è±¡åˆ›å»ºå®Œæˆï¼Œå‚æ•°æ•°é‡: {self._global_model_obj.get_param_count():,}")
        return self._global_model_obj

    async def train_round(self, round_num: int, client_ids: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€è½®è”é‚¦è®­ç»ƒ"""
        self.logger.info(f"\n--- Round {round_num} ---")
        self.logger.info(f"  Selected clients: {client_ids}")

        # åˆ›å»ºè®­ç»ƒè¯·æ±‚
        training_params = {
            "round_number": round_num,
            "num_epochs": 1,
            "batch_size": 32,
            "learning_rate": 0.01
        }

        # å¹¶è¡Œè®­ç»ƒæ‰€æœ‰å®¢æˆ·ç«¯
        tasks = []
        for client_id in client_ids:
            if self.is_client_ready(client_id):
                proxy = self._proxy_manager.get_proxy(client_id)
                if proxy:
                    self.logger.info(f"  [{client_id}] Starting training...")
                    task = proxy.train(training_params)
                    tasks.append((client_id, task))

        # æ”¶é›†ç»“æœ
        client_results = {}
        failed_clients = []

        for client_id, task in tasks:
            try:
                result = await task
                if result.success:
                    client_results[client_id] = result
                    self.logger.info(f"  [{client_id}] Training succeeded: Loss={result.result['loss']:.4f}, Acc={result.result['accuracy']:.4f}")
                else:
                    self.logger.error(f"  [{client_id}] Training failed: {result}")
                    failed_clients.append(client_id)
            except Exception as e:
                self.logger.exception(f"  [{client_id}] Training failed: {e}")
                failed_clients.append(client_id)

        # èšåˆæ¨¡å‹
        aggregated_weights = None
        if client_results:
            aggregated_weights = await self.aggregate_models(client_results)
            if aggregated_weights:
                self.global_model_obj.set_weights_from_dict(aggregated_weights)

        # è®¡ç®—è½®æ¬¡æŒ‡æ ‡
        if client_results:
            avg_loss = np.mean([r.result['loss'] for r in client_results.values()])
            avg_accuracy = np.mean([r.result['accuracy'] for r in client_results.values()])
        else:
            avg_loss, avg_accuracy = 0.0, 0.0

        self.logger.info(f"  Round {round_num} summary: Loss={avg_loss:.4f}, Acc={avg_accuracy:.4f}")

        # è¿”å›ç»“æœ - æ³¨æ„ä¸è¦åŒ…å«å¯èƒ½è¢«çˆ¶ç±»è¯¯ç”¨çš„å­—æ®µå
        return {
            "round": round_num,
            "participants": client_ids,
            "successful_clients": list(client_results.keys()),
            "failed_clients": failed_clients,
            # ä¸ä½¿ç”¨ "aggregated_model" è¿™ä¸ªé”®åï¼Œé¿å…çˆ¶ç±»è¯¯ç”¨
            "model_aggregated": aggregated_weights is not None,
            "round_metrics": {
                "avg_loss": avg_loss,
                "avg_accuracy": avg_accuracy,
                "successful_count": len(client_results)
            }
        }

    async def aggregate_models(self, client_results: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """èšåˆå®¢æˆ·ç«¯æ¨¡å‹ï¼ˆFedAvgï¼‰

        æ”¯æŒæ¥æ”¶torch.Tensoræˆ–numpyæ•°ç»„ï¼Œè‡ªåŠ¨è½¬æ¢
        """
        self.logger.info("  Aggregating models using FedAvg...")

        if not client_results:
            return None

        # è·å–æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹æƒé‡
        client_weights = []
        client_samples = []

        for client_id, result in client_results.items():
            if "model_weights" in result.result:
                # æ™ºèƒ½è½¬æ¢ï¼šæ”¯æŒnumpyæ•°ç»„å’Œtorch.Tensor
                torch_weights = {}
                for k, v in result.result["model_weights"].items():
                    if isinstance(v, np.ndarray):
                        torch_weights[k] = torch.from_numpy(v)
                    elif torch.is_tensor(v):
                        torch_weights[k] = v
                    else:
                        # å¦‚æœæ—¢ä¸æ˜¯numpyä¹Ÿä¸æ˜¯tensorï¼Œå°è¯•è½¬æ¢
                        torch_weights[k] = torch.tensor(v)
                client_weights.append(torch_weights)
                client_samples.append(result.result['samples_used'])

        if not client_weights:
            return None

        # è®¡ç®—åŠ æƒå¹³å‡
        total_samples = sum(client_samples)
        aggregated_weights = {}

        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„é”®
        first_model_keys = client_weights[0].keys()

        for key in first_model_keys:
            # åŠ æƒå¹³å‡æ¯ä¸ªå‚æ•°
            weighted_sum = torch.zeros_like(client_weights[0][key])
            for weights, samples in zip(client_weights, client_samples):
                weighted_sum += weights[key] * samples
            aggregated_weights[key] = weighted_sum / total_samples

        # åˆ†å‘å…¨å±€æ¨¡å‹ï¼ˆç›´æ¥ä¼ é€’tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–ï¼‰
        await self._distribute_global_model(aggregated_weights)

        return aggregated_weights

    async def evaluate_global_model(self) -> Dict[str, Any]:
        """è¯„ä¼°å…¨å±€æ¨¡å‹"""
        self.logger.info("  Evaluating global model...")

        available_clients = self.get_available_clients()
        if not available_clients:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        # å‡†å¤‡å…¨å±€æ¨¡å‹æ•°æ®ï¼ˆç›´æ¥ä¼ é€’torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–ï¼‰
        global_model_data = {
            "model_weights": self.global_model_obj.get_weights_as_dict()
        }

        # å¹¶è¡Œè¯„ä¼°
        tasks = []
        for client_id in available_clients:
            proxy = self._proxy_manager.get_proxy(client_id)
            if proxy:
                task = proxy.evaluate(global_model_data)
                tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ - æ³¨æ„ï¼šproxy.evaluate() è¿”å›çš„æ˜¯ TrainingResponse å¯¹è±¡
        valid_results = []
        for r in results:
            if not isinstance(r, Exception):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ TrainingResponse å¯¹è±¡
                if hasattr(r, 'result') and hasattr(r, 'success'):
                    # æ˜¯ TrainingResponse å¯¹è±¡ï¼Œæå– result å­—æ®µ
                    if r.success and r.result:
                        valid_results.append(r.result)
                elif isinstance(r, dict):
                    # æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    valid_results.append(r)

        if not valid_results:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        total_samples = sum(r.get("samples", 0) for r in valid_results)
        if total_samples == 0:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        weighted_accuracy = sum(r["accuracy"] * r.get("samples", 1) for r in valid_results) / total_samples
        weighted_loss = sum(r["loss"] * r.get("samples", 1) for r in valid_results) / total_samples

        return {
            "accuracy": weighted_accuracy,
            "loss": weighted_loss,
            "samples_count": total_samples
        }

    def should_stop_training(self, round_num: int, round_result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        # æ£€æŸ¥æœ€å¤§è½®æ¬¡
        if round_num >= self.training_config.max_rounds:
            return True

        # æ£€æŸ¥å‡†ç¡®ç‡æ”¶æ•›
        round_metrics = round_result.get("round_metrics", {})
        avg_accuracy = round_metrics.get("avg_accuracy", 0.0)

        if avg_accuracy >= 0.98:
            self.logger.info(f"  High accuracy achieved: {avg_accuracy:.4f}")
            return True

        return False

    async def _distribute_global_model(self, global_weights: Dict[str, torch.Tensor]):
        """åˆ†å‘å…¨å±€æ¨¡å‹åˆ°æ‰€æœ‰å®¢æˆ·ç«¯

        æ³¨æ„ï¼šç›´æ¥ä¼ é€’torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–
        """
        global_model_data = {
            "model_type": "mnist_cnn",
            "parameters": {"weights": global_weights}
        }

        tasks = []
        for client_id in self.get_available_clients():
            proxy = self._proxy_manager.get_proxy(client_id)
            if proxy:
                task = proxy.set_model(global_model_data)
                tasks.append(task)

        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success_count = sum(1 for r in results if not isinstance(r, Exception) and r)
            self.logger.info(f"  Global model distributed to {success_count}/{len(tasks)} clients")

# ==================== 5. ä¸»æ¼”ç¤ºç¨‹åº ====================

async def demo_real_mnist_training():
    """
    çœŸå®MNISTè”é‚¦å­¦ä¹ æ¼”ç¤ºï¼ˆä½¿ç”¨ç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼‰
    """
    print("=" * 80)
    print("ğŸš€ MNISTè”é‚¦å­¦ä¹ çœŸå®è®­ç»ƒæ¼”ç¤ºï¼ˆç»Ÿä¸€åˆå§‹åŒ–ç­–ç•¥ï¼‰")
    print("=" * 80)

    # ğŸ”§ æ¸…ç†Memoryæ¨¡å¼çš„å…±äº«çŠ¶æ€
    from fedcl.communication.memory_manager import MemoryCommunicationManager
    from fedcl.transport.memory import MemoryTransport
    print("\nğŸ§¹ æ¸…ç†Memoryæ¨¡å¼å…±äº«çŠ¶æ€...")
    MemoryCommunicationManager.clear_global_state()
    MemoryTransport.clear_global_state()
    print("âœ… å…±äº«çŠ¶æ€å·²æ¸…ç†\n")

    # æ˜¾ç¤ºå·²æ³¨å†Œçš„ç»„ä»¶
    print("ğŸ“‹ å·²æ³¨å†Œç»„ä»¶:")
    print(f"  Datasets: {list(registry.datasets.keys())}")
    print(f"  Models: {list(registry.models.keys())}")
    print(f"  Trainers: {list(registry.trainers.keys())}")
    print(f"  Learners: {list(registry.learners.keys())}")

    from fedcl.config import CommunicationConfig, TrainingConfig
    from fedcl.api import ComponentBuilder

    # ä½¿ç”¨ComponentBuilderè§£æé…ç½®
    builder = ComponentBuilder()

    # åˆ›å»ºæœåŠ¡å™¨é…ç½®ï¼ˆæ–°æ ¼å¼ï¼‰
    server_config_dict = {
        "training": {
            "trainer": {
                "name": "FedAvgMNIST",
                "params": {
                    "max_rounds": 5,
                    "min_clients": 2,
                    "client_selection_ratio": 1.0,
                    "local_epochs": 1,
                    "learning_rate": 0.01,
                    "batch_size": 32
                }
            },
            "global_model": {
                "name": "MNIST_CNN",
                "params": {
                    "num_classes": 10
                }
            }
        }
    }

    # è§£ææœåŠ¡å™¨é…ç½®
    server_parsed_config = builder.parse_config(server_config_dict)

    server_comm_config = CommunicationConfig(
        mode="process",
        role="server",
        node_id="server_1"
    )

    server_train_config = TrainingConfig()
    # è®¾ç½®æ—§æ ¼å¼çš„é…ç½®ï¼ˆç”¨äºBusinessInitializerï¼‰
    server_train_config.trainer = {
        "name": "FedAvgMNIST",
        "max_rounds": 5,
        "min_clients": 2,
        "client_selection_ratio": 1.0
    }
    # ä¼ é€’è§£æåçš„é…ç½®ï¼ˆç”¨äºTrainerçš„ç»Ÿä¸€åˆå§‹åŒ–ï¼‰
    server_train_config.parsed_config = server_parsed_config
    server_train_config.max_rounds = 5
    server_train_config.min_clients = 2
    # è®¾ç½®modelé…ç½®ä»¥é¿å…BusinessInitializerå‡ºé”™
    server_train_config.model = {"name": "MNIST_CNN"}

    # åˆ›å»ºå®¢æˆ·ç«¯é…ç½®ï¼ˆæ–°æ ¼å¼ï¼‰
    client_configs = []
    for i in range(3):
        client_config_dict = {
            "training": {
                "learner": {
                    "name": "MNISTLearner",
                    "params": {
                        "learning_rate": 0.01,
                        "batch_size": 32,
                        "local_epochs": 1
                    }
                },
                "dataset": {
                    # ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„ MNIST æ•°æ®é›†ï¼Œå¯¹åº” MNISTFederatedDataset ç±»ï¼ˆç¬¬49è¡Œå®šä¹‰ï¼‰
                    "name": "MNIST",  # è¿™ä¼šåˆ›å»º MNISTFederatedDataset å®ä¾‹
                    "params": {
                        "root": "./data",
                        "train": True,
                        "download": True
                    }
                }
            }
        }

        # è§£æå®¢æˆ·ç«¯é…ç½®
        client_parsed_config = builder.parse_config(client_config_dict)

        client_comm_config = CommunicationConfig(
            mode="process",
            role="client",
            node_id=f"client_{i}"
        )

        client_train_config = TrainingConfig()
        # è®¾ç½®æ—§æ ¼å¼çš„é…ç½®ï¼ˆç”¨äºBusinessInitializerï¼‰
        client_train_config.learner = {
            "name": "MNISTLearner",
            "learning_rate": 0.01,
            "batch_size": 32,
            "local_epochs": 1
        }
        # è®¾ç½® dataset é…ç½®ï¼Œä¼šé€šè¿‡æ³¨å†Œè¡¨åˆ›å»º MNISTFederatedDataset
        client_train_config.dataset = {"name": "MNIST"}
        # ä¼ é€’è§£æåçš„é…ç½®ï¼ˆç”¨äºLearnerçš„ç»Ÿä¸€åˆå§‹åŒ–ï¼‰
        client_train_config.parsed_config = client_parsed_config

        client_configs.append((client_comm_config, client_train_config))

    # åˆå¹¶æ‰€æœ‰é…ç½®
    all_configs = [(server_comm_config, server_train_config)] + client_configs

    # åˆ›å»ºFederatedLearning
    fl = FederatedLearning(all_configs)

    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("\nğŸ”§ åˆå§‹åŒ–è”é‚¦å­¦ä¹ ç³»ç»Ÿ...")
        await fl.initialize()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        # è¿è¡Œè®­ç»ƒ
        print("\n" + "=" * 80)
        print("ğŸ‹ï¸ å¼€å§‹çœŸå®MNISTè®­ç»ƒ...")
        print("=" * 80)

        result = await fl.run(max_rounds=5)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
        print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
        print(f"  å®Œæˆè½®æ•°: {result.completed_rounds}/{result.total_rounds}")
        print(f"  ç»ˆæ­¢åŸå› : {result.termination_reason}")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {result.final_accuracy:.4f}")
        print(f"  æœ€ç»ˆæŸå¤±: {result.final_loss:.4f}")
        print(f"  æ€»æ—¶é—´: {result.total_time:.2f}ç§’")

        # æ˜¾ç¤ºè®­ç»ƒè½¨è¿¹
        print(f"\nğŸ“ˆ è®­ç»ƒè½¨è¿¹:")
        for i, round_result in enumerate(result.training_history):
            metrics = round_result.get("round_metrics", {})
            print(f"  Round {i+1}: Loss={metrics.get('avg_loss', 0):.4f}, "
                  f"Acc={metrics.get('avg_accuracy', 0):.4f}, "
                  f"Clients={metrics.get('successful_count', 0)}")

    finally:
        # æ¸…ç†èµ„æº
        await fl.cleanup()

    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")

# ==================== 6. ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    print("=" * 80)
    print("MOE-FedCL çœŸå®MNISTè”é‚¦å­¦ä¹ æ¼”ç¤º")
    print("=" * 80)
    print("\nç‰¹æ€§:")
    print("  âœ… çœŸå®MNISTæ•°æ®é›†åŠ è½½å’Œåˆ’åˆ†")
    print("  âœ… çœŸå®CNNæ¨¡å‹è®­ç»ƒ")
    print("  âœ… FedAvgèšåˆç®—æ³•")
    print("  âœ… è£…é¥°å™¨æ³¨å†Œç»„ä»¶")
    print("  âœ… é…ç½®æ–‡ä»¶é©±åŠ¨")
    print("  âœ… å¼‚æ­¥è®­ç»ƒå’Œè¯„ä¼°")
    print()

    # è¿è¡Œæ¼”ç¤º
    try:
        asyncio.run(demo_real_mnist_training())
    except KeyboardInterrupt:
        print("\nâŒ è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()