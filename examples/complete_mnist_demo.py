"""
å®Œæ•´çš„MNISTè”é‚¦å­¦ä¹ æ¼”ç¤º - çœŸå®è®­ç»ƒç‰ˆæœ¬
ä½¿ç”¨æ–°æ¶æ„å®ç°çœŸå®çš„MNISTè”é‚¦å­¦ä¹ è®­ç»ƒ
examples/complete_mnist_demo.py
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, List
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import torchvision
import torchvision.transforms as transforms

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from fedcl.learner.base_learner import BaseLearner
from fedcl.trainer.trainer import BaseTrainer, TrainingConfig
from fedcl.types import TrainingRequest, TrainingResponse
from fedcl import FederatedLearning

# å¯¼å…¥æ–°å®ç°çš„æ•°æ®é›†å’Œæ¨¡å‹ç®¡ç†
from fedcl.api.decorators import dataset, model
from fedcl.api.registry import registry
from fedcl.methods.datasets.base import FederatedDataset
from fedcl.methods.models.base import FederatedModel

# å¯¼å…¥è£…é¥°å™¨
from fedcl.api import learner, trainer
from fedcl.utils.auto_logger import get_train_logger

# ==================== 1. æ³¨å†ŒçœŸå®çš„MNISTæ•°æ®é›† ====================

@dataset(
    name='MNIST',
    description='MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†',
    dataset_type='image_classification',
    num_classes=10
)
class MNISTFederatedDataset(FederatedDataset):
    """MNISTè”é‚¦æ•°æ®é›†å®ç°"""

    def __init__(self, root: str = './data', train: bool = True, download: bool = True):
        super().__init__(root, train, download)

        # æ•°æ®è½¬æ¢
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        # åŠ è½½MNISTæ•°æ®é›†
        self.dataset = torchvision.datasets.MNIST(
            root=root,
            train=train,
            download=download,
            transform=transform
        )

        # è®¾ç½®å±æ€§
        self.num_classes = 10
        self.input_shape = (1, 28, 28)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'dataset_name': 'MNIST',
            'num_samples': len(self.dataset),
            'num_classes': self.num_classes,
            'input_shape': self.input_shape,
            'train': self.train,
        }

# ==================== 2. æ³¨å†ŒçœŸå®çš„CNNæ¨¡å‹ ====================

@model(
    name='MNIST_CNN',
    description='MNIST CNNåˆ†ç±»æ¨¡å‹',
    task='classification',
    input_shape=(1, 28, 28),
    output_shape=(10,)
)
class MNISTCNNModel(FederatedModel):
    """MNIST CNNæ¨¡å‹"""

    def __init__(self, num_classes: int = 10):
        super().__init__()

        # è®¾ç½®å…ƒæ•°æ®
        self.set_metadata(
            task_type='classification',
            input_shape=(1, 28, 28),
            output_shape=(num_classes,)
        )

        # å®šä¹‰ç½‘ç»œç»“æ„
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(64 * 14 * 14, 128)
        self.fc2 = nn.Linear(128, num_classes)

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = self.dropout1(x)
        x = x.view(-1, 64 * 14 * 14)
        x = torch.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

    def get_weights_as_dict(self) -> Dict[str, torch.Tensor]:
        """è·å–æ¨¡å‹æƒé‡"""
        return {k: v.cpu().clone() for k, v in self.state_dict().items()}

    def set_weights_from_dict(self, weights: Dict[str, torch.Tensor], strict: bool = True):
        """è®¾ç½®æ¨¡å‹æƒé‡"""
        self.load_state_dict(weights)

    def get_param_count(self) -> int:
        """è·å–å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters())

# ==================== 3. å®ç°çœŸå®çš„Learner ====================

@learner('MNISTLearner',
         description='MNISTæ•°æ®é›†å­¦ä¹ å™¨',
         version='1.0',
         author='MOE-FedCL',
         dataset='MNIST')
class MNISTLearner(BaseLearner):
    """MNISTå­¦ä¹ å™¨ - å®ç°çœŸå®çš„è®­ç»ƒ"""

    def __init__(self, client_id: str, config: Dict[str, Any], logger=None):
        super().__init__(client_id, config, logger)
        self.logger = get_train_logger(client_id)

        # è®­ç»ƒé…ç½®
        self.learning_rate = config.get('learning_rate', 0.01)
        self.batch_size = config.get('batch_size', 32)
        self.local_epochs = config.get('local_epochs', 1)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½æ•°æ®é›†
        self._load_dataset()

        # åˆ›å»ºæ¨¡å‹
        self.model = MNISTCNNModel(num_classes=10).to(self.device)
        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=0.9)
        self.criterion = nn.CrossEntropyLoss()

        print(f"âœ… MNISTLearner {client_id} initialized with {len(self.train_dataset)} samples")

    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        # ä»æ³¨å†Œè¡¨è·å–MNISTæ•°æ®é›†
        mnist_dataset_cls = registry.get_dataset('MNIST')
        mnist_dataset = mnist_dataset_cls(root='./data', train=True, download=True)

        # è·å–å®¢æˆ·ç«¯æ•°æ®åˆ’åˆ†
        num_clients = int(self.client_id.split('_')[1]) + 3  # å‡è®¾æ€»å…±3ä¸ªå®¢æˆ·ç«¯
        client_datasets = mnist_dataset.partition(
            num_clients=num_clients,
            strategy='non_iid_label',
            labels_per_client=2
        )

        # è·å–å½“å‰å®¢æˆ·ç«¯çš„æ•°æ®
        client_idx = int(self.client_id.split('_')[1])
        if client_idx in client_datasets:
            self.train_dataset = client_datasets[client_idx]
        else:
            # å¦‚æœæ²¡æœ‰åˆ’åˆ†ï¼Œä½¿ç”¨éƒ¨åˆ†æ•°æ®
            total_size = len(mnist_dataset.dataset)
            start = (client_idx * total_size) // num_clients
            end = ((client_idx + 1) * total_size) // num_clients
            indices = list(range(start, end))
            self.train_dataset = Subset(mnist_dataset.dataset, indices)

        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True
        )

    async def train(self, params: Dict[str, Any]) -> TrainingResponse:
        """è®­ç»ƒæ–¹æ³•"""
        num_epochs = params.get("num_epochs", self.local_epochs)
        round_number = params.get("round_number", 1)

        self.logger.info(f"  [{self.client_id}] Round {round_number}, Training {num_epochs} epochs...")

        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        self.model.train()

        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        for epoch in range(num_epochs):
            epoch_loss = 0.0
            epoch_correct = 0
            epoch_samples = 0

            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)

                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                epoch_correct += pred.eq(target.view_as(pred)).sum().item()
                epoch_samples += data.size(0)

            avg_epoch_loss = epoch_loss / epoch_samples
            epoch_accuracy = epoch_correct / epoch_samples
            self.logger.info(f"    [{self.client_id}] Epoch {epoch+1}: Loss={avg_epoch_loss:.4f}, Acc={epoch_accuracy:.4f}")

            total_loss += epoch_loss
            correct_predictions += epoch_correct
            total_samples += epoch_samples

        # è®¡ç®—å¹³å‡å€¼
        avg_loss = total_loss / total_samples
        accuracy = correct_predictions / total_samples

        # è·å–æ¨¡å‹æƒé‡ï¼ˆç›´æ¥è¿”å›torch.Tensorï¼Œåº•å±‚ä¼šè‡ªåŠ¨è½¬æ¢ï¼‰
        model_weights = self.model.get_weights_as_dict()

        # åˆ›å»ºè®­ç»ƒå“åº”
        response = TrainingResponse(
            request_id="",  # ä¼šè¢«stubå¡«å……
            client_id=self.client_id,
            success=True,
            result={
                "epochs_completed": num_epochs,
                "loss": avg_loss,
                "accuracy": accuracy,
                "samples_used": total_samples,
                "model_weights": model_weights  # æ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–tensor
            },
            execution_time=0.0
        )

        self.logger.info(f"  [{self.client_id}] Round {round_number} completed: Loss={avg_loss:.4f}, Acc={accuracy:.4f}")
        return response

    async def evaluate(self, model_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """è¯„ä¼°æ–¹æ³•"""
        self.model.eval()

        # å¦‚æœæä¾›äº†æ¨¡å‹æƒé‡ï¼Œå…ˆæ›´æ–°æ¨¡å‹
        if model_data and "model_weights" in model_data:
            weights = model_data["model_weights"]
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorch tensor
            torch_weights = {}
            for k, v in weights.items():
                if isinstance(v, np.ndarray):
                    torch_weights[k] = torch.from_numpy(v)
                else:
                    torch_weights[k] = v
            self.model.set_weights_from_dict(torch_weights)

        test_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in self.train_loader:  # ä½¿ç”¨è®­ç»ƒæ•°æ®ä½œä¸ºè¯„ä¼°æ•°æ®
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += data.size(0)

        return {
            "accuracy": correct / total,
            "loss": test_loss / total,
            "samples": total
        }

    async def get_model(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ•°æ®

        ç›´æ¥è¿”å›torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–
        """
        return {
            "model_type": "mnist_cnn",
            "parameters": {"weights": self.model.get_weights_as_dict()},
            "metadata": {
                "client_id": self.client_id,
                "samples": len(self.train_dataset),
                "param_count": self.model.get_param_count()
            }
        }

    async def set_model(self, model_data: Dict[str, Any]) -> bool:
        """è®¾ç½®æ¨¡å‹æ•°æ®

        æ¥å—torch.Tensoræˆ–numpyæ•°ç»„ï¼Œè‡ªåŠ¨è½¬æ¢
        """
        try:
            if "parameters" in model_data and "weights" in model_data["parameters"]:
                weights = model_data["parameters"]["weights"]
                # æ™ºèƒ½è½¬æ¢ï¼šæ”¯æŒnumpyæ•°ç»„ã€torch.Tensorå’Œdict
                torch_weights = {}
                for k, v in weights.items():
                    if isinstance(v, np.ndarray):
                        torch_weights[k] = torch.from_numpy(v)
                    elif torch.is_tensor(v):
                        torch_weights[k] = v
                    else:
                        torch_weights[k] = v
                self.model.set_weights_from_dict(torch_weights)
                return True
        except Exception as e:
            self.logger.exception(f"  [{self.client_id}] Failed to set model: {e}")
        return False

    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        return {
            "total_samples": len(self.train_dataset),
            "num_classes": 10,
            "feature_dim": 784,
            "input_shape": (1, 28, 28)
        }

    async def get_local_model(self) -> Dict[str, Any]:
        return await self.get_model()

    async def set_local_model(self, model_data: Dict[str, Any]) -> bool:
        return await self.set_model(model_data)

# ==================== 4. å®ç°çœŸå®çš„Trainer ====================

@trainer('FedAvgMNIST',
         description='MNISTè”é‚¦å¹³å‡è®­ç»ƒå™¨',
         version='1.0',
         author='MOE-FedCL',
         algorithms=['fedavg'])
class FedAvgMNISTTrainer(BaseTrainer):
    """è”é‚¦å¹³å‡è®­ç»ƒå™¨ - å®ç°çœŸå®çš„æ¨¡å‹èšåˆ"""

    def __init__(self, global_model: Dict[str, Any] = None, training_config=None, logger=None):
        # åˆ›å»ºå…¨å±€æ¨¡å‹å¯¹è±¡ï¼ˆåœ¨è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ä¹‹å‰ï¼‰
        self.global_model_obj = MNISTCNNModel(num_classes=10)

        # å¤„ç†é…ç½®å‚æ•°
        if isinstance(training_config, dict):
            config_obj = TrainingConfig(
                max_rounds=training_config.get("max_rounds", 5),
                min_clients=training_config.get("min_clients", 2),
                client_selection_ratio=training_config.get("client_selection_ratio", 1.0)
            )
        elif isinstance(training_config, TrainingConfig):
            config_obj = training_config
        else:
            config_obj = TrainingConfig()

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä¼ å…¥Noneä½œä¸ºglobal_modelï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è‡ªå·±çš„æ¨¡å‹å¯¹è±¡ï¼‰
        super().__init__(None, config_obj, logger)
        self.logger = get_train_logger("FedAvgMNISTTrainer")

        # ç¡®ä¿ self.global_model æŒ‡å‘æˆ‘ä»¬çš„æ¨¡å‹å¯¹è±¡
        self.global_model = self.global_model_obj

        # å¦‚æœæä¾›äº†åˆå§‹æƒé‡ï¼Œè®¾ç½®åˆ°æ¨¡å‹ä¸­
        if global_model and "parameters" in global_model:
            weights = global_model["parameters"].get("weights", {})
            torch_weights = {}
            for k, v in weights.items():
                if isinstance(v, np.ndarray):
                    torch_weights[k] = torch.from_numpy(v)
                else:
                    torch_weights[k] = v
            self.global_model.set_weights_from_dict(torch_weights)

        self.logger.info(f"FedAvgMNISTTrainer initialized with {self.global_model.get_param_count():,} parameters")

    async def train_round(self, round_num: int, client_ids: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€è½®è”é‚¦è®­ç»ƒ"""
        self.logger.info(f"\n--- Round {round_num} ---")
        self.logger.info(f"  Selected clients: {client_ids}")

        # åˆ›å»ºè®­ç»ƒè¯·æ±‚
        training_params = {
            "round_number": round_num,
            "num_epochs": 1,
            "batch_size": 32,
            "learning_rate": 0.01
        }

        # å¹¶è¡Œè®­ç»ƒæ‰€æœ‰å®¢æˆ·ç«¯
        tasks = []
        for client_id in client_ids:
            if self.is_client_ready(client_id):
                proxy = self._proxy_manager.get_proxy(client_id)
                if proxy:
                    self.logger.info(f"  [{client_id}] Starting training...")
                    task = proxy.train(training_params)
                    tasks.append((client_id, task))

        # æ”¶é›†ç»“æœ
        client_results = {}
        failed_clients = []

        for client_id, task in tasks:
            try:
                result = await task
                if result.success:
                    client_results[client_id] = result
                    self.logger.info(f"  [{client_id}] Training succeeded: Loss={result.result['loss']:.4f}, Acc={result.result['accuracy']:.4f}")
                else:
                    self.logger.error(f"  [{client_id}] Training failed: {result}")
                    failed_clients.append(client_id)
            except Exception as e:
                self.logger.exception(f"  [{client_id}] Training failed: {e}")
                failed_clients.append(client_id)

        # èšåˆæ¨¡å‹
        aggregated_weights = None
        if client_results:
            aggregated_weights = await self.aggregate_models(client_results)
            if aggregated_weights:
                self.global_model.set_weights_from_dict(aggregated_weights)

        # è®¡ç®—è½®æ¬¡æŒ‡æ ‡
        if client_results:
            avg_loss = np.mean([r.result['loss'] for r in client_results.values()])
            avg_accuracy = np.mean([r.result['accuracy'] for r in client_results.values()])
        else:
            avg_loss, avg_accuracy = 0.0, 0.0

        self.logger.info(f"  Round {round_num} summary: Loss={avg_loss:.4f}, Acc={avg_accuracy:.4f}")

        # è¿”å›ç»“æœ - æ³¨æ„ä¸è¦åŒ…å«å¯èƒ½è¢«çˆ¶ç±»è¯¯ç”¨çš„å­—æ®µå
        return {
            "round": round_num,
            "participants": client_ids,
            "successful_clients": list(client_results.keys()),
            "failed_clients": failed_clients,
            # ä¸ä½¿ç”¨ "aggregated_model" è¿™ä¸ªé”®åï¼Œé¿å…çˆ¶ç±»è¯¯ç”¨
            "model_aggregated": aggregated_weights is not None,
            "round_metrics": {
                "avg_loss": avg_loss,
                "avg_accuracy": avg_accuracy,
                "successful_count": len(client_results)
            }
        }

    async def aggregate_models(self, client_results: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """èšåˆå®¢æˆ·ç«¯æ¨¡å‹ï¼ˆFedAvgï¼‰

        æ”¯æŒæ¥æ”¶torch.Tensoræˆ–numpyæ•°ç»„ï¼Œè‡ªåŠ¨è½¬æ¢
        """
        self.logger.info("  Aggregating models using FedAvg...")

        if not client_results:
            return None

        # è·å–æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹æƒé‡
        client_weights = []
        client_samples = []

        for client_id, result in client_results.items():
            if "model_weights" in result.result:
                # æ™ºèƒ½è½¬æ¢ï¼šæ”¯æŒnumpyæ•°ç»„å’Œtorch.Tensor
                torch_weights = {}
                for k, v in result.result["model_weights"].items():
                    if isinstance(v, np.ndarray):
                        torch_weights[k] = torch.from_numpy(v)
                    elif torch.is_tensor(v):
                        torch_weights[k] = v
                    else:
                        # å¦‚æœæ—¢ä¸æ˜¯numpyä¹Ÿä¸æ˜¯tensorï¼Œå°è¯•è½¬æ¢
                        torch_weights[k] = torch.tensor(v)
                client_weights.append(torch_weights)
                client_samples.append(result.result['samples_used'])

        if not client_weights:
            return None

        # è®¡ç®—åŠ æƒå¹³å‡
        total_samples = sum(client_samples)
        aggregated_weights = {}

        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„é”®
        first_model_keys = client_weights[0].keys()

        for key in first_model_keys:
            # åŠ æƒå¹³å‡æ¯ä¸ªå‚æ•°
            weighted_sum = torch.zeros_like(client_weights[0][key])
            for weights, samples in zip(client_weights, client_samples):
                weighted_sum += weights[key] * samples
            aggregated_weights[key] = weighted_sum / total_samples

        # åˆ†å‘å…¨å±€æ¨¡å‹ï¼ˆç›´æ¥ä¼ é€’tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–ï¼‰
        await self._distribute_global_model(aggregated_weights)

        return aggregated_weights

    async def evaluate_global_model(self) -> Dict[str, Any]:
        """è¯„ä¼°å…¨å±€æ¨¡å‹"""
        self.logger.info("  Evaluating global model...")

        available_clients = self.get_available_clients()
        if not available_clients:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        # å‡†å¤‡å…¨å±€æ¨¡å‹æ•°æ®ï¼ˆç›´æ¥ä¼ é€’torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–ï¼‰
        global_model_data = {
            "model_weights": self.global_model.get_weights_as_dict()
        }

        # å¹¶è¡Œè¯„ä¼°
        tasks = []
        for client_id in available_clients:
            proxy = self._proxy_manager.get_proxy(client_id)
            if proxy:
                task = proxy.evaluate(global_model_data)
                tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ - æ³¨æ„ï¼šproxy.evaluate() è¿”å›çš„æ˜¯ TrainingResponse å¯¹è±¡
        valid_results = []
        for r in results:
            if not isinstance(r, Exception):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ TrainingResponse å¯¹è±¡
                if hasattr(r, 'result') and hasattr(r, 'success'):
                    # æ˜¯ TrainingResponse å¯¹è±¡ï¼Œæå– result å­—æ®µ
                    if r.success and r.result:
                        valid_results.append(r.result)
                elif isinstance(r, dict):
                    # æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    valid_results.append(r)

        if not valid_results:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        total_samples = sum(r.get("samples", 0) for r in valid_results)
        if total_samples == 0:
            return {"accuracy": 0.0, "loss": float('inf'), "samples_count": 0}

        weighted_accuracy = sum(r["accuracy"] * r.get("samples", 1) for r in valid_results) / total_samples
        weighted_loss = sum(r["loss"] * r.get("samples", 1) for r in valid_results) / total_samples

        return {
            "accuracy": weighted_accuracy,
            "loss": weighted_loss,
            "samples_count": total_samples
        }

    def should_stop_training(self, round_num: int, round_result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        # æ£€æŸ¥æœ€å¤§è½®æ¬¡
        if round_num >= self.training_config.max_rounds:
            return True

        # æ£€æŸ¥å‡†ç¡®ç‡æ”¶æ•›
        round_metrics = round_result.get("round_metrics", {})
        avg_accuracy = round_metrics.get("avg_accuracy", 0.0)

        if avg_accuracy >= 0.98:
            self.logger.info(f"  High accuracy achieved: {avg_accuracy:.4f}")
            return True

        return False

    async def _distribute_global_model(self, global_weights: Dict[str, torch.Tensor]):
        """åˆ†å‘å…¨å±€æ¨¡å‹åˆ°æ‰€æœ‰å®¢æˆ·ç«¯

        æ³¨æ„ï¼šç›´æ¥ä¼ é€’torch.Tensorï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨åºåˆ—åŒ–
        """
        global_model_data = {
            "model_type": "mnist_cnn",
            "parameters": {"weights": global_weights}
        }

        tasks = []
        for client_id in self.get_available_clients():
            proxy = self._proxy_manager.get_proxy(client_id)
            if proxy:
                task = proxy.set_model(global_model_data)
                tasks.append(task)

        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success_count = sum(1 for r in results if not isinstance(r, Exception) and r)
            self.logger.info(f"  Global model distributed to {success_count}/{len(tasks)} clients")

# ==================== 5. ä¸»æ¼”ç¤ºç¨‹åº ====================

async def demo_real_mnist_training():
    """
    çœŸå®MNISTè”é‚¦å­¦ä¹ æ¼”ç¤º
    """
    print("=" * 80)
    print("ğŸš€ MNISTè”é‚¦å­¦ä¹ çœŸå®è®­ç»ƒæ¼”ç¤º")
    print("=" * 80)

    # ğŸ”§ æ¸…ç†Memoryæ¨¡å¼çš„å…±äº«çŠ¶æ€
    from fedcl.communication.memory_manager import MemoryCommunicationManager
    from fedcl.transport.memory import MemoryTransport
    print("\nğŸ§¹ æ¸…ç†Memoryæ¨¡å¼å…±äº«çŠ¶æ€...")
    MemoryCommunicationManager.clear_global_state()
    MemoryTransport.clear_global_state()
    print("âœ… å…±äº«çŠ¶æ€å·²æ¸…ç†\n")

    # æ˜¾ç¤ºå·²æ³¨å†Œçš„ç»„ä»¶
    print("ğŸ“‹ å·²æ³¨å†Œç»„ä»¶:")
    print(f"  Datasets: {list(registry.datasets.keys())}")
    print(f"  Models: {list(registry.models.keys())}")
    print(f"  Trainers: {list(registry.trainers.keys())}")
    print(f"  Learners: {list(registry.learners.keys())}")

    from fedcl.config import CommunicationConfig, TrainingConfig

    # åˆ›å»ºæœåŠ¡å™¨é…ç½®
    server_comm_config = CommunicationConfig(
        mode="process",
        role="server",
        node_id="server_1"
    )

    server_train_config = TrainingConfig()
    server_train_config.trainer = {
        "name": "FedAvgMNIST",
        "max_rounds": 5,
        "min_clients": 2,
        "client_selection_ratio": 1.0
    }
    server_train_config.aggregator = {"name": None}
    server_train_config.model = {"name": "MNIST_CNN"}

    # åˆ›å»ºå®¢æˆ·ç«¯é…ç½®
    client_configs = []
    for i in range(3):
        client_comm_config = CommunicationConfig(
            mode="process",
            role="client",
            node_id=f"client_{i}"
        )

        client_train_config = TrainingConfig()
        client_train_config.learner = {
            "name": "MNISTLearner",
            "learning_rate": 0.01,
            "batch_size": 32,
            "local_epochs": 1
        }
        client_train_config.dataset = {"name": "MNIST"}

        client_configs.append((client_comm_config, client_train_config))

    # åˆå¹¶æ‰€æœ‰é…ç½®
    all_configs = [(server_comm_config, server_train_config)] + client_configs

    # åˆ›å»ºFederatedLearning
    fl = FederatedLearning(all_configs)

    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("\nğŸ”§ åˆå§‹åŒ–è”é‚¦å­¦ä¹ ç³»ç»Ÿ...")
        await fl.initialize()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        # è¿è¡Œè®­ç»ƒ
        print("\n" + "=" * 80)
        print("ğŸ‹ï¸ å¼€å§‹çœŸå®MNISTè®­ç»ƒ...")
        print("=" * 80)

        result = await fl.run(max_rounds=5)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)
        print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
        print(f"  å®Œæˆè½®æ•°: {result.completed_rounds}/{result.total_rounds}")
        print(f"  ç»ˆæ­¢åŸå› : {result.termination_reason}")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {result.final_accuracy:.4f}")
        print(f"  æœ€ç»ˆæŸå¤±: {result.final_loss:.4f}")
        print(f"  æ€»æ—¶é—´: {result.total_time:.2f}ç§’")

        # æ˜¾ç¤ºè®­ç»ƒè½¨è¿¹
        print(f"\nğŸ“ˆ è®­ç»ƒè½¨è¿¹:")
        for i, round_result in enumerate(result.training_history):
            metrics = round_result.get("round_metrics", {})
            print(f"  Round {i+1}: Loss={metrics.get('avg_loss', 0):.4f}, "
                  f"Acc={metrics.get('avg_accuracy', 0):.4f}, "
                  f"Clients={metrics.get('successful_count', 0)}")

    finally:
        # æ¸…ç†èµ„æº
        await fl.cleanup()

    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")

# ==================== 6. ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    print("=" * 80)
    print("MOE-FedCL çœŸå®MNISTè”é‚¦å­¦ä¹ æ¼”ç¤º")
    print("=" * 80)
    print("\nç‰¹æ€§:")
    print("  âœ… çœŸå®MNISTæ•°æ®é›†åŠ è½½å’Œåˆ’åˆ†")
    print("  âœ… çœŸå®CNNæ¨¡å‹è®­ç»ƒ")
    print("  âœ… FedAvgèšåˆç®—æ³•")
    print("  âœ… è£…é¥°å™¨æ³¨å†Œç»„ä»¶")
    print("  âœ… é…ç½®æ–‡ä»¶é©±åŠ¨")
    print("  âœ… å¼‚æ­¥è®­ç»ƒå’Œè¯„ä¼°")
    print()

    # è¿è¡Œæ¼”ç¤º
    try:
        asyncio.run(demo_real_mnist_training())
    except KeyboardInterrupt:
        print("\nâŒ è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()