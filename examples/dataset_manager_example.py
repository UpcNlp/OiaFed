# examples/dataset_manager_example.py
"""
DatasetManagerä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨DatasetManagerè¿›è¡Œæ•°æ®é›†ç®¡ç†ã€ç¼“å­˜ã€éªŒè¯å’Œä»»åŠ¡åºåˆ—åˆ›å»ºã€‚
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from fedcl.config.config_manager import DictConfig
from fedcl.data.dataset_manager import DatasetManager
from fedcl.data.dataset import Dataset
from fedcl.data.task_generator import TaskGenerator
from fedcl.data.split_strategy import IIDSplitStrategy


def create_sample_dataset(name: str, num_samples: int = 1000, num_classes: int = 10) -> Dataset:
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    data = torch.randn(num_samples, 32, 32, 3)
    targets = torch.randint(0, num_classes, (num_samples,))
    return Dataset(name, data, targets)


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºDatasetManagerçš„ä½¿ç”¨"""
    
    print("=== DatasetManagerä½¿ç”¨ç¤ºä¾‹ ===\n")
    
    # 1. åˆ›å»ºé…ç½®
    config = DictConfig({
        'datasets': {
            'cifar10': {
                'type': 'torchvision',
                'name': 'CIFAR10',
                'root': './data',
                'download': False  # è®¾ä¸ºFalseé¿å…å®é™…ä¸‹è½½
            },
            'custom_dataset': {
                'type': 'custom',
                'data_path': './custom_data'
            }
        },
        'cache': {
            'enable': True,
            'max_size': '100MB',
            'strategy': 'LRU'
        },
        'task_generation': {
            'num_tasks': 5,
            'classes_per_task': 2,
            'type': 'class_incremental',
            'random_seed': 42
        }
    })
    
    # 2. åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨å’Œæ•°æ®é›†ç®¡ç†å™¨
    split_strategy = IIDSplitStrategy(config)
    task_generator = TaskGenerator(config, split_strategy)
    dataset_manager = DatasetManager(config, task_generator)
    
    print("âœ… DatasetManageråˆå§‹åŒ–å®Œæˆ")
    
    # 3. åˆ›å»ºå’Œæ³¨å†Œç¤ºä¾‹æ•°æ®é›†
    print("\nğŸ“ åˆ›å»ºå’Œæ³¨å†Œæ•°æ®é›†...")
    datasets = []
    for i in range(3):
        dataset = create_sample_dataset(f"sample_dataset_{i}", 500, 10)
        datasets.append(dataset)
        dataset_manager.register_dataset(f"sample_{i}", dataset)
        print(f"   - æ³¨å†Œæ•°æ®é›†: sample_{i} (å¤§å°: {len(dataset)})")
    
    # 4. è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
    for i in range(3):
        stats = dataset_manager.get_dataset_statistics(f"sample_{i}")
        print(f"   - sample_{i}:")
        print(f"     * æ ·æœ¬æ•°é‡: {stats['size']}")
        print(f"     * ç±»åˆ«æ•°é‡: {stats['num_classes']}")
        print(f"     * å†…å­˜ä½¿ç”¨: {stats['memory_usage_mb']:.2f} MB")
        print(f"     * æ•°æ®å½¢çŠ¶: {stats['data_shape']}")
    
    # 5. æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print("\nğŸ’¾ ç¼“å­˜åŠŸèƒ½æµ‹è¯•:")
    dataset_manager.cache_dataset("cached_test", datasets[0])
    
    # è·å–ç¼“å­˜ç»Ÿè®¡
    cache_stats = dataset_manager.cache.get_stats()
    print(f"   - ç¼“å­˜å¤§å°: {cache_stats['size']}")
    print(f"   - å†…å­˜ä½¿ç”¨: {cache_stats['memory_usage_mb']:.2f} MB")
    print(f"   - ç¼“å­˜åˆ©ç”¨ç‡: {cache_stats['utilization']:.2%}")
    
    # 6. æ•°æ®é›†éªŒè¯
    print("\nâœ… æ•°æ®é›†éªŒè¯:")
    for i in range(3):
        validation_result = dataset_manager.validate_dataset(datasets[i])
        status = "é€šè¿‡" if validation_result.is_valid else "å¤±è´¥"
        print(f"   - sample_{i}: {status}")
        if validation_result.warnings:
            for warning in validation_result.warnings:
                print(f"     è­¦å‘Š: {warning}")
    
    # 7. åˆ›å»ºä»»åŠ¡åºåˆ—
    print("\nğŸ¯ åˆ›å»ºä»»åŠ¡åºåˆ—:")
    try:
        tasks = dataset_manager.create_task_sequence("sample_0", 5)
        print(f"   - åˆ›å»ºäº† {len(tasks)} ä¸ªä»»åŠ¡")
        for i, task in enumerate(tasks):
            print(f"     * ä»»åŠ¡ {i+1}: ç±»åˆ« {task.classes}")
    except Exception as e:
        print(f"   - ä»»åŠ¡åˆ›å»ºå¤±è´¥: {e}")
    
    # 8. åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    print("\nğŸ“‹ å¯ç”¨æ•°æ®é›†åˆ—è¡¨:")
    available_datasets = dataset_manager.list_available_datasets()
    for dataset_name in available_datasets:
        print(f"   - {dataset_name}")
    
    # 9. ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ DatasetManagerç»Ÿè®¡:")
    manager_stats = dataset_manager.get_manager_statistics()
    print(f"   - å·²æ³¨å†Œæ•°æ®é›†: {manager_stats['registered_datasets']}")
    print(f"   - é…ç½®æ•°æ®é›†: {manager_stats['configured_datasets']}")
    print(f"   - ç¼“å­˜å‘½ä¸­: {manager_stats['cache_hits']}")
    print(f"   - ç¼“å­˜æœªå‘½ä¸­: {manager_stats['cache_misses']}")
    print(f"   - å·²åŠ è½½æ•°æ®é›†: {manager_stats['datasets_loaded']}")
    
    # 10. æ¸…ç†ç¼“å­˜
    print("\nğŸ§¹ æ¸…ç†ç¼“å­˜...")
    dataset_manager.clear_cache()
    print("   - ç¼“å­˜å·²æ¸…ç†")
    
    print("\nğŸ‰ DatasetManagerç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    main()
