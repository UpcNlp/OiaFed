"""
æ–°è”é‚¦å­¦ä¹ æ¶æ„æ¼”ç¤º

æ¼”ç¤ºåŸºäºé•¿è¿æ¥çš„MVPè”é‚¦å­¦ä¹ æ¶æ„ï¼š
1. è½»é‡åŒ–çš„åŸºç¡€è®¾æ–½
2. åŠ¨æ€RPCä»£ç†
3. æ¨é€æœºåˆ¶
4. ç”¨æˆ·è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
from typing import Dict, Any
from loguru import logger

# å¯¼å…¥æ–°æ¶æ„çš„ç»„ä»¶
from fedcl.fl import (
    BaseFLNode,
    TrainerBase
)
from fedcl.comm import MemoryTransport
from fedcl.utils.auto_logger import setup_auto_logging, get_train_logger, get_sys_logger
from fedcl import learner, trainer

# ==================== ç”¨æˆ·è‡ªå®šä¹‰å­¦ä¹ å™¨ ====================

class CustomLearner(BaseFLNode):
    """
    ç”¨æˆ·è‡ªå®šä¹‰å­¦ä¹ å™¨
    æ¼”ç¤ºå¦‚ä½•å®ç°è‡ªå·±çš„ä¸šåŠ¡é€»è¾‘
    """
    
    def __init__(self, learner_id: str, dataset_size: int = 100):
        super().__init__(learner_id, auto_connect=False)  # æ‰‹åŠ¨æ§åˆ¶è¿æ¥
        
        # ä¸šåŠ¡é€»è¾‘ç›¸å…³
        self.dataset_size = dataset_size
        self.model = {'weights': [1.0, 2.0, 3.0], 'bias': 0.1}
        self.local_epochs = 5
        self.learning_rate = 0.01
        
        # RPCæ–¹æ³•æ³¨å†Œ
        self._rpc_handlers = {
            'train': self.train,
            'evaluate': self.evaluate,
            'get_model': self.get_model,
            'set_model': self.set_model,
            'get_dataset_info': self.get_dataset_info,
            'ping': self.ping,
            '__get_methods__': self.get_methods
        }
        
    async def train(self, config: Dict = None) -> Dict[str, Any]:
        """è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘"""
        # ä½¿ç”¨è®­ç»ƒæ—¥å¿—è®°å½•å™¨
        try:
            train_logger = get_train_logger(self.node_id)
            train_logger.info(f"å¼€å§‹è®­ç»ƒ - æ•°æ®é›†å¤§å°: {self.dataset_size}")
        except:
            self.logger.info(f"å¼€å§‹è®­ç»ƒ - æ•°æ®é›†å¤§å°: {self.dataset_size}")
        
        # è§£æè®­ç»ƒé…ç½®
        epochs = config.get('epochs', self.local_epochs) if config else self.local_epochs
        lr = config.get('learning_rate', self.learning_rate) if config else self.learning_rate
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        initial_loss = 1.0
        for epoch in range(epochs):
            # æ¨¡æ‹Ÿè®­ç»ƒè¿­ä»£
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
            
            # æ›´æ–°æ¨¡å‹å‚æ•°
            for i in range(len(self.model['weights'])):
                self.model['weights'][i] += lr * 0.1 * (0.5 - epoch/epochs)
            self.model['bias'] += lr * 0.05
            
        final_loss = initial_loss * (1 - epochs * 0.1)
        accuracy = min(0.95, 0.6 + epochs * 0.05)
        
        result = {
            'model': self.model.copy(),
            'metrics': {
                'loss': final_loss,
                'accuracy': accuracy,
                'epochs': epochs,
                'samples': self.dataset_size
            },
            'learner_id': self.node_id
        }
        
        # è®°å½•è®­ç»ƒå®Œæˆ
        try:
            train_logger = get_train_logger(self.node_id)
            train_logger.info(f"è®­ç»ƒå®Œæˆ - Loss: {final_loss:.3f}, Acc: {accuracy:.3f}")
        except:
            self.logger.info(f"è®­ç»ƒå®Œæˆ - Loss: {final_loss:.3f}, Acc: {accuracy:.3f}")
        return result
        
    async def evaluate(self, test_data: Any = None) -> Dict[str, Any]:
        """æ¨¡å‹è¯„ä¼°"""
        try:
            train_logger = get_train_logger(self.node_id)
            train_logger.info("å¼€å§‹è¯„ä¼°")
        except:
            self.logger.info("å¼€å§‹è¯„ä¼°")
        
        # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
        await asyncio.sleep(0.05)
        
        test_loss = 0.15
        test_accuracy = 0.92
        
        return {
            'test_loss': test_loss,
            'test_accuracy': test_accuracy,
            'test_samples': 50
        }
        
    def get_model(self) -> Dict:
        """è·å–æ¨¡å‹"""
        return self.model.copy()
        
    def set_model(self, model: Dict):
        """è®¾ç½®æ¨¡å‹"""
        self.model = model.copy()
        self.logger.info("æ¨¡å‹å·²æ›´æ–°")
        
    def get_dataset_info(self) -> Dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'dataset_size': self.dataset_size,
            'features': len(self.model['weights']),
            'data_type': 'simulated'
        }
        
    async def ping(self) -> str:
        return f"pong from {self.node_id}"
        
    def get_methods(self) -> Dict[str, Dict]:
        """è¿”å›å¯è°ƒç”¨æ–¹æ³•ä¿¡æ¯"""
        methods = {}
        for name, method in self._rpc_handlers.items():
            if not name.startswith('_'):
                methods[name] = {
                    'is_async': asyncio.iscoroutinefunction(method),
                    'description': method.__doc__ or 'No description'
                }
        return methods
        
    async def on_push_received(self, data: Any):
        """æ¥æ”¶å…¨å±€æ¨¡å‹æ¨é€"""
        if isinstance(data, dict) and 'global_model' in data:
            self.set_model(data['global_model'])
            self.logger.info("æ¥æ”¶åˆ°å…¨å±€æ¨¡å‹æ¨é€")
            
    async def handle_rpc(self, request: Dict) -> Dict:
        """å¤„ç†RPCè¯·æ±‚"""
        method_name = request.get('method')
        args = request.get('args', ())
        kwargs = request.get('kwargs', {})
        
        try:
            handler = self._rpc_handlers.get(method_name)
            if not handler:
                raise ValueError(f"æœªçŸ¥æ–¹æ³•: {method_name}")
                
            if asyncio.iscoroutinefunction(handler):
                result = await handler(*args, **kwargs)
            else:
                result = handler(*args, **kwargs)
                
            return {'result': result}
            
        except Exception as e:
            return {'error': str(e)}


# ==================== ç”¨æˆ·è‡ªå®šä¹‰è®­ç»ƒå™¨ ====================

class CustomTrainer(TrainerBase):
    """
    ç”¨æˆ·è‡ªå®šä¹‰è®­ç»ƒå™¨
    æ¼”ç¤ºå¦‚ä½•å®ç°è‡ªå·±çš„è”é‚¦å­¦ä¹ ç®—æ³•
    """
    
    def __init__(self, trainer_id: str):
        super().__init__(trainer_id)
        
        # ç®—æ³•ç›¸å…³å‚æ•°
        self.global_model = {'weights': [1.0, 2.0, 3.0], 'bias': 0.1}
        self.min_participants = 2
        self.aggregation_strategy = 'weighted_avg'
        self.round_timeout = 30.0  # 30ç§’è¶…æ—¶
        
        # è®­ç»ƒå†å²
        self.training_history = []
        
    async def federated_round(self, round_num: int, config: Dict = None) -> Dict:
        """æ‰§è¡Œä¸€è½®è”é‚¦è®­ç»ƒ"""
        self.logger.info(f"=== å¼€å§‹ç¬¬ {round_num} è½®è”é‚¦è®­ç»ƒ ===")
        start_time = asyncio.get_event_loop().time()
        
        try:
            # 1. é€‰æ‹©å‚ä¸å®¢æˆ·ç«¯ï¼ˆè¿™é‡Œç®€å•é€‰æ‹©æ‰€æœ‰ï¼‰
            participants = list(self.learners.keys())
            if len(participants) < self.min_participants:
                raise ValueError(f"å‚ä¸å®¢æˆ·ç«¯æ•°é‡ä¸è¶³: {len(participants)} < {self.min_participants}")
                
            self.logger.info(f"é€‰æ‹©å‚ä¸å®¢æˆ·ç«¯: {participants}")
            
            # 2. å‡†å¤‡è®­ç»ƒé…ç½®
            training_config = {
                'round': round_num,
                'global_model': self.global_model,
                'epochs': config.get('epochs', 3) if config else 3,
                'learning_rate': config.get('learning_rate', 0.01) if config else 0.01
            }
            
            # 3. å¹¶è¡Œè®­ç»ƒæ‰€æœ‰å‚ä¸å®¢æˆ·ç«¯
            self.logger.info("å¼€å§‹å¹¶è¡Œè®­ç»ƒ...")
            train_results = await self.call_all_learners('train', training_config)
            
            # 4. è¿‡æ»¤æœ‰æ•ˆç»“æœ
            valid_results = {k: v for k, v in train_results.items() if v is not None}
            if len(valid_results) < self.min_participants:
                raise ValueError(f"æœ‰æ•ˆè®­ç»ƒç»“æœä¸è¶³: {len(valid_results)} < {self.min_participants}")
                
            # 5. èšåˆæ¨¡å‹
            self.logger.info(f"èšåˆ {len(valid_results)} ä¸ªæ¨¡å‹...")
            self.global_model = self._custom_aggregate(valid_results)
            
            # 6. å¹¿æ’­å…¨å±€æ¨¡å‹
            await self.broadcast_global_model(self.global_model)
            
            # 7. å¯é€‰ï¼šè¯„ä¼°å…¨å±€æ¨¡å‹
            eval_results = await self.call_all_learners('evaluate')
            avg_eval = self._calculate_avg_evaluation(eval_results)
            
            # 8. è®°å½•è½®æ¬¡ç»“æœ
            round_time = asyncio.get_event_loop().time() - start_time
            round_result = {
                'round': round_num,
                'participants': list(valid_results.keys()),
                'global_model': self.global_model.copy(),
                'training_metrics': self._aggregate_metrics(valid_results),
                'evaluation_metrics': avg_eval,
                'round_time': round_time
            }
            
            self.training_history.append(round_result)
            
            self.logger.info(f"=== ç¬¬ {round_num} è½®è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {round_time:.2f}s ===")
            return round_result
            
        except Exception as e:
            self.logger.exception(f"ç¬¬ {round_num} è½®è®­ç»ƒå¤±è´¥: {e}")
            raise
            
    def _custom_aggregate(self, results: Dict[str, Dict]) -> Dict:
        """è‡ªå®šä¹‰èšåˆç­–ç•¥"""
        if self.aggregation_strategy == 'weighted_avg':
            return self._weighted_average_aggregate(results)
        elif self.aggregation_strategy == 'simple_avg':
            return self._simple_average_aggregate(results)
        else:
            raise ValueError(f"æœªçŸ¥èšåˆç­–ç•¥: {self.aggregation_strategy}")
            
    def _weighted_average_aggregate(self, results: Dict[str, Dict]) -> Dict:
        """åŠ æƒå¹³å‡èšåˆ"""
        total_samples = sum(r['metrics']['samples'] for r in results.values())
        
        # åˆå§‹åŒ–èšåˆæ¨¡å‹
        aggregated = {}
        first_model = list(results.values())[0]['model']
        
        for key in first_model:
            if isinstance(first_model[key], list):
                aggregated[key] = [0.0] * len(first_model[key])
            else:
                aggregated[key] = 0.0
                
        # åŠ æƒèšåˆ
        for result in results.values():
            model = result['model']
            weight = result['metrics']['samples'] / total_samples
            
            for key in model:
                if isinstance(model[key], list):
                    for i in range(len(model[key])):
                        aggregated[key][i] += model[key][i] * weight
                else:
                    aggregated[key] += model[key] * weight
                    
        return aggregated
        
    def _simple_average_aggregate(self, results: Dict[str, Dict]) -> Dict:
        """ç®€å•å¹³å‡èšåˆ"""
        num_models = len(results)
        
        # åˆå§‹åŒ–èšåˆæ¨¡å‹
        aggregated = {}
        first_model = list(results.values())[0]['model']
        
        for key in first_model:
            if isinstance(first_model[key], list):
                aggregated[key] = [0.0] * len(first_model[key])
            else:
                aggregated[key] = 0.0
                
        # ç®€å•å¹³å‡
        for result in results.values():
            model = result['model']
            
            for key in model:
                if isinstance(model[key], list):
                    for i in range(len(model[key])):
                        aggregated[key][i] += model[key][i] / num_models
                else:
                    aggregated[key] += model[key] / num_models
                    
        return aggregated
        
    def _aggregate_metrics(self, results: Dict[str, Dict]) -> Dict:
        """èšåˆè®­ç»ƒæŒ‡æ ‡"""
        metrics = {}
        for result in results.values():
            for key, value in result['metrics'].items():
                if key not in metrics:
                    metrics[key] = []
                if isinstance(value, (int, float)):
                    metrics[key].append(value)
                    
        # è®¡ç®—å¹³å‡å€¼
        return {key: sum(values) / len(values) for key, values in metrics.items()}
        
    def _calculate_avg_evaluation(self, eval_results: Dict) -> Dict:
        """è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡"""
        if not eval_results:
            return {}
            
        valid_evals = [v for v in eval_results.values() if v is not None]
        if not valid_evals:
            return {}
            
        avg_metrics = {}
        for eval_result in valid_evals:
            for key, value in eval_result.items():
                if key not in avg_metrics:
                    avg_metrics[key] = []
                if isinstance(value, (int, float)):
                    avg_metrics[key].append(value)
                    
        return {key: sum(values) / len(values) for key, values in avg_metrics.items()}
        
    async def broadcast_global_model(self, model: Dict):
        """å¹¿æ’­å…¨å±€æ¨¡å‹"""
        await self.broadcast_to_learners({'global_model': model})
        self.logger.info("å…¨å±€æ¨¡å‹å·²å¹¿æ’­")
        
    def get_training_summary(self) -> Dict:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        if not self.training_history:
            return {'status': 'no_training'}
            
        latest = self.training_history[-1]
        return {
            'total_rounds': len(self.training_history),
            'latest_round': latest['round'],
            'latest_metrics': latest.get('training_metrics', {}),
            'latest_evaluation': latest.get('evaluation_metrics', {}),
            'global_model': self.global_model.copy()
        }


# ==================== æ¼”ç¤ºä¸»å‡½æ•° ====================

async def main():
    """æ¼”ç¤ºæ–°æ¶æ„çš„ä½¿ç”¨"""
    logger.info("ğŸš€ å¼€å§‹æ¼”ç¤ºæ–°è”é‚¦å­¦ä¹ æ¶æ„")
    
    # 0. åˆå§‹åŒ–è‡ªåŠ¨åˆ†æµæ—¥å¿—ç³»ç»Ÿ
    from datetime import datetime
    experiment_date = datetime.now().strftime("%Y%m%d-%H-%M-%S")
    auto_logger = setup_auto_logging(experiment_date)
    sys_logger = get_sys_logger()
    
    logger.info(f"ğŸ“‹ æ—¥å¿—ä¿å­˜åˆ°: logs/exp_{experiment_date}/")
    logger.info("  â”œâ”€â”€ comm/    # é€šä¿¡æ—¥å¿—")
    logger.info("  â”œâ”€â”€ train/   # è®­ç»ƒæ—¥å¿—")
    logger.info("  â””â”€â”€ sys/     # ç³»ç»Ÿæ—¥å¿—")
    
    # 1. åˆ›å»ºå†…å­˜ä¼ è¾“ï¼ˆæ¨¡æ‹Ÿåˆ†å¸ƒå¼ç¯å¢ƒï¼‰
    server_transport = MemoryTransport("server")
    
    # 2. åˆ›å»ºè®­ç»ƒå™¨
    trainer = CustomTrainer("server")
    trainer.transport = server_transport
    
    # 3. åˆ›å»ºå­¦ä¹ å™¨ï¼ˆæ¨¡æ‹Ÿ3ä¸ªå®¢æˆ·ç«¯ï¼‰
    learners = []
    client_transports = {}  # å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„ä¼ è¾“å®ä¾‹
    
    for i in range(3):
        learner_id = f"client_{i+1}"
        dataset_size = 50 + i * 25  # ä¸åŒçš„æ•°æ®é›†å¤§å°
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ›å»ºç‹¬ç«‹çš„ä¼ è¾“å®ä¾‹
        client_transport = MemoryTransport(learner_id)
        client_transports[learner_id] = client_transport
        
        # åˆ›å»ºå­¦ä¹ å™¨
        learner = CustomLearner(learner_id, dataset_size)
        learner.transport = client_transport
        learners.append(learner)
        
        # åœ¨è®­ç»ƒå™¨ä¸­æ·»åŠ å­¦ä¹ å™¨ä»£ç†ï¼ˆä½¿ç”¨å®¢æˆ·ç«¯çš„ä¼ è¾“å®ä¾‹ï¼‰
        proxy = trainer.add_learner(learner_id, client_transport)
        
        # è®¾ç½®å­¦ä¹ å™¨çš„RPCå¤„ç†ï¼ˆæ¨¡æ‹ŸçœŸå®çš„RPCè°ƒç”¨ï¼‰
        def create_mock_rpc_call(learner_obj):
            async def mock_rpc_call(method_name, *args, **kwargs):
                request = {
                    'method': method_name,
                    'args': args,
                    'kwargs': kwargs
                }
                result = await learner_obj.handle_rpc(request)
                return result.get('result') if 'result' in result else None
            return mock_rpc_call
            
        # é‡å†™ä»£ç†çš„RPCè°ƒç”¨æ–¹æ³•ï¼ˆåœ¨å®é™…ç¯å¢ƒä¸­è¿™ç”±ä¼ è¾“å±‚å¤„ç†ï¼‰
        proxy._rpc_call = create_mock_rpc_call(learner)
        
    logger.info(f"åˆ›å»ºäº† {len(learners)} ä¸ªå­¦ä¹ å™¨")
    
    # 4. æ¼”ç¤ºæ–¹æ³•å‘ç°
    logger.info("\nğŸ“¡ æ¼”ç¤ºåŠ¨æ€æ–¹æ³•å‘ç°:")
    for learner_id, proxy in trainer.learners.items():
        try:
            methods = await proxy.__get_methods__()
            logger.info(f"{learner_id} å¯ç”¨æ–¹æ³•: {list(methods.keys())}")
        except Exception as e:
            logger.warning(f"æ–¹æ³•å‘ç°å¤±è´¥: {e}")
            
    # 5. æ¼”ç¤ºæ•°æ®é›†ä¿¡æ¯è·å–
    logger.info("\nğŸ“Š è·å–æ•°æ®é›†ä¿¡æ¯:")
    dataset_info = await trainer.call_all_learners('get_dataset_info')
    for learner_id, info in dataset_info.items():
        if info:
            logger.info(f"{learner_id}: {info}")
            
    # 6. æ‰§è¡Œè”é‚¦è®­ç»ƒ
    logger.info("\nğŸ¯ å¼€å§‹è”é‚¦è®­ç»ƒæ¼”ç¤º:")
    
    training_config = {
        'epochs': 3,
        'learning_rate': 0.01
    }
    
    for round_num in range(1, 4):  # æ‰§è¡Œ3è½®è®­ç»ƒ
        try:
            result = await trainer.federated_round(round_num, training_config)
            
            # æ˜¾ç¤ºè½®æ¬¡ç»“æœ
            metrics = result['training_metrics']
            eval_metrics = result.get('evaluation_metrics', {})
            
            logger.info(f"ç¬¬ {round_num} è½®ç»“æœ:")
            logger.info(f"  å‚ä¸è€…: {len(result['participants'])}")
            logger.info(f"  å¹³å‡Loss: {metrics.get('loss', 0):.4f}")
            logger.info(f"  å¹³å‡Acc: {metrics.get('accuracy', 0):.4f}")
            if eval_metrics:
                logger.info(f"  æµ‹è¯•Acc: {eval_metrics.get('test_accuracy', 0):.4f}")
            logger.info(f"  ç”¨æ—¶: {result['round_time']:.2f}s")
            
            # ç¨å¾®ç­‰å¾…
            await asyncio.sleep(0.5)
            
        except Exception as e:
            logger.error(f"ç¬¬ {round_num} è½®è®­ç»ƒå¤±è´¥: {e}")
            break
            
    # 7. æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    logger.info("\nğŸ“ˆ è®­ç»ƒæ€»ç»“:")
    summary = trainer.get_training_summary()
    if 'total_rounds' in summary:
        logger.info(f"æ€»è½®æ•°: {summary['total_rounds']}")
        logger.info(f"æœ€ç»ˆæŒ‡æ ‡: {summary.get('latest_metrics', {})}")
        logger.info(f"æœ€ç»ˆæ¨¡å‹: {summary['global_model']}")
    else:
        logger.info(f"è®­ç»ƒçŠ¶æ€: {summary.get('status', 'unknown')}")
    
    logger.info("âœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
