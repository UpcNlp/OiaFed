"""
FedAvg MNIST è®­ç»ƒå™¨å®ç°
åŸºäºæ‰‹å†™æ•°å­—è¯†åˆ«çš„è”é‚¦å¹³å‡ç®—æ³•å®ç°
"""

import asyncio
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime
import copy

from fedcl.trainer.trainer import BaseTrainer, TrainingConfig
from fedcl.learner.proxy import LearnerProxy
from fedcl.types import ModelData, RoundResult
from fedcl.exceptions import TrainingError


class FedAvgMNISTTrainer(BaseTrainer):
    """åŸºäºMNISTçš„FedAvgè®­ç»ƒå™¨å®ç°"""
    
    def __init__(self,
                 trainer_id: str,
                 model_config: Optional[Dict[str, Any]] = None,
                 aggregation_config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–FedAvgè®­ç»ƒå™¨"""
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„learner_proxieså­—å…¸ï¼Œåç»­ä¼šåŠ¨æ€æ·»åŠ 
        learner_proxies = {}
        
        # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
        global_model = self._initialize_global_model(model_config)
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        training_config = TrainingConfig(
            max_rounds=10,
            min_clients=2,
            client_selection_ratio=1.0,
            round_timeout=300.0,
            convergence_threshold=0.001,
            patience=5
        )
        
        super().__init__(learner_proxies, global_model, training_config)
        
        self.trainer_id = trainer_id
        self.model_config = model_config or {}
        self.aggregation_config = aggregation_config or {"strategy": "fedavg", "weighted": True}
        
        # FedAvgç‰¹å®šé…ç½®
        self.local_epochs = 1  # æœ¬åœ°è®­ç»ƒè½®æ•°
        self.learning_rate = 0.01  # å­¦ä¹ ç‡
        self.batch_size = 32  # æ‰¹æ¬¡å¤§å°
        
        print(f"ğŸš€ FedAvg MNIST Trainer {trainer_id} initialized")
    
    def _initialize_global_model(self, model_config: Optional[Dict[str, Any]] = None) -> ModelData:
        """åˆå§‹åŒ–å…¨å±€æ¨¡å‹ï¼ˆç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œï¼‰"""
        # è¾“å…¥å±‚: 784 (28x28) -> éšè—å±‚: 128 -> è¾“å‡ºå±‚: 10
        np.random.seed(42)  # ç¡®ä¿å¯å¤ç°æ€§
        
        input_size = model_config.get("input_size", 784) if model_config else 784
        hidden_size = model_config.get("hidden_size", 128) if model_config else 128
        output_size = model_config.get("output_size", 10) if model_config else 10
        
        model = {
            "weights": {
                "W1": np.random.normal(0, 0.1, (input_size, hidden_size)).tolist(),
                "b1": np.zeros(hidden_size).tolist(),
                "W2": np.random.normal(0, 0.1, (hidden_size, output_size)).tolist(),
                "b2": np.zeros(output_size).tolist()
            },
            "model_version": 1,
            "architecture": "simple_nn",
            "input_shape": [input_size],
            "output_shape": [output_size],
            "created_at": datetime.now().isoformat()
        }
        
        print("ğŸ§  åˆå§‹åŒ–å…¨å±€æ¨¡å‹ (784->128->10)")
        return model
    
    def add_learner(self, client_id: str, learner):
        """æ·»åŠ å­¦ä¹ å™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å­˜å‚¨å­¦ä¹ å™¨å¼•ç”¨
        if not hasattr(self, '_direct_learners'):
            self._direct_learners = {}
        self._direct_learners[client_id] = learner
        print(f"ğŸ“± æ·»åŠ å®¢æˆ·ç«¯ {client_id}")
    
    async def get_current_model(self) -> ModelData:
        """è·å–å½“å‰å…¨å±€æ¨¡å‹"""
        return copy.deepcopy(self.global_model)
    
    async def train_round_with_learners(self, 
                                      round_num: int, 
                                      selected_clients: List[str]) -> RoundResult:
        """ç›´æ¥ä½¿ç”¨å­¦ä¹ å™¨è¿›è¡Œè®­ç»ƒè½®æ¬¡ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        print(f"\nğŸ”„ å¼€å§‹ç¬¬ {round_num + 1} è½®è®­ç»ƒï¼Œå‚ä¸å®¢æˆ·ç«¯: {selected_clients}")
        
        if not hasattr(self, '_direct_learners'):
            raise TrainingError("æ²¡æœ‰å¯ç”¨çš„å­¦ä¹ å™¨")
        
        # åˆ†å‘å…¨å±€æ¨¡å‹åˆ°é€‰ä¸­çš„å®¢æˆ·ç«¯
        training_results = []
        for client_id in selected_clients:
            if client_id not in self._direct_learners:
                print(f"âš ï¸  å®¢æˆ·ç«¯ {client_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            learner = self._direct_learners[client_id]
            
            # å‘é€å…¨å±€æ¨¡å‹
            await learner.set_local_model(self.global_model)
            
            # æ‰§è¡Œæœ¬åœ°è®­ç»ƒ
            training_params = {
                "global_model": self.global_model["weights"],
                "epochs": self.local_epochs,
                "learning_rate": self.learning_rate,
                "batch_size": self.batch_size,
                "round_num": round_num
            }
            
            try:
                result = await learner.train(training_params)
                training_results.append(result)
                print(f"   âœ… {client_id} è®­ç»ƒå®Œæˆ")
            except Exception as e:
                print(f"   âŒ {client_id} è®­ç»ƒå¤±è´¥: {e}")
        
        # èšåˆæ¨¡å‹æ›´æ–°
        if not training_results:
            raise TrainingError("æ²¡æœ‰æˆåŠŸçš„è®­ç»ƒç»“æœå¯ç”¨äºèšåˆ")
        
        aggregation_result = await self.aggregate_updates(training_results)
        
        # æ›´æ–°å…¨å±€æ¨¡å‹
        self.global_model["weights"] = aggregation_result["aggregated_weights"]
        self.global_model["model_version"] += 1
        
        return {
            "round_num": round_num,
            "participating_clients": selected_clients,
            "training_results": training_results,
            "aggregation_result": aggregation_result,
            "global_model_version": self.global_model["model_version"]
        }
    
    async def aggregate_updates(self, client_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆå®¢æˆ·ç«¯æ›´æ–°ï¼ˆFedAvgç®—æ³•ï¼‰"""
        if not client_results:
            raise TrainingError("æ²¡æœ‰å®¢æˆ·ç«¯ç»“æœç”¨äºèšåˆ")
        
        print(f"ğŸ”„ å¼€å§‹FedAvgèšåˆï¼Œå®¢æˆ·ç«¯æ•°é‡: {len(client_results)}")
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºæ ·æœ¬æ•°é‡ï¼‰
        total_samples = sum(result.get("samples_count", 1) for result in client_results)
        weights = [result.get("samples_count", 1) / total_samples for result in client_results]
        
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºæ¨¡æ¿
        first_model = client_results[0]["model_update"]
        aggregated_weights = {}
        
        # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡ŒåŠ æƒå¹³å‡
        for param_name in first_model.keys():
            if isinstance(first_model[param_name], list):
                # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œè®¡ç®—
                param_arrays = []
                for i, result in enumerate(client_results):
                    param_array = np.array(result["model_update"][param_name])
                    weighted_param = param_array * weights[i]
                    param_arrays.append(weighted_param)
                
                # æ±‚å’Œå¾—åˆ°èšåˆç»“æœ
                aggregated_param = sum(param_arrays)
                aggregated_weights[param_name] = aggregated_param.tolist()
            else:
                # å¤„ç†æ ‡é‡å‚æ•°
                aggregated_weights[param_name] = sum(
                    result["model_update"][param_name] * weights[i]
                    for i, result in enumerate(client_results)
                )
        
        # è®¡ç®—èšåˆç»Ÿè®¡ä¿¡æ¯
        avg_loss = sum(result.get("loss", 0.0) * weights[i] for i, result in enumerate(client_results))
        avg_accuracy = sum(result.get("accuracy", 0.0) * weights[i] for i, result in enumerate(client_results))
        
        aggregation_result = {
            "aggregated_weights": aggregated_weights,
            "average_loss": avg_loss,
            "average_accuracy": avg_accuracy,
            "participating_clients": len(client_results),
            "total_samples": total_samples,
            "aggregation_method": "fedavg"
        }
        
        print(f"   âœ… èšåˆå®Œæˆ: Avg Loss={avg_loss:.4f}, Avg Acc={avg_accuracy:.4f}")
        
        return aggregation_result
    
    # ==================== æŠ½è±¡æ–¹æ³•å®ç° ====================
    
    async def train_round(self, round_num: int, client_ids: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€è½®è”é‚¦è®­ç»ƒ"""
        if hasattr(self, '_direct_learners'):
            return await self.train_round_with_learners(round_num, client_ids)
        else:
            # ä½¿ç”¨ä»£ç†çš„æ ‡å‡†å®ç°
            return await super().train_round(round_num, client_ids)
    
    async def aggregate_models(self, client_results: Dict[str, Any]) -> Dict[str, Any]:
        """èšåˆå®¢æˆ·ç«¯æ¨¡å‹ï¼ˆFedAvgç®—æ³•ï¼‰"""
        if not client_results:
            raise TrainingError("æ²¡æœ‰å®¢æˆ·ç«¯ç»“æœç”¨äºèšåˆ")
        
        # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œä»¥å…¼å®¹æˆ‘ä»¬çš„aggregate_updatesæ–¹æ³•
        results_list = list(client_results.values())
        aggregation_result = await self.aggregate_updates(results_list)
        
        return aggregation_result["aggregated_weights"]
    
    async def evaluate_global_model(self) -> Dict[str, Any]:
        """è¯„ä¼°å…¨å±€æ¨¡å‹"""
        if hasattr(self, '_direct_learners'):
            # ä½¿ç”¨æ‰€æœ‰å®¢æˆ·ç«¯è¯„ä¼°å…¨å±€æ¨¡å‹
            eval_results = []
            
            for client_id, learner in self._direct_learners.items():
                try:
                    result = await learner.evaluate({
                        "model": self.global_model["weights"] if "weights" in self.global_model else self.global_model,
                        "test_data": True
                    })
                    eval_results.append(result)
                except Exception as e:
                    print(f"è¯„ä¼°å®¢æˆ·ç«¯ {client_id} å¤±è´¥: {e}")
            
            if eval_results:
                total_samples = sum(r.get("samples_count", 1) for r in eval_results)
                weighted_accuracy = sum(r.get("accuracy", 0.0) * r.get("samples_count", 1) for r in eval_results) / total_samples
                weighted_loss = sum(r.get("loss", 0.0) * r.get("samples_count", 1) for r in eval_results) / total_samples
                
                return {
                    "accuracy": weighted_accuracy,
                    "loss": weighted_loss,
                    "samples_count": total_samples,
                    "participants": len(eval_results)
                }
        
        return {"accuracy": 0.0, "loss": float('inf'), "participants": 0}
    
    def should_stop_training(self, round_num: int, round_result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        # åŸºäºè½®æ•°çš„åœæ­¢æ¡ä»¶
        max_rounds = getattr(self.training_config, 'max_rounds', 10) if self.training_config else 10
        if round_num >= max_rounds - 1:  # round_numæ˜¯ä»0å¼€å§‹çš„
            return True
        
        # åŸºäºæ”¶æ•›çš„åœæ­¢æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
        if "aggregation_result" in round_result:
            avg_accuracy = round_result["aggregation_result"].get("average_accuracy", 0.0)
            if avg_accuracy > 0.95:  # 95%å‡†ç¡®ç‡æ—¶åœæ­¢
                print(f"ğŸ¯ è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ {avg_accuracy:.4f}ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                return True
        
        return False
    
    async def train_round(self, round_num: int, client_ids: List[str]) -> RoundResult:
        """æ‰§è¡Œä¸€è½®FedAvgè®­ç»ƒ"""
        print(f"\nğŸ”„ å¼€å§‹ç¬¬ {round_num} è½® FedAvg è®­ç»ƒ")
        print(f"   å‚ä¸å®¢æˆ·ç«¯: {client_ids}")
        
        round_start_time = datetime.now()
        
        # å‡†å¤‡è®­ç»ƒå‚æ•°
        training_params = {
            "global_model": self.global_model["weights"],
            "epochs": self.local_epochs,
            "learning_rate": self.learning_rate,
            "batch_size": self.batch_size,
            "round_num": round_num
        }
        
        # å¹¶è¡Œæ‰§è¡Œå®¢æˆ·ç«¯è®­ç»ƒ
        client_results = []
        successful_clients = []
        failed_clients = []
        
        # åˆ›å»ºå¹¶å‘è®­ç»ƒä»»åŠ¡
        training_tasks = []
        for client_id in client_ids:
            if client_id in self.learner_proxies:
                proxy = self.learner_proxies[client_id]
                task = asyncio.create_task(
                    self._train_client(client_id, proxy, training_params)
                )
                training_tasks.append((client_id, task))
        
        # ç­‰å¾…æ‰€æœ‰å®¢æˆ·ç«¯è®­ç»ƒå®Œæˆ
        for client_id, task in training_tasks:
            try:
                result = await task
                if result["status"] == "success":
                    client_results.append(result)
                    successful_clients.append(client_id)
                    print(f"   âœ… å®¢æˆ·ç«¯ {client_id}: Loss={result['loss']:.4f}, Acc={result['accuracy']:.4f}")
                else:
                    failed_clients.append(client_id)
                    print(f"   âŒ å®¢æˆ·ç«¯ {client_id}: {result['error']}")
            except Exception as e:
                failed_clients.append(client_id)
                print(f"   âŒ å®¢æˆ·ç«¯ {client_id}: è®­ç»ƒå¼‚å¸¸ - {str(e)}")
        
        if not successful_clients:
            raise TrainingError(f"Round {round_num}: æ²¡æœ‰å®¢æˆ·ç«¯æˆåŠŸå®Œæˆè®­ç»ƒ")
        
        # æ‰§è¡ŒFedAvgèšåˆ
        print(f"ğŸ”— èšåˆ {len(successful_clients)} ä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹æ›´æ–°")
        aggregated_weights = await self._fedavg_aggregate(client_results)
        
        # æ›´æ–°å…¨å±€æ¨¡å‹
        self.global_model["weights"] = aggregated_weights
        self.global_model["model_version"] += 1
        
        # è®¡ç®—è½®æ¬¡æŒ‡æ ‡
        avg_loss = np.mean([r["loss"] for r in client_results])
        avg_accuracy = np.mean([r["accuracy"] for r in client_results])
        total_samples = sum([r["samples_count"] for r in client_results])
        
        round_end_time = datetime.now()
        round_duration = (round_end_time - round_start_time).total_seconds()
        
        round_result = {
            "round": round_num,
            "participants": client_ids,
            "successful_clients": successful_clients,
            "failed_clients": failed_clients,
            "aggregated_model": self.global_model,
            "round_metrics": {
                "avg_loss": float(avg_loss),
                "avg_accuracy": float(avg_accuracy),
                "total_samples": int(total_samples),
                "round_duration": round_duration,
                "convergence_metric": float(avg_loss)  # ä½¿ç”¨æŸå¤±ä½œä¸ºæ”¶æ•›æŒ‡æ ‡
            },
            "client_results": client_results
        }
        
        print(f"ğŸ“Š ç¬¬ {round_num} è½®å®Œæˆ: Loss={avg_loss:.4f}, Acc={avg_accuracy:.4f}, ç”¨æ—¶={round_duration:.2f}s")
        
        # æ›´æ–°è®­ç»ƒå†å²
        self.training_status.round_results.append(round_result)
        
        return round_result
    
    async def _train_client(self, client_id: str, proxy: LearnerProxy, training_params: Dict[str, Any]) -> Dict[str, Any]:
        """è®­ç»ƒå•ä¸ªå®¢æˆ·ç«¯"""
        try:
            # è°ƒç”¨å®¢æˆ·ç«¯è®­ç»ƒ
            result = await proxy.train(training_params)
            
            return {
                "client_id": client_id,
                "status": "success",
                "model_update": result.get("model_update", result.get("weights")),
                "loss": result.get("loss", 0.0),
                "accuracy": result.get("accuracy", 0.0),
                "samples_count": result.get("samples_count", result.get("samples", 0)),
                "training_time": result.get("training_time", 0.0)
            }
        except Exception as e:
            return {
                "client_id": client_id,
                "status": "failed",
                "error": str(e),
                "loss": float('inf'),
                "accuracy": 0.0,
                "samples_count": 0,
                "training_time": 0.0
            }
    
    async def _fedavg_aggregate(self, client_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """FedAvgç®—æ³•ï¼šæ ¹æ®æ ·æœ¬æ•°åŠ æƒå¹³å‡èšåˆæ¨¡å‹"""
        
        if not client_results:
            return self.global_model["weights"]
        
        # æå–æ¨¡å‹æ›´æ–°å’Œæ ·æœ¬æ•°
        model_updates = []
        sample_counts = []
        
        for result in client_results:
            if result["status"] == "success":
                model_updates.append(result["model_update"])
                sample_counts.append(result["samples_count"])
        
        if not model_updates:
            return self.global_model["weights"]
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºæ ·æœ¬æ•°çš„åŠ æƒå¹³å‡ï¼‰
        total_samples = sum(sample_counts)
        weights = [count / total_samples for count in sample_counts]
        
        print(f"   æ ·æœ¬æƒé‡: {[f'{w:.3f}' for w in weights]}")
        
        # æ‰§è¡ŒåŠ æƒèšåˆ
        aggregated = {}
        
        # è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç»“æ„ä½œä¸ºæ¨¡æ¿
        first_model = model_updates[0]
        
        for param_name in first_model:
            if isinstance(first_model[param_name], list):
                # å¤„ç†æƒé‡çŸ©é˜µ
                param_arrays = [np.array(update[param_name]) for update in model_updates]
                
                # åŠ æƒå¹³å‡
                weighted_sum = np.zeros_like(param_arrays[0])
                for weight, param_array in zip(weights, param_arrays):
                    weighted_sum += weight * param_array
                
                aggregated[param_name] = weighted_sum.tolist()
            else:
                # å¤„ç†æ ‡é‡å‚æ•°
                weighted_sum = sum(weight * update[param_name] for weight, update in zip(weights, model_updates))
                aggregated[param_name] = weighted_sum
        
        return aggregated
    
    def select_clients_for_round(self, round_num: int) -> List[str]:
        """é€‰æ‹©å‚ä¸å½“å‰è½®æ¬¡çš„å®¢æˆ·ç«¯"""
        available_clients = list(self.learner_proxies.keys())
        
        # è®¡ç®—è¦é€‰æ‹©çš„å®¢æˆ·ç«¯æ•°é‡
        num_clients = len(available_clients)
        num_selected = max(1, int(num_clients * self.training_config.client_selection_ratio))
        
        if self.training_config.client_selection == "all":
            return available_clients
        elif self.training_config.client_selection == "random":
            import random
            return random.sample(available_clients, min(num_selected, num_clients))
        else:
            # é»˜è®¤é€‰æ‹©æ‰€æœ‰å®¢æˆ·ç«¯
            return available_clients
    
    async def check_client_readiness(self, client_ids: List[str]) -> Dict[str, bool]:
        """æ£€æŸ¥å®¢æˆ·ç«¯å°±ç»ªçŠ¶æ€"""
        readiness = {}
        
        for client_id in client_ids:
            if client_id in self.learner_proxies:
                try:
                    proxy = self.learner_proxies[client_id]
                    # ç®€å•çš„pingæ£€æŸ¥
                    await asyncio.wait_for(proxy.ping(), timeout=5.0)
                    readiness[client_id] = True
                except:
                    readiness[client_id] = False
            else:
                readiness[client_id] = False
        
        return readiness
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        print("ğŸ”§ åˆå§‹åŒ– FedAvg MNIST Trainer")
        
        # æ£€æŸ¥å…¨å±€æ¨¡å‹
        if not self.global_model:
            self.global_model = self._initialize_global_model()
        
        # æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥
        available_clients = []
        for client_id, proxy in self.learner_proxies.items():
            try:
                await asyncio.wait_for(proxy.ping(), timeout=5.0)
                available_clients.append(client_id)
                print(f"   âœ… å®¢æˆ·ç«¯ {client_id} è¿æ¥æ­£å¸¸")
            except:
                print(f"   âŒ å®¢æˆ·ç«¯ {client_id} è¿æ¥å¤±è´¥")
        
        if len(available_clients) < self.training_config.min_clients:
            print(f"   âŒ å¯ç”¨å®¢æˆ·ç«¯æ•°é‡ä¸è¶³: {len(available_clients)} < {self.training_config.min_clients}")
            return False
        
        print(f"âœ… FedAvg Trainer åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨å®¢æˆ·ç«¯: {len(available_clients)}")
        return True
    
    def should_stop_training(self, round_num: int, round_result: RoundResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åœæ­¢è®­ç»ƒ"""
        
        # æ£€æŸ¥æœ€å¤§è½®æ•°
        if round_num >= self.training_config.max_rounds:
            print(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°: {round_num}")
            return True
        
        # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
        if len(self.training_status.round_results) >= 2:
            current_loss = round_result["round_metrics"]["avg_loss"]
            previous_loss = self.training_status.round_results[-2]["round_metrics"]["avg_loss"]
            
            # æ£€æŸ¥æŸå¤±æ”¹å–„
            loss_improvement = abs(previous_loss - current_loss)
            
            if loss_improvement < self.training_config.convergence_threshold:
                self.training_status.patience_counter += 1
                print(f"ğŸ” æŸå¤±æ”¹å–„å¾®å° ({loss_improvement:.6f} < {self.training_config.convergence_threshold})")
                
                if self.training_status.patience_counter >= self.training_config.patience:
                    print(f"ğŸ›‘ è®­ç»ƒæ”¶æ•›: è¿ç»­ {self.training_config.patience} è½®æ— æ˜æ˜¾æ”¹å–„")
                    return True
            else:
                self.training_status.patience_counter = 0
        
        # æ£€æŸ¥å‡†ç¡®ç‡é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
        current_accuracy = round_result["round_metrics"]["avg_accuracy"]
        if current_accuracy >= 0.99:  # 99%å‡†ç¡®ç‡
            print(f"ğŸ›‘ è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡: {current_accuracy:.4f}")
            return True
        
        return False
    
    async def evaluate_global_model(self) -> Dict[str, Any]:
        """è¯„ä¼°å…¨å±€æ¨¡å‹"""
        print("ğŸ” è¯„ä¼°å…¨å±€æ¨¡å‹")
        
        # åœ¨æ‰€æœ‰å®¢æˆ·ç«¯ä¸Šè¯„ä¼°å…¨å±€æ¨¡å‹
        evaluation_params = {
            "model": self.global_model["weights"],
            "test_data": True  # ä½¿ç”¨æµ‹è¯•æ•°æ®é›†
        }
        
        evaluation_results = []
        
        for client_id, proxy in self.learner_proxies.items():
            try:
                result = await proxy.evaluate(evaluation_params)
                evaluation_results.append({
                    "client_id": client_id,
                    "accuracy": result.get("accuracy", 0.0),
                    "loss": result.get("loss", float('inf')),
                    "samples_count": result.get("samples_count", 0)
                })
                print(f"   å®¢æˆ·ç«¯ {client_id}: Acc={result.get('accuracy', 0):.4f}")
            except Exception as e:
                print(f"   âŒ å®¢æˆ·ç«¯ {client_id} è¯„ä¼°å¤±è´¥: {e}")
        
        if not evaluation_results:
            return {
                "accuracy": 0.0,
                "loss": float('inf'),
                "samples_count": 0,
                "message": "æ‰€æœ‰å®¢æˆ·ç«¯è¯„ä¼°å¤±è´¥"
            }
        
        # èšåˆè¯„ä¼°ç»“æœ
        total_samples = sum(r["samples_count"] for r in evaluation_results)
        if total_samples > 0:
            weighted_accuracy = sum(r["accuracy"] * r["samples_count"] for r in evaluation_results) / total_samples
            weighted_loss = sum(r["loss"] * r["samples_count"] for r in evaluation_results) / total_samples
        else:
            weighted_accuracy = np.mean([r["accuracy"] for r in evaluation_results])
            weighted_loss = np.mean([r["loss"] for r in evaluation_results])
        
        global_eval_result = {
            "accuracy": float(weighted_accuracy),
            "loss": float(weighted_loss),
            "samples_count": int(total_samples),
            "client_count": len(evaluation_results),
            "evaluation_time": datetime.now().isoformat()
        }
        
        print(f"ğŸŒ å…¨å±€æ¨¡å‹è¯„ä¼°: Acc={weighted_accuracy:.4f}, Loss={weighted_loss:.4f}")
        
        return global_eval_result
