"""
MNIST å­¦ä¹ å™¨å®ç°
åŸºäºæ‰‹å†™æ•°å­—è¯†åˆ«çš„å®¢æˆ·ç«¯å­¦ä¹ å™¨
"""

import asyncio
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import copy

from fedcl.learner.base_learner import BaseLearner
from fedcl.types import TrainingResult, EvaluationResult, ModelData
from fedcl.exceptions import TrainingError


class MNISTLearner(BaseLearner):
    """åŸºäºMNISTçš„å®¢æˆ·ç«¯å­¦ä¹ å™¨å®ç°"""
    
    def __init__(self,
                 client_id: str,
                 local_data: Dict[str, Any] = None,
                 model_config: Dict[str, Any] = None,
                 training_config: Dict[str, Any] = None):
        """åˆå§‹åŒ–MNISTå­¦ä¹ å™¨"""
        super().__init__(client_id, local_data, model_config, training_config)
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = training_config.get("learning_rate", 0.01) if training_config else 0.01
        self.batch_size = training_config.get("batch_size", 32) if training_config else 32
        
        # ç”Ÿæˆæˆ–åŠ è½½æœ¬åœ°æ•°æ®
        if local_data is None:
            self.local_data = self._generate_synthetic_data()
        else:
            self.local_data = local_data
        
        # æœ¬åœ°æ¨¡å‹ï¼ˆåˆå§‹ä¸ºç©ºï¼‰
        self._local_model = None
        
        print(f"ğŸ“± MNIST å­¦ä¹ å™¨ {client_id} åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®é›†å¤§å°: {len(self.local_data['X_train'])} è®­ç»ƒæ ·æœ¬, {len(self.local_data['X_test'])} æµ‹è¯•æ ·æœ¬")
        print(f"   æ•°æ®åˆ†å¸ƒ: {self._get_label_distribution()}")
    
    def _generate_synthetic_data(self) -> Dict[str, Any]:
        """ç”ŸæˆåˆæˆMNISTæ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰"""
        np.random.seed(hash(self.client_id) % 2**32)  # åŸºäºå®¢æˆ·ç«¯IDçš„éšæœºç§å­
        
        # æ¨¡æ‹Ÿä¸åŒå®¢æˆ·ç«¯çš„æ•°æ®å¼‚æ„æ€§
        try:
            # å°è¯•ä»å®¢æˆ·ç«¯IDä¸­æå–æ•°å­—
            if '_' in self.client_id:
                client_num = int(self.client_id.split('_')[-1])
            else:
                # å¦‚æœæ²¡æœ‰ä¸‹åˆ’çº¿ï¼Œä½¿ç”¨å“ˆå¸Œå€¼
                client_num = abs(hash(self.client_id)) % 100
        except ValueError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å“ˆå¸Œå€¼
            client_num = abs(hash(self.client_id)) % 100
        
        # ä¸åŒå®¢æˆ·ç«¯åå‘ä¸åŒçš„æ•°å­—ç±»åˆ«ï¼ˆNon-IIDåˆ†å¸ƒï¼‰
        preferred_classes = [(client_num * 3 + i) % 10 for i in range(3)]  # æ¯ä¸ªå®¢æˆ·ç«¯åå‘3ä¸ªæ•°å­—
        other_classes = [i for i in range(10) if i not in preferred_classes]
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        train_samples = np.random.randint(200, 400)  # å‡å°‘æ ·æœ¬é‡ï¼š200-400ä¸ªè®­ç»ƒæ ·æœ¬
        test_samples = np.random.randint(50, 100)     # å‡å°‘æ ·æœ¬é‡ï¼š50-100ä¸ªæµ‹è¯•æ ·æœ¬
        
        # ç”Ÿæˆåå‘ç‰¹å®šç±»åˆ«çš„æ•°æ®
        X_train = []
        y_train = []
        X_test = []
        y_test = []
        
        # è®­ç»ƒæ•°æ®ï¼š70%æ¥è‡ªåå¥½ç±»åˆ«ï¼Œ30%æ¥è‡ªå…¶ä»–ç±»åˆ«
        for i in range(train_samples):
            if np.random.random() < 0.7:  # 70%æ¦‚ç‡é€‰æ‹©åå¥½ç±»åˆ«
                label = np.random.choice(preferred_classes)
            else:  # 30%æ¦‚ç‡é€‰æ‹©å…¶ä»–ç±»åˆ«
                label = np.random.choice(other_classes)
            
            # ç”Ÿæˆè¯¥ç±»åˆ«çš„åˆæˆå›¾åƒç‰¹å¾ï¼ˆ28x28=784ç»´ï¼‰
            image_features = self._generate_digit_features(label)
            X_train.append(image_features)
            y_train.append(label)
        
        # æµ‹è¯•æ•°æ®ï¼šç›¸å¯¹å‡è¡¡åˆ†å¸ƒ
        for i in range(test_samples):
            label = np.random.randint(0, 10)
            image_features = self._generate_digit_features(label)
            X_test.append(image_features)
            y_test.append(label)
        
        return {
            "X_train": np.array(X_train, dtype=np.float32),
            "y_train": np.array(y_train, dtype=np.int32),
            "X_test": np.array(X_test, dtype=np.float32),
            "y_test": np.array(y_test, dtype=np.int32),
            "num_classes": 10,
            "input_shape": [784],
            "preferred_classes": preferred_classes
        }
    
    def _generate_digit_features(self, digit: int) -> np.ndarray:
        """ä¸ºæŒ‡å®šæ•°å­—ç”Ÿæˆåˆæˆç‰¹å¾å‘é‡"""
        # ä¸ºä¸åŒæ•°å­—ç”Ÿæˆä¸åŒçš„ç‰¹å¾æ¨¡å¼
        base_pattern = np.random.normal(digit * 0.1, 0.5, 784)  # åŸºç¡€æ¨¡å¼
        
        # æ·»åŠ ä¸€äº›æ•°å­—ç‰¹å®šçš„ç‰¹å¾
        if digit == 0:  # åœ†å½¢ç‰¹å¾
            base_pattern[100:150] += 1.0
            base_pattern[600:650] += 1.0
        elif digit == 1:  # å‚ç›´çº¿ç‰¹å¾
            base_pattern[::10] += 0.8
        elif digit == 2:  # æ›²çº¿ç‰¹å¾
            base_pattern[200:300] += 0.6
        # ... å¯ä»¥æ·»åŠ æ›´å¤šæ•°å­—ç‰¹å®šç‰¹å¾
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 0.1, 784)
        features = base_pattern + noise
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        features = (features - features.min()) / (features.max() - features.min() + 1e-8)
        
        return features.astype(np.float32)
    
    def _get_label_distribution(self) -> Dict[int, int]:
        """è·å–æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡"""
        unique, counts = np.unique(self.local_data["y_train"], return_counts=True)
        return {int(label): int(count) for label, count in zip(unique, counts)}
    
    async def train(self, training_params: Dict[str, Any]) -> TrainingResult:
        """æ‰§è¡Œæœ¬åœ°è®­ç»ƒ"""
        async with self._lock:
            print(f"ğŸ”„ å®¢æˆ·ç«¯ {self.client_id} å¼€å§‹æœ¬åœ°è®­ç»ƒ")
            
            # è§£æè®­ç»ƒå‚æ•°
            global_model = training_params.get("global_model", {})
            epochs = training_params.get("epochs", 1)
            learning_rate = training_params.get("learning_rate", self.learning_rate)
            batch_size = training_params.get("batch_size", self.batch_size)
            round_num = training_params.get("round_num", 0)
            
            start_time = datetime.now()
            
            try:
                # åˆå§‹åŒ–æˆ–æ›´æ–°æœ¬åœ°æ¨¡å‹
                if global_model:
                    self._local_model = copy.deepcopy(global_model)
                elif self._local_model is None:
                    # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
                    self._local_model = self._initialize_local_model()
                
                # æ‰§è¡Œæœ¬åœ°SGDè®­ç»ƒ
                training_loss, training_accuracy = await self._local_sgd_training(
                    epochs, learning_rate, batch_size
                )
                
                # è®¡ç®—æ¨¡å‹æ›´æ–°ï¼ˆå‘é€ç»™æœåŠ¡å™¨çš„æ˜¯æ›´æ–°åçš„å®Œæ•´æ¨¡å‹ï¼‰
                model_update = copy.deepcopy(self._local_model)
                
                end_time = datetime.now()
                training_time = (end_time - start_time).total_seconds()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.training_count += 1
                self.last_training_time = end_time
                
                result = {
                    "model_update": model_update,
                    "weights": model_update,  # å…¼å®¹æ€§
                    "loss": float(training_loss),
                    "accuracy": float(training_accuracy),
                    "samples_count": len(self.local_data["X_train"]),
                    "samples": len(self.local_data["X_train"]),  # å…¼å®¹æ€§
                    "training_time": training_time,
                    "epochs_completed": epochs,
                    "client_id": self.client_id,
                    "round_num": round_num
                }
                
                print(f"   âœ… è®­ç»ƒå®Œæˆ: Loss={training_loss:.4f}, Acc={training_accuracy:.4f}")
                
                return result
                
            except Exception as e:
                error_msg = f"å®¢æˆ·ç«¯ {self.client_id} è®­ç»ƒå¤±è´¥: {str(e)}"
                print(f"   âŒ {error_msg}")
                raise TrainingError(error_msg)
    
    async def _local_sgd_training(self, epochs: int, learning_rate: float, batch_size: int) -> Tuple[float, float]:
        """æ‰§è¡Œæœ¬åœ°SGDè®­ç»ƒ"""
        
        X_train = self.local_data["X_train"]
        y_train = self.local_data["y_train"]
        
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        
        for epoch in range(epochs):
            # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
            indices = np.random.permutation(len(X_train))
            
            epoch_loss = 0.0
            epoch_accuracy = 0.0
            epoch_batches = 0
            
            # æ‰¹æ¬¡è®­ç»ƒ
            for i in range(0, len(X_train), batch_size):
                batch_indices = indices[i:i + batch_size]
                X_batch = X_train[batch_indices]
                y_batch = y_train[batch_indices]
                
                # å‰å‘ä¼ æ’­
                predictions, loss = self._forward_pass(X_batch, y_batch)
                
                # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
                await self._backward_pass(X_batch, y_batch, predictions, learning_rate)
                
                # è®¡ç®—å‡†ç¡®ç‡
                batch_accuracy = np.mean(np.argmax(predictions, axis=1) == y_batch)
                
                epoch_loss += loss
                epoch_accuracy += batch_accuracy
                epoch_batches += 1
                
                # æ¨¡æ‹Ÿå¼‚æ­¥è®­ç»ƒï¼ˆè®©å‡ºæ§åˆ¶æƒï¼‰
                if epoch_batches % 5 == 0:  # å‡å°‘é¢‘ç‡
                    await asyncio.sleep(0.001)
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            if epoch_batches > 0:
                epoch_loss /= epoch_batches
                epoch_accuracy /= epoch_batches
                
                total_loss += epoch_loss
                total_accuracy += epoch_accuracy
                num_batches += 1
        
        # è¿”å›å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        avg_accuracy = total_accuracy / num_batches if num_batches > 0 else 0.0
        
        return avg_loss, avg_accuracy
    
    def _forward_pass(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, float]:
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€å±‚ï¼š784 -> 128
        z1 = np.dot(X, np.array(self._local_model["W1"])) + np.array(self._local_model["b1"])
        a1 = self._relu(z1)
        
        # ç¬¬äºŒå±‚ï¼š128 -> 10
        z2 = np.dot(a1, np.array(self._local_model["W2"])) + np.array(self._local_model["b2"])
        a2 = self._softmax(z2)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = self._cross_entropy_loss(a2, y)
        
        # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
        self._forward_cache = {
            "X": X, "z1": z1, "a1": a1, "z2": z2, "a2": a2, "y": y
        }
        
        return a2, loss
    
    async def _backward_pass(self, X: np.ndarray, y: np.ndarray, predictions: np.ndarray, learning_rate: float):
        """åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°"""
        m = X.shape[0]  # æ‰¹æ¬¡å¤§å°
        
        # ä»ç¼“å­˜è·å–å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
        a1 = self._forward_cache["a1"]
        a2 = predictions
        
        # è®¡ç®—æ¢¯åº¦
        # è¾“å‡ºå±‚æ¢¯åº¦
        y_one_hot = np.eye(10)[y]
        dz2 = a2 - y_one_hot
        dW2 = (1/m) * np.dot(a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0)
        
        # éšè—å±‚æ¢¯åº¦
        da1 = np.dot(dz2, np.array(self._local_model["W2"]).T)
        dz1 = da1 * self._relu_derivative(self._forward_cache["z1"])
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0)
        
        # æ›´æ–°å‚æ•°
        W1 = np.array(self._local_model["W1"])
        b1 = np.array(self._local_model["b1"])
        W2 = np.array(self._local_model["W2"])
        b2 = np.array(self._local_model["b2"])
        
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        self._local_model["W1"] = W1.tolist()
        self._local_model["b1"] = b1.tolist()
        self._local_model["W2"] = W2.tolist()
        self._local_model["b2"] = b2.tolist()
        
        # æ¨¡æ‹Ÿå¼‚æ­¥è®¡ç®—
        await asyncio.sleep(0.0001)  # å‡å°‘ç¡çœ æ—¶é—´
    
    def _relu(self, x: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def _relu_derivative(self, x: np.ndarray) -> np.ndarray:
        """ReLUå¯¼æ•°"""
        return (x > 0).astype(np.float32)
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def _cross_entropy_loss(self, predictions: np.ndarray, labels: np.ndarray) -> float:
        """äº¤å‰ç†µæŸå¤±å‡½æ•°"""
        m = predictions.shape[0]
        # é¿å…log(0)
        predictions = np.clip(predictions, 1e-15, 1 - 1e-15)
        
        # è®¡ç®—äº¤å‰ç†µ
        log_likelihood = -np.log(predictions[range(m), labels])
        loss = np.mean(log_likelihood)
        
        return float(loss)
    
    async def evaluate(self, evaluation_params: Dict[str, Any]) -> EvaluationResult:
        """æ‰§è¡Œæœ¬åœ°è¯„ä¼°"""
        async with self._lock:
            print(f"ğŸ” å®¢æˆ·ç«¯ {self.client_id} å¼€å§‹è¯„ä¼°")
            
            # è§£æè¯„ä¼°å‚æ•°
            model = evaluation_params.get("model", self._local_model)
            use_test_data = evaluation_params.get("test_data", True)
            
            start_time = datetime.now()
            
            try:
                # é€‰æ‹©è¯„ä¼°æ•°æ®é›†
                if use_test_data:
                    X_eval = self.local_data["X_test"]
                    y_eval = self.local_data["y_test"]
                else:
                    X_eval = self.local_data["X_train"]
                    y_eval = self.local_data["y_train"]
                
                # ä¸´æ—¶è®¾ç½®æ¨¡å‹ç”¨äºè¯„ä¼°
                original_model = self._local_model
                if model:
                    self._local_model = copy.deepcopy(model)
                
                # æ‰§è¡Œè¯„ä¼°
                predictions, loss = self._forward_pass(X_eval, y_eval)
                accuracy = np.mean(np.argmax(predictions, axis=1) == y_eval)
                
                # æ¢å¤åŸæ¨¡å‹
                self._local_model = original_model
                
                end_time = datetime.now()
                evaluation_time = (end_time - start_time).total_seconds()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.evaluation_count += 1
                self.last_evaluation_time = end_time
                
                result = {
                    "accuracy": float(accuracy),
                    "loss": float(loss),
                    "samples_count": len(X_eval),
                    "evaluation_time": evaluation_time,
                    "client_id": self.client_id
                }
                
                print(f"   âœ… è¯„ä¼°å®Œæˆ: Loss={loss:.4f}, Acc={accuracy:.4f}")
                
                return result
                
            except Exception as e:
                error_msg = f"å®¢æˆ·ç«¯ {self.client_id} è¯„ä¼°å¤±è´¥: {str(e)}"
                print(f"   âŒ {error_msg}")
                raise TrainingError(error_msg)
    
    async def get_local_model(self) -> ModelData:
        """è·å–æœ¬åœ°æ¨¡å‹å‚æ•°"""
        if self._local_model is None:
            self._local_model = self._initialize_local_model()
        
        return {
            "weights": copy.deepcopy(self._local_model),
            "model_version": getattr(self, '_model_version', 1),
            "client_id": self.client_id,
            "last_updated": datetime.now().isoformat()
        }
    
    async def set_local_model(self, model_data: ModelData) -> bool:
        """è®¾ç½®æœ¬åœ°æ¨¡å‹å‚æ•°"""
        try:
            if "weights" in model_data:
                self._local_model = copy.deepcopy(model_data["weights"])
            else:
                self._local_model = copy.deepcopy(model_data)
            
            self._model_version = model_data.get("model_version", 1)
            
            print(f"ğŸ“¥ å®¢æˆ·ç«¯ {self.client_id} æ›´æ–°æœ¬åœ°æ¨¡å‹")
            return True
            
        except Exception as e:
            print(f"âŒ å®¢æˆ·ç«¯ {self.client_id} æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def _initialize_local_model(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        # ä½¿ç”¨å®¢æˆ·ç«¯ç‰¹å®šçš„éšæœºç§å­
        np.random.seed(hash(self.client_id) % 2**32)
        
        model = {
            "W1": np.random.normal(0, 0.1, (784, 128)).tolist(),
            "b1": np.zeros(128).tolist(),
            "W2": np.random.normal(0, 0.1, (128, 10)).tolist(),
            "b2": np.zeros(10).tolist()
        }
        
        return model
    
    def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_samples": len(self.local_data["X_train"]),
            "test_samples": len(self.local_data["X_test"]),
            "num_classes": self.local_data["num_classes"],
            "input_shape": self.local_data["input_shape"],
            "label_distribution": self._get_label_distribution(),
            "preferred_classes": self.local_data["preferred_classes"],
            "data_type": "synthetic_mnist",
            "available": True
        }
