#!/usr/bin/env python3
"""
è¯„ä¼°é…ç½®æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–°çš„é…ç½®æ ¼å¼ï¼šè§£å†³è¯„ä¼°å™¨å’Œæµ‹è¯•æ•°æ®é›†çš„æ˜ å°„é—®é¢˜
æ”¯æŒå¤šç§é…ç½®æ–¹å¼ï¼šå•ä¸ªã€ä»»åŠ¡åˆ—è¡¨ã€æ˜ å°„ã€ç¬›å¡å°”ç§¯
"""

import yaml
from typing import Dict, Any


def test_evaluation_config():
    """æµ‹è¯•å¤šç§è¯„ä¼°é…ç½®æ–¹å¼"""
    
    # ç¤ºä¾‹é…ç½® - åŒ…å«æ‰€æœ‰é…ç½®æ–¹å¼
    config = {
        "test_datas": {
            "test_data": {
                "type": "StandardDataLoader",
                "dataset": "mnist_test",
                "batch_size": 100
            },
            "validation_data": {
                "type": "StandardDataLoader",
                "dataset": "mnist_validation", 
                "batch_size": 100
            },
            "challenge_data": {
                "type": "StandardDataLoader",
                "dataset": "mnist_challenge", 
                "batch_size": 50
            }
        },
        "evaluators": {
            "accuracy_evaluator": {
                "type": "accuracy_evaluator",
                "metrics": ["accuracy"]
            },
            "loss_evaluator": {
                "type": "loss_evaluator",
                "metrics": ["loss"]
            },
            "comprehensive_evaluator": {
                "type": "comprehensive_evaluator",
                "metrics": ["accuracy", "precision", "recall"]
            }
        },
        "evaluation": {
            "learners": {
                # æ–¹å¼1: å•ä¸ªè¯„ä¼°å™¨å’Œæ•°æ®é›†
                "learner_1": {
                    "evaluator": "accuracy_evaluator",
                    "test_dataset": "test_data"
                },
                # æ–¹å¼2: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ - ç²¾ç¡®æ§åˆ¶
                "learner_2": {
                    "evaluation_tasks": [
                        {"evaluator": "accuracy_evaluator", "test_dataset": "test_data", "name": "test_accuracy"},
                        {"evaluator": "accuracy_evaluator", "test_dataset": "validation_data", "name": "val_accuracy"},
                        {"evaluator": "loss_evaluator", "test_dataset": "test_data", "name": "test_loss"}
                    ]
                },
                # æ–¹å¼3: è¯„ä¼°å™¨æ˜ å°„
                "learner_3": {
                    "evaluator_mapping": {
                        "accuracy_evaluator": ["test_data", "validation_data"],
                        "loss_evaluator": ["test_data"],
                        "comprehensive_evaluator": ["challenge_data"]
                    }
                },
                # æ–¹å¼4: å‘åå…¼å®¹ - ç¬›å¡å°”ç§¯
                "learner_4": {
                    "evaluators": ["accuracy_evaluator", "loss_evaluator"],
                    "test_datasets": ["test_data", "validation_data"]
                }
            }
        }
    }
    
    # æ¨¡æ‹Ÿé…ç½®è§£æå’Œè¯„ä¼°ä»»åŠ¡ç”Ÿæˆ
    def parse_evaluation_config(learner_id, learner_eval_config):
        """è§£æè¯„ä¼°é…ç½®å¹¶ç”Ÿæˆè¯„ä¼°ä»»åŠ¡"""
        evaluation_tasks = []
        
        if "evaluation_tasks" in learner_eval_config:
            # æ–¹å¼2: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
            evaluation_tasks = learner_eval_config["evaluation_tasks"]
            print(f"  æ–¹å¼2 - è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼š{len(evaluation_tasks)} ä¸ªä»»åŠ¡")
            
        elif "evaluator_mapping" in learner_eval_config:
            # æ–¹å¼3: è¯„ä¼°å™¨æ˜ å°„
            evaluator_mapping = learner_eval_config["evaluator_mapping"]
            for evaluator_id, dataset_list in evaluator_mapping.items():
                if isinstance(dataset_list, str):
                    dataset_list = [dataset_list]
                
                for dataset_id in dataset_list:
                    evaluation_tasks.append({
                        "evaluator": evaluator_id,
                        "test_dataset": dataset_id,
                        "name": f"{evaluator_id}_{dataset_id}"
                    })
            print(f"  æ–¹å¼3 - è¯„ä¼°å™¨æ˜ å°„ï¼šç”Ÿæˆ {len(evaluation_tasks)} ä¸ªä»»åŠ¡")
            
        elif "evaluator" in learner_eval_config and "test_dataset" in learner_eval_config:
            # æ–¹å¼1: å•ä¸ªè¯„ä¼°å™¨å’Œæ•°æ®é›†
            evaluation_tasks = [{
                "evaluator": learner_eval_config["evaluator"],
                "test_dataset": learner_eval_config["test_dataset"],
                "name": f"{learner_eval_config['evaluator']}_{learner_eval_config['test_dataset']}"
            }]
            print(f"  æ–¹å¼1 - å•ä¸ªé…ç½®ï¼š1 ä¸ªä»»åŠ¡")
            
        elif "evaluators" in learner_eval_config and "test_datasets" in learner_eval_config:
            # æ–¹å¼4: å‘åå…¼å®¹ - ç¬›å¡å°”ç§¯
            evaluators = learner_eval_config["evaluators"]
            test_datasets = learner_eval_config["test_datasets"]
            
            for evaluator_id in evaluators:
                for dataset_id in test_datasets:
                    evaluation_tasks.append({
                        "evaluator": evaluator_id,
                        "test_dataset": dataset_id,
                        "name": f"{evaluator_id}_{dataset_id}"
                    })
            print(f"  æ–¹å¼4 - ç¬›å¡å°”ç§¯ï¼šç”Ÿæˆ {len(evaluation_tasks)} ä¸ªä»»åŠ¡")
        
        return evaluation_tasks
    
    print("=== å¤šç§è¯„ä¼°é…ç½®æ–¹å¼æµ‹è¯• ===")
    
    for learner_id, learner_config in config.get("evaluation", {}).get("learners", {}).items():
        print(f"\n{learner_id}:")
        tasks = parse_evaluation_config(learner_id, learner_config)
        
        for i, task in enumerate(tasks, 1):
            print(f"    ä»»åŠ¡{i}: {task['evaluator']} + {task['test_dataset']} -> {task['name']}")
    
    print("\nâœ… é…ç½®è§£ææµ‹è¯•é€šè¿‡!")


def test_yaml_config():
    """æµ‹è¯•YAMLé…ç½®æ–‡ä»¶"""
    
    yaml_config = """
# æ”¹è¿›çš„è¯„ä¼°é…ç½® - è§£å†³è¯„ä¼°å™¨å’Œæ•°æ®é›†æ˜ å°„é—®é¢˜
test_datas:
  test_data:
    type: "StandardDataLoader"
    dataset: "mnist_test"
    batch_size: 100
    
  validation_data:
    type: "StandardDataLoader"
    dataset: "mnist_validation"
    batch_size: 100

evaluators:
  accuracy_evaluator:
    type: "accuracy_evaluator"
    metrics: ["accuracy"]
    
  loss_evaluator:
    type: "loss_evaluator"
    metrics: ["loss"]

evaluation:
  learners:
    # ç²¾ç¡®æ§åˆ¶æ¯ä¸ªè¯„ä¼°ä»»åŠ¡
    learner_precise:
      evaluation_tasks:
        - evaluator: "accuracy_evaluator"
          test_dataset: "test_data"
          name: "test_accuracy"
        - evaluator: "loss_evaluator"
          test_dataset: "validation_data"
          name: "validation_loss"
          
    # è¯„ä¼°å™¨æ˜ å°„æ–¹å¼
    learner_mapping:
      evaluator_mapping:
        accuracy_evaluator: ["test_data", "validation_data"]
        loss_evaluator: ["test_data"]
"""
    
    config = yaml.safe_load(yaml_config)
    
    print("=== YAMLé…ç½®æµ‹è¯• ===")
    print("test_datas:", list(config.get("test_datas", {}).keys()))
    print("evaluators:", list(config.get("evaluators", {}).keys()))
    
    for learner_id, learner_config in config.get("evaluation", {}).get("learners", {}).items():
        print(f"\n{learner_id}:")
        if "evaluation_tasks" in learner_config:
            print("  é…ç½®æ–¹å¼: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨")
            for task in learner_config["evaluation_tasks"]:
                print(f"    {task['name']}: {task['evaluator']} + {task['test_dataset']}")
        elif "evaluator_mapping" in learner_config:
            print("  é…ç½®æ–¹å¼: è¯„ä¼°å™¨æ˜ å°„")
            for evaluator, datasets in learner_config["evaluator_mapping"].items():
                print(f"    {evaluator}: {datasets}")
    
    print("\nâœ… YAMLé…ç½®æµ‹è¯•é€šè¿‡!")


def test_task_generation():
    """æµ‹è¯•è¯„ä¼°ä»»åŠ¡ç”Ÿæˆé€»è¾‘"""
    
    print("=== è¯„ä¼°ä»»åŠ¡ç”Ÿæˆæµ‹è¯• ===")
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "name": "å•ä¸ªè¯„ä¼°å™¨+å•ä¸ªæ•°æ®é›†",
            "config": {
                "evaluator": "accuracy_evaluator",
                "test_dataset": "test_data"
            },
            "expected_tasks": 1
        },
        {
            "name": "è¯„ä¼°ä»»åŠ¡åˆ—è¡¨",
            "config": {
                "evaluation_tasks": [
                    {"evaluator": "accuracy_evaluator", "test_dataset": "test_data"},
                    {"evaluator": "loss_evaluator", "test_dataset": "validation_data"}
                ]
            },
            "expected_tasks": 2
        },
        {
            "name": "è¯„ä¼°å™¨æ˜ å°„",
            "config": {
                "evaluator_mapping": {
                    "accuracy_evaluator": ["test_data", "validation_data"],
                    "loss_evaluator": ["test_data"]
                }
            },
            "expected_tasks": 3
        },
        {
            "name": "ç¬›å¡å°”ç§¯ï¼ˆå‘åå…¼å®¹ï¼‰",
            "config": {
                "evaluators": ["accuracy_evaluator", "loss_evaluator"],
                "test_datasets": ["test_data", "validation_data"]
            },
            "expected_tasks": 4  # 2 * 2 = 4
        }
    ]
    
    for case in test_cases:
        print(f"\næµ‹è¯•æ¡ˆä¾‹: {case['name']}")
        
        # ç®€åŒ–çš„ä»»åŠ¡ç”Ÿæˆé€»è¾‘
        tasks = []
        config = case["config"]
        
        if "evaluation_tasks" in config:
            tasks = config["evaluation_tasks"]
        elif "evaluator_mapping" in config:
            for evaluator_id, dataset_list in config["evaluator_mapping"].items():
                if isinstance(dataset_list, str):
                    dataset_list = [dataset_list]
                for dataset_id in dataset_list:
                    tasks.append({"evaluator": evaluator_id, "test_dataset": dataset_id})
        elif "evaluator" in config and "test_dataset" in config:
            tasks = [{"evaluator": config["evaluator"], "test_dataset": config["test_dataset"]}]
        elif "evaluators" in config and "test_datasets" in config:
            for evaluator_id in config["evaluators"]:
                for dataset_id in config["test_datasets"]:
                    tasks.append({"evaluator": evaluator_id, "test_dataset": dataset_id})
        
        print(f"  ç”Ÿæˆä»»åŠ¡æ•°: {len(tasks)} (é¢„æœŸ: {case['expected_tasks']})")
        for i, task in enumerate(tasks, 1):
            print(f"    ä»»åŠ¡{i}: {task['evaluator']} + {task['test_dataset']}")
        
        if len(tasks) == case["expected_tasks"]:
            print("  âœ… é€šè¿‡")
        else:
            print("  âŒ å¤±è´¥")
    
    print("\nğŸ‰ è¯„ä¼°ä»»åŠ¡ç”Ÿæˆæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    print("ï¿½ æµ‹è¯•æ”¹è¿›çš„è¯„ä¼°é…ç½®æ ¼å¼")
    print("ğŸ¯ è§£å†³è¯„ä¼°å™¨å’Œæµ‹è¯•æ•°æ®é›†çš„æ˜ å°„é—®é¢˜")
    print("ğŸ“Š æ”¯æŒ4ç§é…ç½®æ–¹å¼ï¼šå•ä¸ªã€ä»»åŠ¡åˆ—è¡¨ã€æ˜ å°„ã€ç¬›å¡å°”ç§¯")
    print()
    
    test_evaluation_config()
    print()
    test_yaml_config()
    print()
    test_task_generation()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°é…ç½®æ ¼å¼å®Œç¾è§£å†³äº†è¯„ä¼°å™¨-æ•°æ®é›†æ˜ å°„é—®é¢˜ã€‚")
