#!/usr/bin/env python3
"""
æµ‹è¯•DDDRç±»åæ¼”å’Œå›¾åƒç”Ÿæˆè¿‡ç¨‹
"""

import os
import sys
import torch
import numpy as np
from omegaconf import OmegaConf

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/nlp/ct/projects/MOE-FedCL')

def test_diffusion_model_init():
    """æµ‹è¯•æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–"""
    print("=" * 50)
    print("æµ‹è¯•æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–")
    print("=" * 50)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = "config/ldm_dddr.yaml"
    ckpt_path = "PM/ldm/text2img-large/model.ckpt"
    
    print(f"é…ç½®æ–‡ä»¶è·¯å¾„: {config_path}")
    print(f"æ£€æŸ¥ç‚¹è·¯å¾„: {ckpt_path}")
    
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return False
    
    if not os.path.exists(ckpt_path):
        print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
        return False
    
    print("âœ… é…ç½®æ–‡ä»¶å­˜åœ¨")
    print("âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶å­˜åœ¨")
    
    try:
        # åŠ è½½é…ç½®
        config = OmegaConf.load(config_path)
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•LDMæ¨¡å—å¯¼å…¥
        try:
            from fedcl.models.ldm import LatentDiffusion
            print("âœ… LatentDiffusionæ¨¡å—å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ LatentDiffusionæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return False
        
        # å°è¯•åˆ›å»ºæ¨¡å‹
        config.model.params.ckpt_path = ckpt_path
        config['model']["params"]['personalization_config']["params"]['num_classes'] = 20
        
        print("å°è¯•åˆ›å»ºLatentDiffusionæ¨¡å‹...")
        generator = LatentDiffusion(**config['model']["params"])
        print("âœ… LatentDiffusionæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        print("å°è¯•åŠ è½½æ£€æŸ¥ç‚¹...")
        checkpoint = torch.load(ckpt_path, map_location="cpu")
        
        if "state_dict" in checkpoint:
            state_dict = checkpoint["state_dict"]
        else:
            state_dict = checkpoint
        
        # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„é”®
        model_state_dict = generator.state_dict()
        filtered_state_dict = {}
        skipped_keys = []
        
        for key, value in state_dict.items():
            if key in model_state_dict and model_state_dict[key].shape == value.shape:
                filtered_state_dict[key] = value
            else:
                skipped_keys.append(key)
        
        print(f"âœ… è¿‡æ»¤ååŠ è½½ {len(filtered_state_dict)} ä¸ªåŒ¹é…çš„é”®")
        print(f"âš ï¸  è·³è¿‡ {len(skipped_keys)} ä¸ªä¸åŒ¹é…çš„é”®")
        
        # åŠ è½½åŒ¹é…çš„æƒé‡
        missing_keys, unexpected_keys = generator.load_state_dict(filtered_state_dict, strict=False)
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå¤±çš„é”®: {len(missing_keys)}")
            for key in missing_keys[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   ç¼ºå¤±: {key}")
        
        if unexpected_keys:
            print(f"âš ï¸  æ„å¤–çš„é”®: {len(unexpected_keys)}")
            for key in unexpected_keys[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   æ„å¤–: {key}")
        
        print("âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥åµŒå…¥ç®¡ç†å™¨
        if hasattr(generator, 'embedding_manager'):
            print("âœ… åµŒå…¥ç®¡ç†å™¨å­˜åœ¨")
            print(f"   åµŒå…¥ç®¡ç†å™¨çŠ¶æ€: {list(generator.embedding_manager.state_dict().keys())}")
        else:
            print("âŒ åµŒå…¥ç®¡ç†å™¨ä¸å­˜åœ¨")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_class_inversion():
    """æµ‹è¯•ç±»åæ¼”è¿‡ç¨‹"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç±»åæ¼”è¿‡ç¨‹")
    print("=" * 50)
    
    try:
        from fedcl.methods.trainers.dddr_federation_trainer import DDDRFederationTrainer
        
        # åˆ›å»ºé…ç½®
        config = {
            "num_tasks": 1,
            "classes_per_task": 20,
            "total_classes": 100,
            "num_clients": 2,
            "com_rounds": 5,
            "local_epochs": 2,
            "batch_size": 32,
            "ldm_config": "config/ldm_dddr.yaml",
            "ldm_ckpt": "PM/ldm/text2img-large/model.ckpt",
            "pre_size": 200,
            "cur_size": 50,
            "n_iter": 2,
            "com_rounds_gen": 2,
            "g_local_train_steps": 5,
            "w_kd": 10.0,
            "w_ce_pre": 0.5,
            "w_scl": 1.0,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "frac": 1.0,
            "g_sigma": 0.0
        }
        
        print("åˆ›å»ºDDDRè”é‚¦è®­ç»ƒå™¨...")
        trainer = DDDRFederationTrainer(config)
        
        # æ£€æŸ¥æ‰©æ•£æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
        if trainer._generator is not None:
            print("âœ… æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•ç±»åæ¼”
            task_id = 0
            class_ids = [0, 1, 2, 3, 4]  # å‰5ä¸ªç±»åˆ«
            
            print(f"å¼€å§‹æµ‹è¯•ç±»åæ¼”ï¼Œä»»åŠ¡ {task_id}ï¼Œç±»åˆ« {class_ids}")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç®¡ç†å™¨
            trainer.data_manager = None
            
            # åˆ›å»ºå­¦ä¹ å™¨
            trainer.learners = []
            for i in range(config["num_clients"]):
                from fedcl.methods.learners.dddr import DDDRLearner
                learner_config = trainer._create_learner_config(f"client_{i}")
                learner = DDDRLearner(f"client_{i}", learner_config)
                trainer.learners.append(learner)
            
            # æ‰§è¡Œç±»åæ¼”
            inv_text_embeds = trainer._federated_class_inversion()
            
            if inv_text_embeds is not None:
                print("âœ… ç±»åæ¼”æˆåŠŸ")
                print(f"   åµŒå…¥å­—å…¸é”®: {list(inv_text_embeds.keys())}")
                return True
            else:
                print("âŒ ç±»åæ¼”å¤±è´¥")
                return False
        else:
            print("âŒ æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ç±»åæ¼”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_image_generation():
    """æµ‹è¯•å›¾åƒç”Ÿæˆè¿‡ç¨‹"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å›¾åƒç”Ÿæˆè¿‡ç¨‹")
    print("=" * 50)
    
    try:
        from fedcl.methods.trainers.dddr_federation_trainer import DDDRFederationTrainer
        
        # åˆ›å»ºé…ç½®
        config = {
            "num_tasks": 1,
            "classes_per_task": 20,
            "total_classes": 100,
            "num_clients": 2,
            "com_rounds": 5,
            "local_epochs": 2,
            "batch_size": 32,
            "ldm_config": "config/ldm_dddr.yaml",
            "ldm_ckpt": "PM/ldm/text2img-large/model.ckpt",
            "pre_size": 200,
            "cur_size": 50,
            "n_iter": 1,  # å‡å°‘è¿­ä»£æ¬¡æ•°ç”¨äºæµ‹è¯•
            "com_rounds_gen": 1,
            "g_local_train_steps": 2,
            "w_kd": 10.0,
            "w_ce_pre": 0.5,
            "w_scl": 1.0,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "frac": 1.0,
            "g_sigma": 0.0
        }
        
        print("åˆ›å»ºDDDRè”é‚¦è®­ç»ƒå™¨...")
        trainer = DDDRFederationTrainer(config)
        
        if trainer._generator is not None:
            print("âœ… æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•å›¾åƒç”Ÿæˆ
            task_id = 0
            class_ids = [0, 1]  # åªæµ‹è¯•2ä¸ªç±»åˆ«
            
            print(f"å¼€å§‹æµ‹è¯•å›¾åƒç”Ÿæˆï¼Œä»»åŠ¡ {task_id}ï¼Œç±»åˆ« {class_ids}")
            
            # åˆ›å»ºæ¨¡æ‹ŸåµŒå…¥
            inv_text_embeds = {}
            for class_id in class_ids:
                inv_text_embeds[f"*_{class_id}"] = torch.randn(1280)  # æ¨¡æ‹ŸåµŒå…¥
            
            # æ‰§è¡Œå›¾åƒç”Ÿæˆ
            trainer._synthesis_images(inv_text_embeds)
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            outdir = os.path.join("syn_imgs", f"task_{task_id}")
            if os.path.exists(outdir):
                print(f"âœ… å›¾åƒç”Ÿæˆç›®å½•åˆ›å»ºæˆåŠŸ: {outdir}")
                
                # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„å›¾åƒ
                for class_id in class_ids:
                    class_dir = os.path.join(outdir, str(class_id))
                    if os.path.exists(class_dir):
                        image_files = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]
                        print(f"   ç±»åˆ« {class_id}: {len(image_files)} å¼ å›¾åƒ")
                    else:
                        print(f"   âŒ ç±»åˆ« {class_id} ç›®å½•ä¸å­˜åœ¨")
                
                return True
            else:
                print(f"âŒ å›¾åƒç”Ÿæˆç›®å½•ä¸å­˜åœ¨: {outdir}")
                return False
        else:
            print("âŒ æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ å›¾åƒç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("DDDRç±»åæ¼”å’Œå›¾åƒç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1: æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–
    model_ok = test_diffusion_model_init()
    
    if model_ok:
        # æµ‹è¯•2: ç±»åæ¼”
        inversion_ok = test_class_inversion()
        
        # æµ‹è¯•3: å›¾åƒç”Ÿæˆ
        generation_ok = test_image_generation()
        
        print("\n" + "=" * 60)
        print("æµ‹è¯•ç»“æœæ€»ç»“:")
        print(f"æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–: {'âœ… æˆåŠŸ' if model_ok else 'âŒ å¤±è´¥'}")
        print(f"ç±»åæ¼”è¿‡ç¨‹: {'âœ… æˆåŠŸ' if inversion_ok else 'âŒ å¤±è´¥'}")
        print(f"å›¾åƒç”Ÿæˆè¿‡ç¨‹: {'âœ… æˆåŠŸ' if generation_ok else 'âŒ å¤±è´¥'}")
        
        if model_ok and inversion_ok and generation_ok:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DDDRçš„ç±»åæ¼”å’Œå›¾åƒç”ŸæˆåŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®å’Œä¾èµ–ã€‚")
    else:
        print("\nâŒ æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­æµ‹è¯•ã€‚")

if __name__ == "__main__":
    main()
