# fedcl/automation/data_manager.py
"""
è‡ªåŠ¨æ•°æ®ç®¡ç†å™¨

å¤„ç†çœŸè”é‚¦ï¼ˆå¤šæœºï¼‰å’Œä¼ªè”é‚¦ï¼ˆæœ¬åœ°ï¼‰ç¯å¢ƒä¸‹çš„æ•°æ®åˆ†å‘å’Œç®¡ç†ã€‚
æ”¯æŒIIDå’ŒNon-IIDæ•°æ®åˆ†å¸ƒï¼Œä»¥åŠæ•°æ®éšç§ä¿æŠ¤ã€‚
"""

import hashlib
import json
import pickle
import random
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Subset
from loguru import logger

from .communication import TransparentCommunication, Message


class DataDistributionType(Enum):
    """æ•°æ®åˆ†å¸ƒç±»å‹"""
    IID = "iid"                        # ç‹¬ç«‹åŒåˆ†å¸ƒ
    NON_IID_LABEL = "non_iid_label"    # æ ‡ç­¾ä¸å‡è¡¡
    NON_IID_FEATURE = "non_iid_feature" # ç‰¹å¾åˆ†å¸ƒä¸åŒ
    NON_IID_QUANTITY = "non_iid_quantity" # æ•°æ®é‡ä¸å‡è¡¡
    TEMPORAL = "temporal"              # æ—¶åºåˆ†å¸ƒ


@dataclass
class DataPartition:
    """æ•°æ®åˆ†åŒº"""
    client_id: str
    indices: List[int]
    labels: List[int]
    size: int
    distribution_info: Dict[str, Any]


@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    distribution_type: DataDistributionType = DataDistributionType.IID
    num_clients: int = 3
    min_samples_per_client: int = 100
    alpha: float = 0.5  # Dirichletåˆ†å¸ƒå‚æ•°
    num_classes: int = 10
    seed: int = 42
    validation_split: float = 0.1
    batch_size: int = 32
    shuffle: bool = True


class BaseDataPartitioner(ABC):
    """æ•°æ®åˆ†åŒºå™¨åŸºç±»"""
    
    def __init__(self, config: DataConfig):
        self.config = config
        self.logger = logger.bind(component="DataPartitioner")
        np.random.seed(config.seed)
        random.seed(config.seed)
    
    @abstractmethod
    def partition_data(self, dataset: Dataset) -> List[DataPartition]:
        """åˆ†åŒºæ•°æ®"""
        pass
    
    def _get_labels(self, dataset: Dataset) -> np.ndarray:
        """è·å–æ•°æ®é›†æ ‡ç­¾"""
        if hasattr(dataset, 'targets'):
            return np.array(dataset.targets)
        elif hasattr(dataset, 'labels'):
            return np.array(dataset.labels)
        else:
            # éå†æ•°æ®é›†è·å–æ ‡ç­¾
            labels = []
            for i in range(len(dataset)):
                _, label = dataset[i]
                labels.append(label)
            return np.array(labels)


class IIDPartitioner(BaseDataPartitioner):
    """IIDæ•°æ®åˆ†åŒºå™¨"""
    
    def partition_data(self, dataset: Dataset) -> List[DataPartition]:
        """IIDåˆ†åŒº"""
        self.logger.info(f"ğŸ² å¼€å§‹IIDæ•°æ®åˆ†åŒº - {self.config.num_clients}ä¸ªå®¢æˆ·ç«¯")
        
        total_size = len(dataset)
        labels = self._get_labels(dataset)
        
        # éšæœºæ‰“ä¹±ç´¢å¼•
        indices = np.random.permutation(total_size)
        
        # å¹³å‡åˆ†é…
        partition_size = total_size // self.config.num_clients
        partitions = []
        
        for i in range(self.config.num_clients):
            start_idx = i * partition_size
            if i == self.config.num_clients - 1:
                end_idx = total_size  # æœ€åä¸€ä¸ªå®¢æˆ·ç«¯è·å–å‰©ä½™æ‰€æœ‰æ•°æ®
            else:
                end_idx = (i + 1) * partition_size
            
            client_indices = indices[start_idx:end_idx].tolist()
            client_labels = labels[client_indices].tolist()
            
            # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
            unique_labels, counts = np.unique(client_labels, return_counts=True)
            label_distribution = dict(zip(unique_labels.tolist(), counts.tolist()))
            
            partition = DataPartition(
                client_id=f"client_{i}",
                indices=client_indices,
                labels=client_labels,
                size=len(client_indices),
                distribution_info={
                    "type": "iid",
                    "label_distribution": label_distribution,
                    "num_classes": len(unique_labels)
                }
            )
            partitions.append(partition)
        
        self.logger.info(f"âœ… IIDåˆ†åŒºå®Œæˆ - å¹³å‡æ¯å®¢æˆ·ç«¯: {partition_size}æ ·æœ¬")
        return partitions


class NonIIDLabelPartitioner(BaseDataPartitioner):
    """Non-IIDæ ‡ç­¾åˆ†åŒºå™¨ï¼ˆåŸºäºDirichletåˆ†å¸ƒï¼‰"""
    
    def partition_data(self, dataset: Dataset) -> List[DataPartition]:
        """Non-IIDæ ‡ç­¾åˆ†åŒº"""
        self.logger.info(f"ğŸ¯ å¼€å§‹Non-IIDæ ‡ç­¾åˆ†åŒº - Î±={self.config.alpha}")
        
        labels = self._get_labels(dataset)
        num_classes = len(np.unique(labels))
        
        # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®
        class_indices = {i: np.where(labels == i)[0] for i in range(num_classes)}
        
        # ä½¿ç”¨Dirichletåˆ†å¸ƒä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…ç±»åˆ«æ¯”ä¾‹
        partitions = []
        
        for client_id in range(self.config.num_clients):
            # ä¸ºå½“å‰å®¢æˆ·ç«¯ç”Ÿæˆç±»åˆ«åˆ†å¸ƒ
            proportions = np.random.dirichlet([self.config.alpha] * num_classes)
            
            client_indices = []
            client_labels = []
            
            # æ ¹æ®æ¯”ä¾‹ä»æ¯ä¸ªç±»åˆ«é‡‡æ ·
            for class_id, proportion in enumerate(proportions):
                if class_id in class_indices:
                    available_indices = class_indices[class_id]
                    num_samples = int(proportion * self.config.min_samples_per_client)
                    
                    if num_samples > 0 and len(available_indices) > 0:
                        # éšæœºé‡‡æ ·
                        sampled_indices = np.random.choice(
                            available_indices, 
                            size=min(num_samples, len(available_indices)), 
                            replace=False
                        )
                        client_indices.extend(sampled_indices.tolist())
                        client_labels.extend([class_id] * len(sampled_indices))
                        
                        # ä»å¯ç”¨ç´¢å¼•ä¸­ç§»é™¤å·²ä½¿ç”¨çš„
                        class_indices[class_id] = np.setdiff1d(available_indices, sampled_indices)
            
            # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
            unique_labels, counts = np.unique(client_labels, return_counts=True)
            label_distribution = dict(zip(unique_labels.tolist(), counts.tolist()))
            
            partition = DataPartition(
                client_id=f"client_{client_id}",
                indices=client_indices,
                labels=client_labels,
                size=len(client_indices),
                distribution_info={
                    "type": "non_iid_label",
                    "alpha": self.config.alpha,
                    "label_distribution": label_distribution,
                    "num_classes": len(unique_labels),
                    "proportions": proportions.tolist()
                }
            )
            partitions.append(partition)
        
        self.logger.info(f"âœ… Non-IIDæ ‡ç­¾åˆ†åŒºå®Œæˆ")
        return partitions


class NonIIDQuantityPartitioner(BaseDataPartitioner):
    """Non-IIDæ•°é‡åˆ†åŒºå™¨ï¼ˆä¸åŒå®¢æˆ·ç«¯æ•°æ®é‡ä¸åŒï¼‰"""
    
    def partition_data(self, dataset: Dataset) -> List[DataPartition]:
        """Non-IIDæ•°é‡åˆ†åŒº"""
        self.logger.info(f"ğŸ“Š å¼€å§‹Non-IIDæ•°é‡åˆ†åŒº")
        
        total_size = len(dataset)
        labels = self._get_labels(dataset)
        
        # ç”Ÿæˆä¸å‡åŒ€çš„æ•°æ®é‡åˆ†å¸ƒ
        sizes = np.random.lognormal(mean=np.log(total_size / self.config.num_clients), sigma=0.5, size=self.config.num_clients)
        sizes = sizes / sizes.sum() * total_size  # å½’ä¸€åŒ–åˆ°æ€»æ•°æ®é‡
        sizes = np.maximum(sizes, self.config.min_samples_per_client)  # ç¡®ä¿æœ€å°æ ·æœ¬æ•°
        sizes = sizes.astype(int)
        
        # è°ƒæ•´æœ€åä¸€ä¸ªå®¢æˆ·ç«¯çš„å¤§å°ä»¥åŒ¹é…æ€»æ•°æ®é‡
        sizes[-1] = total_size - sizes[:-1].sum()
        
        indices = np.random.permutation(total_size)
        partitions = []
        current_idx = 0
        
        for i in range(self.config.num_clients):
            size = sizes[i]
            client_indices = indices[current_idx:current_idx + size].tolist()
            client_labels = labels[client_indices].tolist()
            
            # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
            unique_labels, counts = np.unique(client_labels, return_counts=True)
            label_distribution = dict(zip(unique_labels.tolist(), counts.tolist()))
            
            partition = DataPartition(
                client_id=f"client_{i}",
                indices=client_indices,
                labels=client_labels,
                size=len(client_indices),
                distribution_info={
                    "type": "non_iid_quantity",
                    "expected_size": int(total_size / self.config.num_clients),
                    "actual_size": size,
                    "label_distribution": label_distribution,
                    "size_ratio": size / (total_size / self.config.num_clients)
                }
            )
            partitions.append(partition)
            current_idx += size
        
        self.logger.info(f"âœ… Non-IIDæ•°é‡åˆ†åŒºå®Œæˆ - å¤§å°èŒƒå›´: {min(sizes)}-{max(sizes)}")
        return partitions


class FederatedDataLoader:
    """è”é‚¦æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, dataset: Dataset, partition: DataPartition, config: DataConfig):
        self.dataset = dataset
        self.partition = partition
        self.config = config
        self.logger = logger.bind(component="FederatedDataLoader", client=partition.client_id)
        
        # åˆ›å»ºå­æ•°æ®é›†
        self.client_dataset = Subset(dataset, partition.indices)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        self.train_loader, self.val_loader = self._create_data_loaders()
    
    def _create_data_loaders(self) -> Tuple[DataLoader, Optional[DataLoader]]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        dataset_size = len(self.client_dataset)
        
        if self.config.validation_split > 0:
            # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            val_size = int(dataset_size * self.config.validation_split)
            train_size = dataset_size - val_size
            
            train_dataset, val_dataset = torch.utils.data.random_split(
                self.client_dataset, [train_size, val_size]
            )
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                shuffle=self.config.shuffle
            )
            
            val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.batch_size,
                shuffle=False
            )
            
            return train_loader, val_loader
        else:
            train_loader = DataLoader(
                self.client_dataset,
                batch_size=self.config.batch_size,
                shuffle=self.config.shuffle
            )
            return train_loader, None
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "client_id": self.partition.client_id,
            "total_samples": self.partition.size,
            "train_batches": len(self.train_loader),
            "val_batches": len(self.val_loader) if self.val_loader else 0,
            "distribution_info": self.partition.distribution_info
        }


class AutoDataManager:
    """
    è‡ªåŠ¨æ•°æ®ç®¡ç†å™¨
    
    æä¾›è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹çš„è‡ªåŠ¨æ•°æ®åˆ†å‘å’Œç®¡ç†åŠŸèƒ½
    """
    
    def __init__(
        self,
        communication: Optional[TransparentCommunication] = None,
        config: Optional[DataConfig] = None
    ):
        self.communication = communication
        self.config = config or DataConfig()
        self.logger = logger.bind(component="AutoDataManager")
        
        # æ•°æ®åˆ†åŒºå™¨æ˜ å°„
        self.partitioners = {
            DataDistributionType.IID: IIDPartitioner,
            DataDistributionType.NON_IID_LABEL: NonIIDLabelPartitioner,
            DataDistributionType.NON_IID_QUANTITY: NonIIDQuantityPartitioner,
        }
        
        self.partitions: List[DataPartition] = []
        self.data_loaders: Dict[str, FederatedDataLoader] = {}
    
    def distribute_data(
        self, 
        dataset: Dataset, 
        num_clients: Optional[int] = None,
        distribution_type: Optional[DataDistributionType] = None
    ) -> List[DataPartition]:
        """
        è‡ªåŠ¨åˆ†å‘æ•°æ®åˆ°å„å®¢æˆ·ç«¯
        
        Args:
            dataset: åŸå§‹æ•°æ®é›†
            num_clients: å®¢æˆ·ç«¯æ•°é‡
            distribution_type: åˆ†å¸ƒç±»å‹
            
        Returns:
            æ•°æ®åˆ†åŒºåˆ—è¡¨
        """
        # æ›´æ–°é…ç½®
        if num_clients:
            self.config.num_clients = num_clients
        if distribution_type:
            self.config.distribution_type = distribution_type
        
        self.logger.info(f"ğŸ¯ å¼€å§‹æ•°æ®åˆ†å‘ - ç±»å‹: {self.config.distribution_type.value}")
        
        # é€‰æ‹©åˆ†åŒºå™¨
        partitioner_class = self.partitioners.get(self.config.distribution_type)
        if not partitioner_class:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å¸ƒç±»å‹: {self.config.distribution_type}")
        
        partitioner = partitioner_class(self.config)
        self.partitions = partitioner.partition_data(dataset)
        
        # æ‰“å°åˆ†åŒºç»Ÿè®¡ä¿¡æ¯
        self._log_partition_stats()
        
        return self.partitions
    
    def create_federated_dataloaders(
        self, 
        dataset: Dataset,
        client_ids: Optional[List[str]] = None
    ) -> Dict[str, FederatedDataLoader]:
        """
        åˆ›å»ºè”é‚¦æ•°æ®åŠ è½½å™¨
        
        Args:
            dataset: åŸå§‹æ•°æ®é›†
            client_ids: æŒ‡å®šçš„å®¢æˆ·ç«¯IDåˆ—è¡¨
            
        Returns:
            å®¢æˆ·ç«¯æ•°æ®åŠ è½½å™¨å­—å…¸
        """
        if not self.partitions:
            raise ValueError("è¯·å…ˆè°ƒç”¨ distribute_data è¿›è¡Œæ•°æ®åˆ†å‘")
        
        self.logger.info(f"ğŸ“š åˆ›å»ºè”é‚¦æ•°æ®åŠ è½½å™¨")
        
        target_partitions = self.partitions
        if client_ids:
            target_partitions = [p for p in self.partitions if p.client_id in client_ids]
        
        for partition in target_partitions:
            data_loader = FederatedDataLoader(dataset, partition, self.config)
            self.data_loaders[partition.client_id] = data_loader
        
        self.logger.info(f"âœ… åˆ›å»ºäº† {len(self.data_loaders)} ä¸ªæ•°æ®åŠ è½½å™¨")
        return self.data_loaders
    
    def handle_data_heterogeneity(self, datasets: List[Dataset]) -> Dict[str, Any]:
        """
        å¤„ç†æ•°æ®å¼‚æ„æ€§
        
        Args:
            datasets: ä¸åŒå®¢æˆ·ç«¯çš„æ•°æ®é›†åˆ—è¡¨
            
        Returns:
            å¼‚æ„æ€§åˆ†æç»“æœ
        """
        self.logger.info("ğŸ” åˆ†ææ•°æ®å¼‚æ„æ€§")
        
        heterogeneity_stats = {
            "size_variance": 0.0,
            "label_distribution_variance": 0.0,
            "feature_similarity": 0.0,
            "recommendations": []
        }
        
        # åˆ†ææ•°æ®å¤§å°å¼‚æ„æ€§
        sizes = [len(partition.indices) for partition in self.partitions]
        size_mean = np.mean(sizes)
        size_variance = np.var(sizes) / (size_mean ** 2)  # å˜å¼‚ç³»æ•°
        heterogeneity_stats["size_variance"] = float(size_variance)
        
        # åˆ†ææ ‡ç­¾åˆ†å¸ƒå¼‚æ„æ€§
        label_distributions = []
        for partition in self.partitions:
            dist = partition.distribution_info.get("label_distribution", {})
            total_samples = sum(dist.values()) if dist else 1
            normalized_dist = {k: v/total_samples for k, v in dist.items()}
            label_distributions.append(normalized_dist)
        
        # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒçš„KLæ•£åº¦
        kl_divergences = []
        for i in range(len(label_distributions)):
            for j in range(i+1, len(label_distributions)):
                kl_div = self._calculate_kl_divergence(label_distributions[i], label_distributions[j])
                kl_divergences.append(kl_div)
        
        if kl_divergences:
            heterogeneity_stats["label_distribution_variance"] = float(np.mean(kl_divergences))
        
        # ç”Ÿæˆæ¨è
        recommendations = []
        if size_variance > 0.5:
            recommendations.append("æ•°æ®å¤§å°å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨åŠ æƒèšåˆ")
        if heterogeneity_stats["label_distribution_variance"] > 1.0:
            recommendations.append("æ ‡ç­¾åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ")
        
        heterogeneity_stats["recommendations"] = recommendations
        
        self.logger.info(f"ğŸ“Š å¼‚æ„æ€§åˆ†æå®Œæˆ - å¤§å°å˜å¼‚: {size_variance:.3f}")
        return heterogeneity_stats
    
    def _calculate_kl_divergence(self, p: Dict[str, float], q: Dict[str, float]) -> float:
        """è®¡ç®—KLæ•£åº¦"""
        all_keys = set(p.keys()) | set(q.keys())
        kl_div = 0.0
        
        for key in all_keys:
            p_val = p.get(key, 1e-10)
            q_val = q.get(key, 1e-10)
            kl_div += p_val * np.log(p_val / q_val)
        
        return kl_div
    
    def _log_partition_stats(self):
        """æ‰“å°åˆ†åŒºç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("ğŸ“Š æ•°æ®åˆ†åŒºç»Ÿè®¡:")
        
        for partition in self.partitions:
            dist_info = partition.distribution_info
            self.logger.info(
                f"  {partition.client_id}: {partition.size}æ ·æœ¬, "
                f"{dist_info.get('num_classes', 0)}ç±»åˆ«"
            )
    
    def get_data_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‘æ‘˜è¦"""
        if not self.partitions:
            return {"status": "no_partitions"}
        
        total_samples = sum(p.size for p in self.partitions)
        avg_samples = total_samples / len(self.partitions)
        
        return {
            "num_clients": len(self.partitions),
            "total_samples": total_samples,
            "avg_samples_per_client": avg_samples,
            "distribution_type": self.config.distribution_type.value,
            "partitions": [
                {
                    "client_id": p.client_id,
                    "size": p.size,
                    "distribution_info": p.distribution_info
                }
                for p in self.partitions
            ]
        }
    
    def save_partitions(self, filepath: Union[str, Path]):
        """ä¿å­˜æ•°æ®åˆ†åŒºä¿¡æ¯"""
        filepath = Path(filepath)
        
        partition_data = {
            "config": {
                "distribution_type": self.config.distribution_type.value,
                "num_clients": self.config.num_clients,
                "seed": self.config.seed
            },
            "partitions": [
                {
                    "client_id": p.client_id,
                    "indices": p.indices,
                    "labels": p.labels,
                    "size": p.size,
                    "distribution_info": p.distribution_info
                }
                for p in self.partitions
            ]
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(partition_data, f)
        
        self.logger.info(f"ğŸ’¾ æ•°æ®åˆ†åŒºå·²ä¿å­˜åˆ°: {filepath}")
    
    def load_partitions(self, filepath: Union[str, Path]) -> List[DataPartition]:
        """åŠ è½½æ•°æ®åˆ†åŒºä¿¡æ¯"""
        filepath = Path(filepath)
        
        with open(filepath, 'rb') as f:
            partition_data = pickle.load(f)
        
        self.partitions = [
            DataPartition(
                client_id=p_data["client_id"],
                indices=p_data["indices"],
                labels=p_data["labels"],
                size=p_data["size"],
                distribution_info=p_data["distribution_info"]
            )
            for p_data in partition_data["partitions"]
        ]
        
        self.logger.info(f"ğŸ“‚ æ•°æ®åˆ†åŒºå·²ä» {filepath} åŠ è½½")
        return self.partitions