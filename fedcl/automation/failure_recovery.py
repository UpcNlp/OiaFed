# fedcl/automation/failure_recovery.py
"""
æ•…éšœæ¢å¤æœºåˆ¶

å¤„ç†çœŸè”é‚¦ï¼ˆå¤šæœºï¼‰å’Œä¼ªè”é‚¦ç¯å¢ƒä¸‹çš„å„ç§æ•…éšœæƒ…å†µï¼š
- ç½‘ç»œè¿æ¥æ•…éšœ
- å®¢æˆ·ç«¯èŠ‚ç‚¹å´©æºƒ  
- æœåŠ¡å™¨æ•…éšœ
- å†…å­˜ä¸è¶³
- è®­ç»ƒå‘æ•£
"""

import json
import pickle
import time
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable, Union
import shutil
import psutil

import torch
from loguru import logger

from .communication import TransparentCommunication


class FailureType(Enum):
    """æ•…éšœç±»å‹"""
    NETWORK_TIMEOUT = "network_timeout"
    CLIENT_DISCONNECT = "client_disconnect"
    SERVER_UNAVAILABLE = "server_unavailable"
    MEMORY_OVERFLOW = "memory_overflow"
    TRAINING_DIVERGENCE = "training_divergence"
    MODEL_CORRUPTION = "model_corruption"
    COMMUNICATION_ERROR = "communication_error"
    RESOURCE_EXHAUSTION = "resource_exhaustion"


@dataclass
class FailureEvent:
    """æ•…éšœäº‹ä»¶"""
    failure_type: FailureType
    component: str  # æ•…éšœç»„ä»¶ (client_id, server, communicationç­‰)
    timestamp: float
    details: Dict[str, Any]
    severity: str = "medium"  # low, medium, high, critical
    recovery_attempts: int = 0
    resolved: bool = False


@dataclass
class Checkpoint:
    """æ£€æŸ¥ç‚¹"""
    round_number: int
    timestamp: float
    global_model_state: Optional[Dict[str, Any]]
    client_states: Dict[str, Dict[str, Any]]
    training_metrics: Dict[str, Any]
    system_state: Dict[str, Any]
    checkpoint_path: str


class BaseRecoveryStrategy(ABC):
    """æ¢å¤ç­–ç•¥åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logger.bind(component="RecoveryStrategy")
    
    @abstractmethod
    def can_handle(self, failure: FailureEvent) -> bool:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥æ•…éšœ"""
        pass
    
    @abstractmethod
    def recover(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """æ‰§è¡Œæ¢å¤æ“ä½œ"""
        pass
    
    def get_priority(self) -> int:
        """è·å–ç­–ç•¥ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰"""
        return 50


class NetworkTimeoutRecoveryStrategy(BaseRecoveryStrategy):
    """ç½‘ç»œè¶…æ—¶æ¢å¤ç­–ç•¥"""
    
    def can_handle(self, failure: FailureEvent) -> bool:
        return failure.failure_type == FailureType.NETWORK_TIMEOUT
    
    def recover(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """æ¢å¤ç½‘ç»œè¶…æ—¶"""
        self.logger.info(f"ğŸ”„ å°è¯•æ¢å¤ç½‘ç»œè¶…æ—¶æ•…éšœ: {failure.component}")
        
        max_retries = self.config.get("max_network_retries", 3)
        retry_interval = self.config.get("retry_interval", 5.0)
        
        for attempt in range(max_retries):
            self.logger.info(f"ğŸ“¡ é‡è¯•ç½‘ç»œè¿æ¥ {attempt + 1}/{max_retries}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
            time.sleep(retry_interval * (attempt + 1))
            
            # å°è¯•é‡æ–°å»ºç«‹è¿æ¥
            communication = context.get("communication")
            if communication and hasattr(communication, "backend"):
                try:
                    # é‡å¯é€šä¿¡åç«¯
                    communication.backend.stop()
                    time.sleep(1)
                    if communication.backend.start():
                        self.logger.info("âœ… ç½‘ç»œè¿æ¥æ¢å¤æˆåŠŸ")
                        return True
                except Exception as e:
                    self.logger.warning(f"é‡è¿å°è¯•å¤±è´¥: {e}")
        
        self.logger.error("âŒ ç½‘ç»œè¿æ¥æ¢å¤å¤±è´¥")
        return False
    
    def get_priority(self) -> int:
        return 10


class ClientDisconnectRecoveryStrategy(BaseRecoveryStrategy):
    """å®¢æˆ·ç«¯æ–­è¿æ¢å¤ç­–ç•¥"""
    
    def can_handle(self, failure: FailureEvent) -> bool:
        return failure.failure_type == FailureType.CLIENT_DISCONNECT
    
    def recover(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """å¤„ç†å®¢æˆ·ç«¯æ–­è¿"""
        client_id = failure.component
        self.logger.info(f"ğŸ‘¤ å¤„ç†å®¢æˆ·ç«¯æ–­è¿: {client_id}")
        
        # ä»æ´»è·ƒå®¢æˆ·ç«¯åˆ—è¡¨ä¸­ç§»é™¤
        active_clients = context.get("active_clients", set())
        if client_id in active_clients:
            active_clients.remove(client_id)
            self.logger.info(f"ğŸ“ å·²ä»æ´»è·ƒåˆ—è¡¨ç§»é™¤å®¢æˆ·ç«¯: {client_id}")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¶³å¤Ÿçš„å®¢æˆ·ç«¯ç»§ç»­è®­ç»ƒ
        min_clients = self.config.get("min_clients_for_training", 2)
        remaining_clients = len(active_clients)
        
        if remaining_clients >= min_clients:
            self.logger.info(f"âœ… å‰©ä½™ {remaining_clients} ä¸ªå®¢æˆ·ç«¯ï¼Œç»§ç»­è®­ç»ƒ")
            return True
        else:
            self.logger.warning(f"âš ï¸ å®¢æˆ·ç«¯æ•°é‡ä¸è¶³ ({remaining_clients}/{min_clients})ï¼Œæš‚åœè®­ç»ƒ")
            # æ ‡è®°éœ€è¦ç­‰å¾…æ›´å¤šå®¢æˆ·ç«¯
            context["waiting_for_clients"] = True
            return False
    
    def get_priority(self) -> int:
        return 20


class MemoryOverflowRecoveryStrategy(BaseRecoveryStrategy):
    """å†…å­˜æº¢å‡ºæ¢å¤ç­–ç•¥"""
    
    def can_handle(self, failure: FailureEvent) -> bool:
        return failure.failure_type == FailureType.MEMORY_OVERFLOW
    
    def recover(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """æ¢å¤å†…å­˜æº¢å‡º"""
        self.logger.info("ğŸ’¾ å¤„ç†å†…å­˜æº¢å‡ºæ•…éšœ")
        
        try:
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                self.logger.info("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
            
            # å‡å°æ‰¹æ¬¡å¤§å°
            trainer = context.get("trainer")
            if trainer and hasattr(trainer, "config"):
                current_batch_size = trainer.config.get("batch_size", 32)
                new_batch_size = max(1, current_batch_size // 2)
                trainer.config["batch_size"] = new_batch_size
                self.logger.info(f"ğŸ“‰ å‡å°æ‰¹æ¬¡å¤§å°: {current_batch_size} â†’ {new_batch_size}")
            
            # å»ºè®®å‡å°‘æ¨¡å‹å¤§å°æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
            recommendations = [
                "è€ƒè™‘ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯",
                "å‡å°‘æ¨¡å‹å‚æ•°æ•°é‡",
                "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
                "å¯ç”¨æ•°æ®å¹¶è¡Œ"
            ]
            
            for rec in recommendations:
                self.logger.info(f"ğŸ’¡ å»ºè®®: {rec}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"å†…å­˜æº¢å‡ºæ¢å¤å¤±è´¥: {e}")
            return False
    
    def get_priority(self) -> int:
        return 30


class TrainingDivergenceRecoveryStrategy(BaseRecoveryStrategy):
    """è®­ç»ƒå‘æ•£æ¢å¤ç­–ç•¥"""
    
    def can_handle(self, failure: FailureEvent) -> bool:
        return failure.failure_type == FailureType.TRAINING_DIVERGENCE
    
    def recover(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """æ¢å¤è®­ç»ƒå‘æ•£"""
        self.logger.info("ğŸ“ˆ å¤„ç†è®­ç»ƒå‘æ•£æ•…éšœ")
        
        try:
            # é™ä½å­¦ä¹ ç‡
            trainer = context.get("trainer")
            if trainer and hasattr(trainer, "config"):
                current_lr = trainer.config.get("learning_rate", 0.01)
                new_lr = current_lr * 0.5
                trainer.config["learning_rate"] = new_lr
                self.logger.info(f"ğŸ“‰ é™ä½å­¦ä¹ ç‡: {current_lr} â†’ {new_lr}")
            
            # å›æ»šåˆ°æœ€è¿‘çš„ç¨³å®šæ£€æŸ¥ç‚¹
            checkpoint_manager = context.get("checkpoint_manager")
            if checkpoint_manager:
                latest_checkpoint = checkpoint_manager.get_latest_checkpoint()
                if latest_checkpoint:
                    checkpoint_manager.restore_checkpoint(latest_checkpoint.checkpoint_path)
                    self.logger.info(f"ğŸ”„ å›æ»šåˆ°æ£€æŸ¥ç‚¹: round {latest_checkpoint.round_number}")
                    return True
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆæœ€åæ‰‹æ®µï¼‰
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ£€æŸ¥ç‚¹ï¼Œå»ºè®®é‡æ–°åˆå§‹åŒ–æ¨¡å‹")
            return False
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå‘æ•£æ¢å¤å¤±è´¥: {e}")
            return False
    
    def get_priority(self) -> int:
        return 40


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir: Union[str, Path], max_checkpoints: int = 5):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.max_checkpoints = max_checkpoints
        self.logger = logger.bind(component="CheckpointManager")
        
        self.checkpoints: List[Checkpoint] = []
        self._load_existing_checkpoints()
    
    def create_checkpoint(
        self,
        round_number: int,
        global_model_state: Optional[Dict[str, Any]] = None,
        client_states: Optional[Dict[str, Dict[str, Any]]] = None,
        training_metrics: Optional[Dict[str, Any]] = None,
        system_state: Optional[Dict[str, Any]] = None
    ) -> Checkpoint:
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""
        timestamp = time.time()
        checkpoint_filename = f"checkpoint_round_{round_number}_{int(timestamp)}.pkl"
        checkpoint_path = self.checkpoint_dir / checkpoint_filename
        
        checkpoint = Checkpoint(
            round_number=round_number,
            timestamp=timestamp,
            global_model_state=global_model_state,
            client_states=client_states or {},
            training_metrics=training_metrics or {},
            system_state=system_state or {},
            checkpoint_path=str(checkpoint_path)
        )
        
        # ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æ–‡ä»¶
        try:
            with open(checkpoint_path, 'wb') as f:
                pickle.dump(checkpoint, f)
            
            self.checkpoints.append(checkpoint)
            self.checkpoints.sort(key=lambda x: x.round_number)
            
            # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
            self._cleanup_old_checkpoints()
            
            self.logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²åˆ›å»º: round {round_number}")
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            raise
    
    def restore_checkpoint(self, checkpoint_path: str) -> Optional[Checkpoint]:
        """æ¢å¤æ£€æŸ¥ç‚¹"""
        try:
            with open(checkpoint_path, 'rb') as f:
                checkpoint = pickle.load(f)
            
            self.logger.info(f"ğŸ”„ æ£€æŸ¥ç‚¹å·²æ¢å¤: round {checkpoint.round_number}")
            return checkpoint
            
        except Exception as e:
            self.logger.error(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None
    
    def get_latest_checkpoint(self) -> Optional[Checkpoint]:
        """è·å–æœ€æ–°æ£€æŸ¥ç‚¹"""
        if self.checkpoints:
            return max(self.checkpoints, key=lambda x: x.round_number)
        return None
    
    def get_checkpoint_by_round(self, round_number: int) -> Optional[Checkpoint]:
        """æ ¹æ®è½®æ¬¡è·å–æ£€æŸ¥ç‚¹"""
        for checkpoint in self.checkpoints:
            if checkpoint.round_number == round_number:
                return checkpoint
        return None
    
    def _load_existing_checkpoints(self):
        """åŠ è½½ç°æœ‰æ£€æŸ¥ç‚¹"""
        for checkpoint_file in self.checkpoint_dir.glob("checkpoint_*.pkl"):
            try:
                with open(checkpoint_file, 'rb') as f:
                    checkpoint = pickle.load(f)
                self.checkpoints.append(checkpoint)
            except Exception as e:
                self.logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥ {checkpoint_file}: {e}")
        
        self.checkpoints.sort(key=lambda x: x.round_number)
        self.logger.info(f"ğŸ“‚ åŠ è½½äº† {len(self.checkpoints)} ä¸ªç°æœ‰æ£€æŸ¥ç‚¹")
    
    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
        while len(self.checkpoints) > self.max_checkpoints:
            old_checkpoint = self.checkpoints.pop(0)
            try:
                Path(old_checkpoint.checkpoint_path).unlink()
                self.logger.debug(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: round {old_checkpoint.round_number}")
            except Exception as e:
                self.logger.warning(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def list_checkpoints(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
        return [
            {
                "round_number": cp.round_number,
                "timestamp": cp.timestamp,
                "checkpoint_path": cp.checkpoint_path,
                "has_global_model": cp.global_model_state is not None,
                "num_clients": len(cp.client_states)
            }
            for cp in self.checkpoints
        ]


class FailureDetector:
    """æ•…éšœæ£€æµ‹å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logger.bind(component="FailureDetector")
        self.active_monitors: Dict[str, threading.Thread] = {}
        self.running = False
        
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        self.running = True
        
        # å¯åŠ¨å„ç§ç›‘æ§çº¿ç¨‹
        self._start_network_monitor()
        self._start_memory_monitor()
        self._start_training_monitor()
        
        self.logger.info("ğŸ” æ•…éšœæ£€æµ‹å™¨å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        for monitor_name, thread in self.active_monitors.items():
            thread.join(timeout=5)
            
        self.active_monitors.clear()
        self.logger.info("â¹ï¸ æ•…éšœæ£€æµ‹å™¨åœæ­¢")
    
    def _start_network_monitor(self):
        """å¯åŠ¨ç½‘ç»œç›‘æ§"""
        def network_monitor():
            while self.running:
                try:
                    # æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€
                    # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ç½‘ç»œæ£€æµ‹é€»è¾‘
                    time.sleep(self.config.get("network_check_interval", 10))
                except Exception as e:
                    self.logger.error(f"ç½‘ç»œç›‘æ§é”™è¯¯: {e}")
        
        thread = threading.Thread(target=network_monitor, daemon=True)
        thread.start()
        self.active_monitors["network"] = thread
    
    def _start_memory_monitor(self):
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        def memory_monitor():
            memory_threshold = self.config.get("memory_threshold", 0.9)
            
            while self.running:
                try:
                    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡
                    memory_percent = psutil.virtual_memory().percent / 100.0
                    
                    if memory_percent > memory_threshold:
                        self.logger.warning(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1%}")
                        # è§¦å‘å†…å­˜æº¢å‡ºæ•…éšœäº‹ä»¶
                        # è¿™é‡Œå¯ä»¥è§¦å‘å›è°ƒå‡½æ•°
                    
                    # æ£€æŸ¥GPUå†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if torch.cuda.is_available():
                        for device_id in range(torch.cuda.device_count()):
                            gpu_memory = torch.cuda.memory_usage(device_id)
                            if gpu_memory > memory_threshold:
                                self.logger.warning(f"âš ï¸ GPU {device_id} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {gpu_memory:.1%}")
                    
                    time.sleep(self.config.get("memory_check_interval", 5))
                    
                except Exception as e:
                    self.logger.error(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
        
        thread = threading.Thread(target=memory_monitor, daemon=True)
        thread.start()
        self.active_monitors["memory"] = thread
    
    def _start_training_monitor(self):
        """å¯åŠ¨è®­ç»ƒç›‘æ§"""
        def training_monitor():
            while self.running:
                try:
                    # æ£€æŸ¥è®­ç»ƒæ˜¯å¦å‘æ•£
                    # è¿™é‡Œå¯ä»¥å®ç°è®­ç»ƒçŠ¶æ€æ£€æµ‹é€»è¾‘
                    time.sleep(self.config.get("training_check_interval", 30))
                except Exception as e:
                    self.logger.error(f"è®­ç»ƒç›‘æ§é”™è¯¯: {e}")
        
        thread = threading.Thread(target=training_monitor, daemon=True)
        thread.start()
        self.active_monitors["training"] = thread


class FailureRecoveryManager:
    """
    æ•…éšœæ¢å¤ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†æ•…éšœæ£€æµ‹ã€æ¢å¤ç­–ç•¥å’Œæ£€æŸ¥ç‚¹ç³»ç»Ÿ
    """
    
    def __init__(
        self,
        checkpoint_dir: Union[str, Path] = "./checkpoints",
        config: Optional[Dict[str, Any]] = None
    ):
        self.config = config or {}
        self.logger = logger.bind(component="FailureRecoveryManager")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.checkpoint_manager = CheckpointManager(checkpoint_dir)
        self.failure_detector = FailureDetector(self.config)
        
        # æ³¨å†Œæ¢å¤ç­–ç•¥
        self.recovery_strategies: List[BaseRecoveryStrategy] = []
        self._register_default_strategies()
        
        # æ•…éšœäº‹ä»¶å†å²
        self.failure_history: List[FailureEvent] = []
        self.recovery_callbacks: List[Callable] = []
        
    def _register_default_strategies(self):
        """æ³¨å†Œé»˜è®¤æ¢å¤ç­–ç•¥"""
        strategies = [
            NetworkTimeoutRecoveryStrategy(self.config),
            ClientDisconnectRecoveryStrategy(self.config),
            MemoryOverflowRecoveryStrategy(self.config),
            TrainingDivergenceRecoveryStrategy(self.config)
        ]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        strategies.sort(key=lambda x: x.get_priority())
        self.recovery_strategies.extend(strategies)
        
        self.logger.info(f"ğŸ“‹ å·²æ³¨å†Œ {len(strategies)} ä¸ªæ¢å¤ç­–ç•¥")
    
    def register_strategy(self, strategy: BaseRecoveryStrategy):
        """æ³¨å†Œè‡ªå®šä¹‰æ¢å¤ç­–ç•¥"""
        self.recovery_strategies.append(strategy)
        self.recovery_strategies.sort(key=lambda x: x.get_priority())
        self.logger.info(f"âœ… æ³¨å†Œæ–°æ¢å¤ç­–ç•¥: {strategy.__class__.__name__}")
    
    def register_recovery_callback(self, callback: Callable):
        """æ³¨å†Œæ¢å¤å›è°ƒå‡½æ•°"""
        self.recovery_callbacks.append(callback)
    
    def detect_failure(self, failure_type: FailureType, component: str, details: Dict[str, Any]) -> FailureEvent:
        """æ£€æµ‹å¹¶è®°å½•æ•…éšœ"""
        failure = FailureEvent(
            failure_type=failure_type,
            component=component,
            timestamp=time.time(),
            details=details,
            severity=details.get("severity", "medium")
        )
        
        self.failure_history.append(failure)
        self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ•…éšœ: {failure_type.value} in {component}")
        
        return failure
    
    def recover_from_failure(self, failure: FailureEvent, context: Dict[str, Any]) -> bool:
        """ä»æ•…éšœä¸­æ¢å¤"""
        self.logger.info(f"ğŸ”§ å°è¯•æ¢å¤æ•…éšœ: {failure.failure_type.value}")
        
        # æŸ¥æ‰¾åˆé€‚çš„æ¢å¤ç­–ç•¥
        for strategy in self.recovery_strategies:
            if strategy.can_handle(failure):
                self.logger.info(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥: {strategy.__class__.__name__}")
                
                try:
                    success = strategy.recover(failure, context)
                    failure.recovery_attempts += 1
                    
                    if success:
                        failure.resolved = True
                        self.logger.info(f"âœ… æ•…éšœæ¢å¤æˆåŠŸ")
                        
                        # è°ƒç”¨æ¢å¤å›è°ƒ
                        for callback in self.recovery_callbacks:
                            try:
                                callback(failure, True)
                            except Exception as e:
                                self.logger.error(f"æ¢å¤å›è°ƒå¤±è´¥: {e}")
                        
                        return True
                    else:
                        self.logger.warning(f"âŒ ç­–ç•¥æ‰§è¡Œå¤±è´¥")
                        
                except Exception as e:
                    self.logger.error(f"ç­–ç•¥æ‰§è¡Œå¼‚å¸¸: {e}")
        
        self.logger.error(f"âŒ æ‰€æœ‰æ¢å¤ç­–ç•¥éƒ½å¤±è´¥äº†")
        
        # è°ƒç”¨å¤±è´¥å›è°ƒ
        for callback in self.recovery_callbacks:
            try:
                callback(failure, False)
            except Exception as e:
                self.logger.error(f"å¤±è´¥å›è°ƒå¼‚å¸¸: {e}")
        
        return False
    
    def create_checkpoint(self, **kwargs) -> Checkpoint:
        """åˆ›å»ºæ£€æŸ¥ç‚¹çš„ä¾¿æ·æ–¹æ³•"""
        return self.checkpoint_manager.create_checkpoint(**kwargs)
    
    def start_monitoring(self):
        """å¯åŠ¨æ•…éšœç›‘æ§"""
        self.failure_detector.start_monitoring()
    
    def stop_monitoring(self):
        """åœæ­¢æ•…éšœç›‘æ§"""
        self.failure_detector.stop_monitoring()
    
    def get_failure_stats(self) -> Dict[str, Any]:
        """è·å–æ•…éšœç»Ÿè®¡ä¿¡æ¯"""
        if not self.failure_history:
            return {"total_failures": 0}
        
        failure_counts = {}
        resolved_count = 0
        
        for failure in self.failure_history:
            failure_type = failure.failure_type.value
            failure_counts[failure_type] = failure_counts.get(failure_type, 0) + 1
            if failure.resolved:
                resolved_count += 1
        
        return {
            "total_failures": len(self.failure_history),
            "resolved_failures": resolved_count,
            "failure_types": failure_counts,
            "success_rate": resolved_count / len(self.failure_history) if self.failure_history else 0,
            "checkpoints_available": len(self.checkpoint_manager.checkpoints)
        }