# fedcl/automation/model_sync.py
"""
è‡ªåŠ¨æ¨¡å‹åŒæ­¥ç®¡ç†å™¨

å¤„ç†çœŸè”é‚¦ï¼ˆå¤šæœºï¼‰å’Œä¼ªè”é‚¦ï¼ˆæœ¬åœ°ï¼‰ç¯å¢ƒä¸‹çš„æ¨¡å‹å‚æ•°åŒæ­¥ã€‚
æ”¯æŒåŒæ­¥ã€å¼‚æ­¥ã€è‡ªé€‚åº”ç­‰å¤šç§åŒæ­¥ç­–ç•¥ã€‚
"""

import time
import threading
from concurrent.futures import ThreadPoolExecutor, Future, TimeoutError
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Callable, Union
import copy

import torch
import torch.nn as nn
from loguru import logger

from .communication import TransparentCommunication, Message, CommunicationMode


class FeatureTransferMode(Enum):
    """ç‰¹å¾ä¼ é€’æ¨¡å¼"""
    FORWARD_FEATURES = "forward_features"    # å‰å‘ç‰¹å¾ä¼ é€’
    BACKWARD_GRADIENTS = "backward_gradients"  # åå‘æ¢¯åº¦ä¼ é€’
    INTERMEDIATE_RESULTS = "intermediate_results"  # ä¸­é—´ç»“æœä¼ é€’


@dataclass
class FeaturePacket:
    """ç‰¹å¾æ•°æ®åŒ…"""
    packet_id: str
    source_model: str
    target_model: str
    features: torch.Tensor
    metadata: Dict[str, Any]
    timestamp: float
    requires_grad: bool = True


@dataclass 
class GradientPacket:
    """æ¢¯åº¦æ•°æ®åŒ…"""
    packet_id: str
    source_model: str
    target_model: str
    gradients: torch.Tensor
    loss_value: float
    metadata: Dict[str, Any]
    timestamp: float


class SyncMode(Enum):
    """åŒæ­¥æ¨¡å¼"""
    SYNCHRONOUS = "synchronous"      # åŒæ­¥æ¨¡å¼ï¼šç­‰å¾…æ‰€æœ‰å®¢æˆ·ç«¯
    ASYNCHRONOUS = "asynchronous"    # å¼‚æ­¥æ¨¡å¼ï¼šéƒ¨åˆ†å®¢æˆ·ç«¯å®Œæˆå³èšåˆ
    ADAPTIVE = "adaptive"            # è‡ªé€‚åº”æ¨¡å¼ï¼šæ ¹æ®ç½‘ç»œçŠ¶å†µåŠ¨æ€è°ƒæ•´


@dataclass
class ClientUpdate:
    """å®¢æˆ·ç«¯æ›´æ–°"""
    client_id: str
    model_weights: Dict[str, torch.Tensor]
    num_samples: int
    accuracy: float
    loss: float
    timestamp: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SyncConfig:
    """åŒæ­¥é…ç½®"""
    mode: SyncMode = SyncMode.SYNCHRONOUS
    timeout_seconds: float = 300.0
    min_clients_ratio: float = 0.8  # å¼‚æ­¥æ¨¡å¼ä¸‹æœ€å°‘ç­‰å¾…å®¢æˆ·ç«¯æ¯”ä¾‹
    max_staleness_rounds: int = 5   # æœ€å¤§è¿‡æœŸè½®æ¬¡
    compression_enabled: bool = False
    quantization_bits: int = 8
    gradient_clipping: bool = True
    max_gradient_norm: float = 1.0


class ModelSynchronizer:
    """
    æ¨¡å‹åŒæ­¥å™¨
    
    è´Ÿè´£åè°ƒæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯ä¹‹é—´çš„æ¨¡å‹å‚æ•°åŒæ­¥ï¼Œ
    æ”¯æŒçœŸè”é‚¦çš„ç½‘ç»œåŒæ­¥å’Œä¼ªè”é‚¦çš„æœ¬åœ°åŒæ­¥ã€‚
    """
    
    def __init__(
        self, 
        communication: TransparentCommunication,
        sync_config: Optional[SyncConfig] = None,
        is_server: bool = True
    ):
        self.communication = communication
        self.sync_config = sync_config or SyncConfig()
        self.is_server = is_server
        self.logger = logger.bind(component="ModelSynchronizer", role="server" if is_server else "client")
        
        # æœåŠ¡å™¨ç«¯çŠ¶æ€
        if is_server:
            self.client_updates: Dict[str, ClientUpdate] = {}
            self.global_model: Optional[Dict[str, torch.Tensor]] = None
            self.current_round = 0
            self.sync_lock = threading.Lock()
            self.executor = ThreadPoolExecutor(max_workers=10)
            
        # å®¢æˆ·ç«¯çŠ¶æ€
        else:
            self.local_model: Optional[Dict[str, torch.Tensor]] = None
            self.last_sync_round = 0
            
        # æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨
        self._register_handlers()
        
    def _register_handlers(self):
        """æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨"""
        if self.is_server:
            self.communication.register_handler("model_update", self._handle_client_update)
            self.communication.register_handler("sync_request", self._handle_sync_request)
        else:
            self.communication.register_handler("global_model", self._handle_global_model)
            self.communication.register_handler("sync_command", self._handle_sync_command)
    
    def _handle_client_update(self, message: Message):
        """å¤„ç†å®¢æˆ·ç«¯æ¨¡å‹æ›´æ–°"""
        try:
            payload = message.payload
            client_update = ClientUpdate(
                client_id=message.sender,
                model_weights=payload["model_weights"],
                num_samples=payload["num_samples"],
                accuracy=payload["accuracy"],
                loss=payload["loss"],
                timestamp=message.timestamp,
                metadata=payload.get("metadata", {})
            )
            
            with self.sync_lock:
                self.client_updates[message.sender] = client_update
                self.logger.info(f"ğŸ“¥ æ”¶åˆ°å®¢æˆ·ç«¯ {message.sender} çš„æ¨¡å‹æ›´æ–°")
                
        except Exception as e:
            self.logger.error(f"å¤„ç†å®¢æˆ·ç«¯æ›´æ–°å¤±è´¥: {e}")
    
    def _handle_global_model(self, message: Message):
        """å¤„ç†å…¨å±€æ¨¡å‹æ›´æ–°"""
        try:
            payload = message.payload
            self.global_model = payload["model_weights"]
            self.last_sync_round = payload["round"]
            
            self.logger.info(f"ğŸ“¥ æ”¶åˆ°å…¨å±€æ¨¡å‹æ›´æ–° - è½®æ¬¡: {self.last_sync_round}")
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å…¨å±€æ¨¡å‹å¤±è´¥: {e}")
    
    def _handle_sync_request(self, message: Message):
        """å¤„ç†åŒæ­¥è¯·æ±‚"""
        self.logger.info(f"æ”¶åˆ°æ¥è‡ª {message.sender} çš„åŒæ­¥è¯·æ±‚")
    
    def _handle_sync_command(self, message: Message):
        """å¤„ç†åŒæ­¥å‘½ä»¤"""
        command = message.payload.get("command")
        if command == "start_training":
            self.logger.info("ğŸ“¡ æ”¶åˆ°å¼€å§‹è®­ç»ƒå‘½ä»¤")
        elif command == "pause_training":
            self.logger.info("â¸ï¸ æ”¶åˆ°æš‚åœè®­ç»ƒå‘½ä»¤")
    
    def aggregate_models(self, client_list: List[str]) -> Optional[Dict[str, torch.Tensor]]:
        """
        èšåˆå®¢æˆ·ç«¯æ¨¡å‹ - æœåŠ¡å™¨ç«¯
        
        Args:
            client_list: æœŸæœ›çš„å®¢æˆ·ç«¯åˆ—è¡¨
            
        Returns:
            èšåˆåçš„å…¨å±€æ¨¡å‹æƒé‡
        """
        if not self.is_server:
            raise ValueError("åªæœ‰æœåŠ¡å™¨ç«¯å¯ä»¥æ‰§è¡Œæ¨¡å‹èšåˆ")
            
        self.logger.info(f"ğŸ”„ å¼€å§‹æ¨¡å‹èšåˆ - è½®æ¬¡: {self.current_round + 1}")
        
        # ç­‰å¾…å®¢æˆ·ç«¯æ›´æ–°
        collected_updates = self._collect_client_updates(client_list)
        
        if not collected_updates:
            self.logger.warning("æœªæ”¶åˆ°ä»»ä½•å®¢æˆ·ç«¯æ›´æ–°")
            return None
        
        # æ‰§è¡Œèšåˆ
        if self.sync_config.mode == SyncMode.SYNCHRONOUS:
            aggregated_model = self._federated_averaging(collected_updates)
        elif self.sync_config.mode == SyncMode.ASYNCHRONOUS:
            aggregated_model = self._async_aggregation(collected_updates)
        else:  # ADAPTIVE
            aggregated_model = self._adaptive_aggregation(collected_updates)
        
        # æ›´æ–°å…¨å±€æ¨¡å‹
        self.global_model = aggregated_model
        self.current_round += 1
        
        self.logger.info(f"âœ… æ¨¡å‹èšåˆå®Œæˆ - å‚ä¸å®¢æˆ·ç«¯: {len(collected_updates)}")
        return aggregated_model
    
    def _collect_client_updates(self, client_list: List[str]) -> List[ClientUpdate]:
        """æ”¶é›†å®¢æˆ·ç«¯æ›´æ–°"""
        collected_updates = []
        timeout = self.sync_config.timeout_seconds
        start_time = time.time()
        
        while True:
            current_time = time.time()
            elapsed = current_time - start_time
            
            # æ£€æŸ¥è¶…æ—¶
            if elapsed > timeout:
                self.logger.warning(f"â° æ”¶é›†å®¢æˆ·ç«¯æ›´æ–°è¶…æ—¶: {elapsed:.2f}s")
                break
            
            # æ£€æŸ¥å·²æ”¶é›†çš„æ›´æ–°
            with self.sync_lock:
                for client_id in client_list:
                    if client_id in self.client_updates and client_id not in [u.client_id for u in collected_updates]:
                        collected_updates.append(self.client_updates[client_id])
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åŒæ­¥æ¡ä»¶
            if self._should_proceed_aggregation(collected_updates, client_list):
                break
                
            time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
        
        return collected_updates
    
    def _should_proceed_aggregation(self, collected_updates: List[ClientUpdate], client_list: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼€å§‹èšåˆ"""
        collected_ratio = len(collected_updates) / len(client_list)
        
        if self.sync_config.mode == SyncMode.SYNCHRONOUS:
            return collected_ratio >= 1.0
        elif self.sync_config.mode == SyncMode.ASYNCHRONOUS:
            return collected_ratio >= self.sync_config.min_clients_ratio
        else:  # ADAPTIVE
            # è‡ªé€‚åº”é€»è¾‘ï¼šæ ¹æ®å†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´
            return collected_ratio >= max(0.5, self.sync_config.min_clients_ratio)
    
    def _federated_averaging(self, updates: List[ClientUpdate]) -> Dict[str, torch.Tensor]:
        """è”é‚¦å¹³å‡èšåˆ"""
        if not updates:
            return {}
        
        total_samples = sum(update.num_samples for update in updates)
        aggregated_weights = {}
        
        # è·å–ç¬¬ä¸€ä¸ªæ›´æ–°çš„æƒé‡ç»“æ„
        first_weights = updates[0].model_weights
        
        for param_name in first_weights.keys():
            weighted_sum = torch.zeros_like(first_weights[param_name])
            
            for update in updates:
                weight = update.num_samples / total_samples
                weighted_sum += weight * update.model_weights[param_name]
            
            aggregated_weights[param_name] = weighted_sum
        
        self.logger.debug(f"ğŸ“Š è”é‚¦å¹³å‡å®Œæˆ - æ€»æ ·æœ¬æ•°: {total_samples}")
        return aggregated_weights
    
    def _async_aggregation(self, updates: List[ClientUpdate]) -> Dict[str, torch.Tensor]:
        """å¼‚æ­¥èšåˆ"""
        # è¿‡æ»¤è¿‡æœŸæ›´æ–°
        current_time = time.time()
        fresh_updates = [
            update for update in updates 
            if current_time - update.timestamp < self.sync_config.timeout_seconds
        ]
        
        if not fresh_updates:
            self.logger.warning("æ‰€æœ‰æ›´æ–°éƒ½å·²è¿‡æœŸï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ›´æ–°")
            fresh_updates = updates
        
        return self._federated_averaging(fresh_updates)
    
    def _adaptive_aggregation(self, updates: List[ClientUpdate]) -> Dict[str, torch.Tensor]:
        """è‡ªé€‚åº”èšåˆ"""
        # æ ¹æ®å®¢æˆ·ç«¯æ€§èƒ½åŠ æƒ
        performance_weights = []
        for update in updates:
            # ç»¼åˆè€ƒè™‘å‡†ç¡®ç‡å’Œæ—¶æ•ˆæ€§
            time_weight = 1.0 / (1.0 + time.time() - update.timestamp)
            accuracy_weight = update.accuracy
            performance_weight = time_weight * accuracy_weight
            performance_weights.append(performance_weight)
        
        # å½’ä¸€åŒ–æƒé‡
        total_performance = sum(performance_weights)
        if total_performance > 0:
            performance_weights = [w / total_performance for w in performance_weights]
        else:
            performance_weights = [1.0 / len(updates)] * len(updates)
        
        # åŠ æƒèšåˆ
        aggregated_weights = {}
        first_weights = updates[0].model_weights
        
        for param_name in first_weights.keys():
            weighted_sum = torch.zeros_like(first_weights[param_name])
            
            for update, weight in zip(updates, performance_weights):
                weighted_sum += weight * update.model_weights[param_name]
            
            aggregated_weights[param_name] = weighted_sum
        
        self.logger.debug("ğŸ“Š è‡ªé€‚åº”èšåˆå®Œæˆ")
        return aggregated_weights
    
    def distribute_global_model(self, client_list: List[str]) -> Dict[str, bool]:
        """
        åˆ†å‘å…¨å±€æ¨¡å‹ - æœåŠ¡å™¨ç«¯
        
        Args:
            client_list: å®¢æˆ·ç«¯åˆ—è¡¨
            
        Returns:
            åˆ†å‘ç»“æœ
        """
        if not self.is_server:
            raise ValueError("åªæœ‰æœåŠ¡å™¨ç«¯å¯ä»¥åˆ†å‘å…¨å±€æ¨¡å‹")
        
        if self.global_model is None:
            self.logger.error("å…¨å±€æ¨¡å‹ä¸ºç©ºï¼Œæ— æ³•åˆ†å‘")
            return {client: False for client in client_list}
        
        self.logger.info(f"ğŸ“¤ å¼€å§‹åˆ†å‘å…¨å±€æ¨¡å‹åˆ° {len(client_list)} ä¸ªå®¢æˆ·ç«¯")
        
        # å‡†å¤‡æ¨¡å‹æ•°æ®
        model_data = {
            "model_weights": self.global_model,
            "round": self.current_round,
            "timestamp": time.time(),
            "metadata": {
                "aggregation_mode": self.sync_config.mode.value,
                "num_participants": len(self.client_updates)
            }
        }
        
        # å¹¿æ’­æ¨¡å‹
        results = self.communication.broadcast_global_model(client_list, model_data)
        
        success_count = sum(1 for success in results.values() if success)
        self.logger.info(f"ğŸ“¤ å…¨å±€æ¨¡å‹åˆ†å‘å®Œæˆ: {success_count}/{len(client_list)} æˆåŠŸ")
        
        return results
    
    def upload_model_update(
        self, 
        model_weights: Dict[str, torch.Tensor],
        num_samples: int,
        accuracy: float,
        loss: float,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        ä¸Šä¼ æ¨¡å‹æ›´æ–° - å®¢æˆ·ç«¯
        
        Args:
            model_weights: æ¨¡å‹æƒé‡
            num_samples: è®­ç»ƒæ ·æœ¬æ•°
            accuracy: è®­ç»ƒå‡†ç¡®ç‡
            loss: è®­ç»ƒæŸå¤±
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if self.is_server:
            raise ValueError("æœåŠ¡å™¨ç«¯ä¸èƒ½ä¸Šä¼ æ¨¡å‹æ›´æ–°")
        
        self.logger.info(f"ğŸ“¤ ä¸Šä¼ æ¨¡å‹æ›´æ–° - æ ·æœ¬æ•°: {num_samples}, å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # å‹ç¼©æ¨¡å‹æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.sync_config.compression_enabled:
            model_weights = self._compress_model_weights(model_weights)
        
        # å‡†å¤‡æ›´æ–°æ•°æ®
        update_data = {
            "model_weights": model_weights,
            "num_samples": num_samples,
            "accuracy": accuracy,
            "loss": loss,
            "metadata": metadata or {}
        }
        
        # å‘é€åˆ°æœåŠ¡å™¨
        success = self.communication.send_model_update("server", update_data)
        
        if success:
            self.logger.info("âœ… æ¨¡å‹æ›´æ–°ä¸Šä¼ æˆåŠŸ")
        else:
            self.logger.error("âŒ æ¨¡å‹æ›´æ–°ä¸Šä¼ å¤±è´¥")
            
        return success
    
    def _compress_model_weights(self, weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """å‹ç¼©æ¨¡å‹æƒé‡"""
        if self.sync_config.quantization_bits < 32:
            # ç®€å•çš„é‡åŒ–å‹ç¼©
            compressed_weights = {}
            for name, tensor in weights.items():
                # é‡åŒ–åˆ°æŒ‡å®šä½æ•°
                scale = tensor.abs().max()
                quantized = (tensor / scale * (2 ** (self.sync_config.quantization_bits - 1) - 1)).round()
                compressed_weights[name] = quantized * scale / (2 ** (self.sync_config.quantization_bits - 1) - 1)
            return compressed_weights
        return weights
    
    def get_sync_stats(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        if self.is_server:
            return {
                "current_round": self.current_round,
                "connected_clients": len(self.client_updates),
                "sync_mode": self.sync_config.mode.value,
                "has_global_model": self.global_model is not None
            }
        else:
            return {
                "last_sync_round": self.last_sync_round,
                "has_local_model": self.local_model is not None,
                "sync_mode": self.sync_config.mode.value
            }
    
    def reset_round(self):
        """é‡ç½®è½®æ¬¡çŠ¶æ€"""
        if self.is_server:
            with self.sync_lock:
                self.client_updates.clear()
            self.logger.info(f"ğŸ”„ é‡ç½®è½®æ¬¡çŠ¶æ€ - å½“å‰è½®æ¬¡: {self.current_round}")


class TransparentFeatureSync:
    """
    é€æ˜ç‰¹å¾åŒæ­¥å™¨
    
    æ”¯æŒæœåŠ¡å™¨-å®¢æˆ·ç«¯ä¸­é—´ç‰¹å¾çš„é€æ˜ä¼ é€’ï¼š
    1. æœåŠ¡å™¨å‘å®¢æˆ·ç«¯ä¼ é€’ä¸­é—´ç‰¹å¾
    2. å®¢æˆ·ç«¯æ¥æ”¶ç‰¹å¾ã€è®¡ç®—æŸå¤±ã€æ¢¯åº¦å›ä¼ 
    3. æ•´ä¸ªè¿‡ç¨‹å¯¹ç”¨æˆ·å®Œå…¨é€æ˜
    """
    
    def __init__(self, communication: TransparentCommunication, node_id: str):
        self.communication = communication
        self.node_id = node_id
        self.logger = logger.bind(component="TransparentFeatureSync", node=node_id)
        
        # ç‰¹å¾ä¼ é€’çš„å›è°ƒå‡½æ•°
        self.feature_handlers: Dict[str, Callable] = {}
        self.gradient_handlers: Dict[str, Callable] = {}
        
        # æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨
        self._register_feature_handlers()
        
        # ç‰¹å¾ä¼ é€’çš„å¾…å¤„ç†é˜Ÿåˆ—
        self.pending_features: Dict[str, FeaturePacket] = {}
        self.pending_gradients: Dict[str, GradientPacket] = {}
    
    def _register_feature_handlers(self):
        """æ³¨å†Œç‰¹å¾ä¼ é€’çš„æ¶ˆæ¯å¤„ç†å™¨"""
        self.communication.register_handler("forward_features", self._handle_forward_features)
        self.communication.register_handler("backward_gradients", self._handle_backward_gradients)
        self.communication.register_handler("intermediate_results", self._handle_intermediate_results)
    
    def send_features_to_client(
        self, 
        client_id: str, 
        model_name: str,
        features: torch.Tensor, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        æœåŠ¡å™¨å‘å®¢æˆ·ç«¯å‘é€ä¸­é—´ç‰¹å¾ï¼ˆé€æ˜ï¼‰
        
        Args:
            client_id: ç›®æ ‡å®¢æˆ·ç«¯
            model_name: æ¨¡å‹åç§°
            features: ä¸­é—´ç‰¹å¾å¼ é‡
            metadata: é™„åŠ å…ƒæ•°æ®
            
        Returns:
            ç‰¹å¾åŒ…IDï¼Œç”¨äºè¿½è¸ªå›ä¼ çš„æ¢¯åº¦
        """
        packet_id = f"feat_{int(time.time() * 1000000)}"
        
        feature_packet = FeaturePacket(
            packet_id=packet_id,
            source_model=f"{self.node_id}_{model_name}",
            target_model=f"{client_id}_{model_name}",
            features=features,
            metadata=metadata or {},
            timestamp=time.time(),
            requires_grad=True
        )
        
        # å‘é€ç‰¹å¾æ•°æ®
        message_payload = {
            "packet_id": packet_id,
            "source_model": feature_packet.source_model,
            "target_model": feature_packet.target_model,
            "features": features.detach().cpu(),  # ä¼ è¾“æ—¶ç§»åŠ¨åˆ°CPU
            "metadata": feature_packet.metadata,
            "timestamp": feature_packet.timestamp,
            "requires_grad": feature_packet.requires_grad
        }
        
        success = self.communication.send_model_update(client_id, {
            "message_type": "forward_features",
            "payload": message_payload
        })
        
        if success:
            # è®°å½•å¾…å¤„ç†çš„ç‰¹å¾ï¼Œç­‰å¾…æ¢¯åº¦å›ä¼ 
            self.pending_features[packet_id] = feature_packet
            self.logger.info(f"ğŸ“¤ ç‰¹å¾å·²å‘é€è‡³ {client_id}: {features.shape}")
        else:
            self.logger.error(f"âŒ ç‰¹å¾å‘é€å¤±è´¥è‡³ {client_id}")
            
        return packet_id
    
    def _handle_forward_features(self, message: Message):
        """å¤„ç†æ¥æ”¶åˆ°çš„å‰å‘ç‰¹å¾ï¼ˆå®¢æˆ·ç«¯ï¼‰"""
        try:
            payload = message.payload["payload"]
            
            feature_packet = FeaturePacket(
                packet_id=payload["packet_id"],
                source_model=payload["source_model"],
                target_model=payload["target_model"],
                features=payload["features"].requires_grad_(payload["requires_grad"]),
                metadata=payload["metadata"],
                timestamp=payload["timestamp"],
                requires_grad=payload["requires_grad"]
            )
            
            self.logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°ç‰¹å¾: {feature_packet.features.shape}")
            
            # è°ƒç”¨ç”¨æˆ·æ³¨å†Œçš„ç‰¹å¾å¤„ç†å™¨
            handler = self.feature_handlers.get(feature_packet.target_model)
            if handler:
                # ç”¨æˆ·å¤„ç†ç‰¹å¾ï¼Œè¿”å›æŸå¤±å’Œæ¢¯åº¦
                loss, gradients = handler(feature_packet.features, feature_packet.metadata)
                
                # è‡ªåŠ¨å›ä¼ æ¢¯åº¦
                self._send_gradients_back(feature_packet.packet_id, 
                                        feature_packet.source_model,
                                        gradients, loss, feature_packet.metadata)
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°ç‰¹å¾å¤„ç†å™¨: {feature_packet.target_model}")
                
        except Exception as e:
            self.logger.error(f"å¤„ç†å‰å‘ç‰¹å¾å¤±è´¥: {e}")
    
    def _send_gradients_back(
        self, 
        packet_id: str, 
        target_model: str,
        gradients: torch.Tensor, 
        loss_value: float,
        metadata: Dict[str, Any]
    ):
        """è‡ªåŠ¨å›ä¼ æ¢¯åº¦åˆ°æœåŠ¡å™¨"""
        
        gradient_packet = GradientPacket(
            packet_id=packet_id,
            source_model=self.node_id,
            target_model=target_model,
            gradients=gradients,
            loss_value=loss_value,
            metadata=metadata,
            timestamp=time.time()
        )
        
        # æå–æœåŠ¡å™¨ID
        server_id = target_model.split("_")[0]
        
        message_payload = {
            "packet_id": packet_id,
            "source_model": gradient_packet.source_model,
            "target_model": gradient_packet.target_model,
            "gradients": gradients.detach().cpu(),
            "loss_value": loss_value,
            "metadata": gradient_packet.metadata,
            "timestamp": gradient_packet.timestamp
        }
        
        success = self.communication.send_model_update(server_id, {
            "message_type": "backward_gradients",
            "payload": message_payload
        })
        
        if success:
            self.logger.info(f"ğŸ”™ æ¢¯åº¦å·²å›ä¼ : loss={loss_value:.6f}")
        else:
            self.logger.error(f"âŒ æ¢¯åº¦å›ä¼ å¤±è´¥")
    
    def _handle_backward_gradients(self, message: Message):
        """å¤„ç†æ¥æ”¶åˆ°çš„åå‘æ¢¯åº¦ï¼ˆæœåŠ¡å™¨ï¼‰"""
        try:
            payload = message.payload["payload"]
            packet_id = payload["packet_id"]
            
            # æŸ¥æ‰¾å¯¹åº”çš„ç‰¹å¾åŒ…
            if packet_id in self.pending_features:
                feature_packet = self.pending_features[packet_id]
                
                gradient_packet = GradientPacket(
                    packet_id=packet_id,
                    source_model=payload["source_model"],
                    target_model=payload["target_model"],
                    gradients=payload["gradients"],
                    loss_value=payload["loss_value"],
                    metadata=payload["metadata"],
                    timestamp=payload["timestamp"]
                )
                
                self.logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°æ¢¯åº¦: loss={gradient_packet.loss_value:.6f}")
                
                # è°ƒç”¨ç”¨æˆ·æ³¨å†Œçš„æ¢¯åº¦å¤„ç†å™¨
                handler = self.gradient_handlers.get(feature_packet.source_model)
                if handler:
                    handler(feature_packet.features, gradient_packet.gradients, gradient_packet.loss_value)
                
                # æ¸…ç†å·²å¤„ç†çš„ç‰¹å¾åŒ…
                del self.pending_features[packet_id]
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°å¯¹åº”çš„ç‰¹å¾åŒ…: {packet_id}")
                
        except Exception as e:
            self.logger.error(f"å¤„ç†åå‘æ¢¯åº¦å¤±è´¥: {e}")
    
    def register_feature_handler(self, model_name: str, handler: Callable):
        """
        æ³¨å†Œç‰¹å¾å¤„ç†å™¨ï¼ˆå®¢æˆ·ç«¯ä½¿ç”¨ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            handler: å¤„ç†å‡½æ•°ï¼Œç­¾åä¸º (features, metadata) -> (loss, gradients)
        """
        full_model_name = f"{self.node_id}_{model_name}"
        self.feature_handlers[full_model_name] = handler
        self.logger.info(f"âœ… æ³¨å†Œç‰¹å¾å¤„ç†å™¨: {full_model_name}")
    
    def register_gradient_handler(self, model_name: str, handler: Callable):
        """
        æ³¨å†Œæ¢¯åº¦å¤„ç†å™¨ï¼ˆæœåŠ¡å™¨ä½¿ç”¨ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°  
            handler: å¤„ç†å‡½æ•°ï¼Œç­¾åä¸º (features, gradients, loss) -> None
        """
        full_model_name = f"{self.node_id}_{model_name}"
        self.gradient_handlers[full_model_name] = handler
        self.logger.info(f"âœ… æ³¨å†Œæ¢¯åº¦å¤„ç†å™¨: {full_model_name}")
    
    def _handle_intermediate_results(self, message: Message):
        """å¤„ç†ä¸­é—´ç»“æœä¼ é€’"""
        # ç”¨äºæ”¯æŒæ›´å¤æ‚çš„ä¸­é—´è®¡ç®—ç»“æœä¼ é€’
        pass


class MultiModelManager:
    """
    å¤šæ¨¡å‹ç®¡ç†å™¨
    
    æ”¯æŒå®¢æˆ·ç«¯å¤šä¸ªæ¨¡å‹çš„é€æ˜ç®¡ç†ï¼š
    1. å¤šä¸ªæ¨¡å‹å®ä¾‹çš„è‡ªåŠ¨ç®¡ç†
    2. æ¨¡å‹é—´é€šä¿¡çš„è‡ªåŠ¨åè°ƒ
    3. ä¸šåŠ¡é€»è¾‘ä¸é€šä¿¡çš„å®Œå…¨è§£è€¦
    """
    
    def __init__(self, communication: TransparentCommunication, node_id: str):
        self.communication = communication
        self.node_id = node_id
        self.logger = logger.bind(component="MultiModelManager", node=node_id)
        
        # æ¨¡å‹å®ä¾‹ç®¡ç†
        self.model_instances: Dict[str, Any] = {}
        self.model_synchronizers: Dict[str, ModelSynchronizer] = {}
        
        # ç‰¹å¾åŒæ­¥å™¨
        self.feature_sync = TransparentFeatureSync(communication, node_id)
        
        # æ¨¡å‹é—´é€šä¿¡è·¯ç”±
        self.model_routes: Dict[str, List[str]] = {}
        
    def register_model(self, model_name: str, model_instance: Any, 
                      sync_config: Optional[SyncConfig] = None) -> str:
        """
        æ³¨å†Œæ¨¡å‹å®ä¾‹ï¼ˆå¯¹ç”¨æˆ·é€æ˜ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_instance: æ¨¡å‹å®ä¾‹
            sync_config: åŒæ­¥é…ç½®
            
        Returns:
            æ¨¡å‹çš„å…¨å±€ID
        """
        full_model_id = f"{self.node_id}_{model_name}"
        
        # æ³¨å†Œæ¨¡å‹å®ä¾‹
        self.model_instances[full_model_id] = model_instance
        
        # åˆ›å»ºä¸“ç”¨çš„æ¨¡å‹åŒæ­¥å™¨
        model_sync = ModelSynchronizer(
            communication=self.communication,
            sync_config=sync_config,
            is_server=False  # é»˜è®¤ä½œä¸ºå®¢æˆ·ç«¯
        )
        self.model_synchronizers[full_model_id] = model_sync
        
        self.logger.info(f"âœ… æ¨¡å‹æ³¨å†ŒæˆåŠŸ: {full_model_id}")
        return full_model_id
    
    def setup_model_communication(self, model_name: str, 
                                feature_handler: Optional[Callable] = None,
                                gradient_handler: Optional[Callable] = None):
        """
        è®¾ç½®æ¨¡å‹çš„é€æ˜é€šä¿¡ï¼ˆå¯¹ç”¨æˆ·é€æ˜ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            feature_handler: ç‰¹å¾å¤„ç†å‡½æ•°
            gradient_handler: æ¢¯åº¦å¤„ç†å‡½æ•°
        """
        if feature_handler:
            self.feature_sync.register_feature_handler(model_name, feature_handler)
            
        if gradient_handler:
            self.feature_sync.register_gradient_handler(model_name, gradient_handler)
            
        self.logger.info(f"âœ… æ¨¡å‹é€šä¿¡è®¾ç½®å®Œæˆ: {model_name}")
    
    def auto_sync_model(self, model_name: str, target_nodes: List[str]) -> bool:
        """
        è‡ªåŠ¨åŒæ­¥æ¨¡å‹ï¼ˆå¯¹ç”¨æˆ·é€æ˜ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            target_nodes: ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            åŒæ­¥æ˜¯å¦æˆåŠŸ
        """
        full_model_id = f"{self.node_id}_{model_name}"
        
        if full_model_id not in self.model_synchronizers:
            self.logger.error(f"æ¨¡å‹æœªæ³¨å†Œ: {model_name}")
            return False
            
        model_sync = self.model_synchronizers[full_model_id]
        model_instance = self.model_instances[full_model_id]
        
        # è·å–æ¨¡å‹æƒé‡
        if hasattr(model_instance, 'state_dict'):
            model_weights = model_instance.state_dict()
        else:
            self.logger.warning(f"æ¨¡å‹ä¸æ”¯æŒ state_dict: {model_name}")
            return False
            
        # è‡ªåŠ¨ä¸Šä¼ æ¨¡å‹æ›´æ–°
        success = model_sync.upload_model_update(
            model_weights=model_weights,
            num_samples=1000,  # å¯ä»¥ä»æ¨¡å‹å®ä¾‹è·å–
            accuracy=0.95,     # å¯ä»¥ä»æ¨¡å‹å®ä¾‹è·å–
            loss=0.05,         # å¯ä»¥ä»æ¨¡å‹å®ä¾‹è·å–
            metadata={"model_name": model_name, "target_nodes": target_nodes}
        )
        
        return success
    
    def get_model_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "registered_models": len(self.model_instances),
            "active_synchronizers": len(self.model_synchronizers),
            "model_list": list(self.model_instances.keys()),
            "communication_stats": self.communication.get_stats()
        }
    """
    è‡ªåŠ¨æ¨¡å‹åŒæ­¥ç®¡ç†å™¨
    
    æä¾›æ›´é«˜çº§çš„è‡ªåŠ¨åŒ–åŒæ­¥æ¥å£ï¼Œé›†æˆé€šä¿¡å’ŒåŒæ­¥é€»è¾‘
    """
    
    def __init__(
        self,
        node_id: str,
        communication_mode: CommunicationMode,
        is_server: bool = False,
        sync_config: Optional[SyncConfig] = None,
        network_config = None
    ):
        self.node_id = node_id
        self.is_server = is_server
        self.logger = logger.bind(component="AutoModelSync", node=node_id)
        
        # åˆå§‹åŒ–é€æ˜é€šä¿¡
        self.communication = TransparentCommunication(
            node_id=node_id,
            mode=communication_mode,
            config=network_config,
            is_server=is_server
        )
        
        # åˆå§‹åŒ–æ¨¡å‹åŒæ­¥å™¨
        self.synchronizer = ModelSynchronizer(
            communication=self.communication,
            sync_config=sync_config,
            is_server=is_server
        )
        
        self.is_running = False
    
    def start(self) -> bool:
        """å¯åŠ¨è‡ªåŠ¨åŒæ­¥"""
        if self.communication.start():
            self.is_running = True
            self.logger.info("ğŸš€ è‡ªåŠ¨æ¨¡å‹åŒæ­¥å·²å¯åŠ¨")
            return True
        return False
    
    def stop(self) -> bool:
        """åœæ­¢è‡ªåŠ¨åŒæ­¥"""
        self.is_running = False
        return self.communication.stop()
    
    def sync_global_model(self, learner_instances: List[Any]):
        """è‡ªåŠ¨åŒæ­¥å…¨å±€æ¨¡å‹åˆ°æ‰€æœ‰å­¦ä¹ å™¨ - ç®€åŒ–ç‰ˆ"""
        if not self.is_server:
            return
            
        client_ids = [f"client_{i}" for i in range(len(learner_instances))]
        
        # æ”¶é›†æ›´æ–°å¹¶èšåˆ
        aggregated_model = self.synchronizer.aggregate_models(client_ids)
        
        if aggregated_model:
            # åˆ†å‘æ¨¡å‹
            self.synchronizer.distribute_global_model(client_ids)
    
    def collect_model_updates(self, learner_instances: List[Any]):
        """è‡ªåŠ¨æ”¶é›†æ¨¡å‹æ›´æ–° - ç®€åŒ–ç‰ˆ"""
        pass  # åœ¨å®é™…å®ç°ä¸­ä¼šè°ƒç”¨å…·ä½“çš„å­¦ä¹ å™¨æ¥å£
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        comm_stats = self.communication.get_stats()
        sync_stats = self.synchronizer.get_sync_stats()
        
        return {
            "communication": comm_stats,
            "synchronization": sync_stats,
            "is_running": self.is_running
        }