# fedcl/federation/coordinators/multi_learner_federated_client.py
"""
å¤šlearnerè”é‚¦å®¢æˆ·ç«¯

é‡æ„åçš„å®ç°ï¼Œä½¿ç”¨å±‚çº§çŠ¶æ€ç®¡ç†ç³»ç»Ÿï¼š
- åè°ƒå±‚çŠ¶æ€ç®¡ç†ï¼ˆClientLifecycleStateï¼‰
- ä¸æ§åˆ¶å±‚çŠ¶æ€è‡ªåŠ¨åŒæ­¥
- ä¿æŒåŸæœ‰æ‰€æœ‰åŠŸèƒ½
- æä¾›æ›´å¥½çš„çŠ¶æ€ä¸€è‡´æ€§ä¿è¯
"""

import time
import threading
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, field
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from loguru import logger
from omegaconf import OmegaConf
import traceback

# åŸºç¡€é€šä¿¡ç±»å¯¼å…¥
from .base import (
    FederatedCommunicator, 
    CommunicationConfig, 
    CommunicatorRole, 
    MessageType
)

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from ...core.execution_context import ExecutionContext
from ...core.base_learner import BaseLearner

# çŠ¶æ€ç®¡ç†å¯¼å…¥
from ...federation.state.state_manager import (
    ClientLifecycleState, 
    TrainingPhaseState,
    
)
from ...federation.state.hierarchical_state_manager import create_hierarchical_state_manager

# å…¶ä»–ç»„ä»¶å¯¼å…¥
from ..exceptions import FederationError
from ...config.config_manager import DictConfig


@dataclass
class MultiLearnerTrainingResult:
    """å¤šlearnerè®­ç»ƒç»“æœ"""
    client_id: str
    round_id: int
    phase_results: Dict[str, Any]  # å„é˜¶æ®µçš„è®­ç»ƒç»“æœ
    aggregated_model_update: Dict[str, torch.Tensor]  # èšåˆåçš„æ¨¡å‹æ›´æ–°
    total_samples: int
    training_metrics: Dict[str, Any]  # èšåˆåçš„è®­ç»ƒæŒ‡æ ‡
    total_training_time: float
    learner_contributions: Dict[str, float]  # å„learnerçš„è´¡çŒ®æƒé‡
    evaluation_results: Dict[str, Any] = field(default_factory=dict)  # è¯„ä¼°ç»“æœ
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LearnerInfo:
    """Learnerä¿¡æ¯"""
    learner_id: str
    learner_type: str
    learner_instance: BaseLearner
    dataloader_id: str
    scheduler_id: str
    priority: int = 0
    is_active: bool = True


@dataclass
class PhaseResult:
    """è®­ç»ƒé˜¶æ®µç»“æœ"""
    phase_name: str
    executed_epochs: List[int]
    metrics: Dict[str, List[float]]
    final_state: Dict[str, Any]
    exported_knowledge: Optional[Dict[str, Any]]
    execution_time: float
    memory_usage: Dict[str, float]
    error_info: Optional[Exception] = None
    success: bool = True
    
    def get_final_metrics(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆæŒ‡æ ‡"""
        final_metrics = {}
        for metric_name, metric_values in self.metrics.items():
            if metric_values:
                final_metrics[metric_name] = metric_values[-1]
        return final_metrics


class MultiLearnerFederatedClient(FederatedCommunicator):
    """
    å¤šlearnerè”é‚¦å®¢æˆ·ç«¯åè°ƒå™¨
    
    é‡æ„åçš„å®ç°ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨HierarchicalStateManagerè¿›è¡Œå±‚çº§çŠ¶æ€ç®¡ç†
    2. åè°ƒå±‚ä¸“æ³¨äºå®¢æˆ·ç«¯ç”Ÿå‘½å‘¨æœŸç®¡ç†
    3. ä¸è®­ç»ƒå¼•æ“çš„æ§åˆ¶å±‚çŠ¶æ€è‡ªåŠ¨åŒæ­¥
    4. ä¿æŒæ‰€æœ‰åŸæœ‰åŠŸèƒ½ä¸å˜
    5. æä¾›æ›´å¥½çš„çŠ¶æ€ç›‘æ§å’Œè°ƒè¯•èƒ½åŠ›
    
    ä¸»è¦èŒè´£ï¼š
    - å®¢æˆ·ç«¯ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆåè°ƒå±‚çŠ¶æ€ï¼‰
    - ä¸æœåŠ¡ç«¯çš„é€šä¿¡åè°ƒ
    - å¤šlearnerç»„ä»¶çš„åˆ›å»ºå’Œç®¡ç†
    - è®­ç»ƒä»»åŠ¡çš„è°ƒåº¦å’Œç»“æœèšåˆ
    """
    
    def __init__(self, client_id: str, config: DictConfig):
        """
        åˆå§‹åŒ–å¤šlearnerè”é‚¦å®¢æˆ·ç«¯
        
        Args:
            client_id: å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†
            config: å®¢æˆ·ç«¯å®Œæ•´é…ç½®
        """
        try:
            # æ„å»ºé€šä¿¡é…ç½®
            comm_config = self._build_communication_config(client_id, config)
            
            # åˆå§‹åŒ–é€šä¿¡åŸºç±»
            super().__init__(comm_config)
            
            # åŸºæœ¬å±æ€§
            self.client_id = client_id
            self.client_config = config
            # æ³¨æ„ï¼šä¸è¦é‡æ–°ç»‘å®š self.loggerï¼ŒåŸºç±»å·²ç»æ­£ç¡®è®¾ç½®äº†ç»„ä»¶æ—¥å¿—å™¨
            
            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            self.context = self._create_execution_context(config)
            
            # åˆ›å»ºå±‚çº§çŠ¶æ€ç®¡ç†å™¨
            self.hierarchical_state_manager = create_hierarchical_state_manager(
                self.context, 
                client_id,
                max_history=config.get('state_management', {}).get('max_history', 1000),
                enable_validation=config.get('state_management', {}).get('enable_validation', True)
            )
            
            # å¤šlearnerç›¸å…³å±æ€§
            self.learners_info: Dict[str, LearnerInfo] = {}
            self.dataloaders: Dict[str, DataLoader] = {}
            
            # è®­ç»ƒå¼•æ“ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼‰
            self.enhanced_training_engine = None
            
            # å®¢æˆ·ç«¯çŠ¶æ€
            self.current_round = 0
            self.is_training = False
            self.received_global_models: Dict[str, torch.nn.Module] = {}
            
            # è®­ç»ƒå†å²
            self.training_history: List[MultiLearnerTrainingResult] = []
            
            # è®­ç»ƒçº¿ç¨‹ç®¡ç†
            self.training_thread: Optional[threading.Thread] = None
            self.training_lock = threading.RLock()
            
            # åˆå§‹åŒ–å¤šlearnerç»„ä»¶
            self._initialize_multi_learner_components()
            
            # åˆ›å»ºè®­ç»ƒå¼•æ“
            self._initialize_training_engine()
            
            # æ³¨å†ŒçŠ¶æ€å›è°ƒ
            self._register_state_callbacks()
            
            # æ³¨å†ŒHookï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            self._register_hooks(config.get('hooks', {}))
            
            self.logger.debug(f"å¤šå­¦ä¹ å™¨è”é‚¦å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {client_id}")
            self.logger.debug(f"Learners: {list(self.learners_info.keys())}")
            self.logger.debug(f"çŠ¶æ€ç®¡ç†: å±‚çº§åŒ–çŠ¶æ€ç®¡ç†å™¨")
            
        except Exception as e:
            self.logger.error(f"MultiLearnerFederatedClientåˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise FederationError(f"MultiLearnerFederatedClient initialization failed: {e}")
    
    def _build_communication_config(self, client_id: str, config: DictConfig) -> CommunicationConfig:
        """æ„å»ºé€šä¿¡é…ç½®"""
        comm_settings = config.get('communication', {})
        
        return CommunicationConfig(
            role=CommunicatorRole.CLIENT,
            component_id=client_id,
            host=comm_settings.get('host', 'localhost'),
            port=comm_settings.get('port', 8080),
            max_workers=comm_settings.get('max_workers', 5),
            heartbeat_interval=comm_settings.get('heartbeat_interval', 30.0),
            message_timeout=comm_settings.get('timeout', 60.0)
        )
    
    def _initialize_training_engine(self):
        """åˆå§‹åŒ–è®­ç»ƒå¼•æ“"""
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from ...engine.training_engine import TrainingEngine
            
            # æ„å»ºè®­ç»ƒå¼•æ“é…ç½®
            enhanced_config = self._build_enhanced_training_config(self.client_config)
            
            # åˆ›å»ºè®­ç»ƒå¼•æ“
            self.enhanced_training_engine = TrainingEngine(
                context=self.context,
                config=enhanced_config,
                control_state_manager=self.hierarchical_state_manager.control_state_manager
            )
            
            # å°†å®¢æˆ·ç«¯çš„loggerèµ‹ç»™è®­ç»ƒå¼•æ“
            self.enhanced_training_engine.logger = self.logger
            
            self.logger.debug("è®­ç»ƒå¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    # ===== FederatedCommunicator æŠ½è±¡æ–¹æ³•å®ç° =====
    
    def on_start(self) -> None:
        """å®¢æˆ·ç«¯å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
        try:
            self.logger.debug(f"å¯åŠ¨å¤šlearnerå®¢æˆ·ç«¯: {self.client_id}")
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šINITIALIZING -> LOADING_CONFIG
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.LOADING_CONFIG, 
                {
                    "action": "loading_configuration",
                    "timestamp": time.time(),
                    "learner_count": len(self.learners_info)
                }
            )
            
            # 1. åˆå§‹åŒ–å¢å¼ºè®­ç»ƒå¼•æ“
            if self.enhanced_training_engine:
                self.enhanced_training_engine.initialize_training()
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šLOADING_CONFIG -> PREPARING_DATA
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.PREPARING_DATA,
                {
                    "action": "preparing_multi_learner_data",
                    "timestamp": time.time()
                }
            )
            
            # 2. åŠ è½½å®¢æˆ·ç«¯æ•°æ®
            self._load_multi_learner_data()
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šPREPARING_DATA -> REGISTERING
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.REGISTERING,
                {
                    "action": "registering_to_server",
                    "timestamp": time.time()
                }
            )
            
            # 3. å‘æœåŠ¡ç«¯æ³¨å†Œ
            registration_success = self._register_to_server()
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šREGISTERING -> REGISTERED æˆ– ERROR
            if registration_success:
                self.hierarchical_state_manager.transition_coordination_state(
                    ClientLifecycleState.REGISTERED,
                    {
                        "action": "registration_å®Œæˆ",
                        "success": True,
                        "timestamp": time.time()
                    }
                )
                
                # æ³¨å†ŒæˆåŠŸåè½¬ä¸ºREADYçŠ¶æ€
                self.hierarchical_state_manager.transition_coordination_state(
                    ClientLifecycleState.READY,
                    {
                        "action": "client_ready",
                        "timestamp": time.time()
                    }
                )
            else:
                self.hierarchical_state_manager.transition_coordination_state(
                    ClientLifecycleState.ERROR,
                    {
                        "action": "registration_failed",
                        "error": "Server registration failed",
                        "timestamp": time.time()
                    }
                )
                return
            
            # 4. å‘å¸ƒå®¢æˆ·ç«¯å¯åŠ¨äº‹ä»¶
            self.context.publish_event("multi_learner_client_started", {
                "client_id": self.client_id,
                "learners": list(self.learners_info.keys()),
                "timestamp": time.time()
            })
            
            self.logger.debug(f"å¤šlearnerå®¢æˆ·ç«¯å¯åŠ¨æˆåŠŸ: {self.client_id}")
            
        except Exception as e:
            self.logger.error(f"å®¢æˆ·ç«¯å¯åŠ¨å¤±è´¥: {self.client_id}: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢åˆ°é”™è¯¯çŠ¶æ€
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.ERROR,
                {
                    "action": "startup_failed",
                    "error": str(e),
                    "timestamp": time.time()
                }
            )
            
            raise FederationError(f"Multi-learner client startup failed: {e}")
    
    def on_stop(self) -> None:
        """å®¢æˆ·ç«¯åœæ­¢æ—¶çš„æ¸…ç†"""
        try:
            self.logger.debug(f"åœæ­¢å¤šlearnerå®¢æˆ·ç«¯: {self.client_id}")
            
            # 1. åœæ­¢æ­£åœ¨è¿›è¡Œçš„è®­ç»ƒ
            with self.training_lock:
                if self.is_training and self.training_thread and self.training_thread.is_alive():
                    self.logger.debug("ç­‰å¾…è®­ç»ƒçº¿ç¨‹ç»“æŸ...")
                    if self.enhanced_training_engine:
                        self.enhanced_training_engine.stop_training()
                    self.is_training = False
                    
                    # ç­‰å¾…è®­ç»ƒçº¿ç¨‹ç»“æŸ
                    self.training_thread.join(timeout=30)
                    if self.training_thread.is_alive():
                        self.logger.warning("è®­ç»ƒçº¿ç¨‹æœªèƒ½æ­£å¸¸ç»“æŸ")
            
            # 2. æ¸…ç†æ‰€æœ‰learnerèµ„æº
            for learner_info in self.learners_info.values():
                if hasattr(learner_info.learner_instance, 'cleanup'):
                    try:
                        learner_info.learner_instance.cleanup()
                    except Exception as e:
                        self.logger.warning(f"æ¸…ç†learnerå¤±è´¥ {learner_info.learner_id}: {e}")
            
            # 3. æ¸…ç†å¢å¼ºè®­ç»ƒå¼•æ“
            if self.enhanced_training_engine and hasattr(self.enhanced_training_engine, 'cleanup_training_environment'):
                try:
                    self.enhanced_training_engine.cleanup_training_environment()
                except Exception as e:
                    self.logger.warning(f"æ¸…ç†è®­ç»ƒå¼•æ“å¤±è´¥: {e}")
            
            # 4. æ¸…ç†çŠ¶æ€ç®¡ç†å™¨
            try:
                self.hierarchical_state_manager.cleanup()
            except Exception as e:
                self.logger.warning(f"æ¸…ç†çŠ¶æ€ç®¡ç†å™¨å¤±è´¥: {e}")
            
            # 5. å‘å¸ƒå®¢æˆ·ç«¯åœæ­¢äº‹ä»¶
            self.context.publish_event("multi_learner_client_å·²åœæ­¢", {
                "client_id": self.client_id,
                "timestamp": time.time()
            })
            
            self.logger.debug(f"å¤šlearnerå®¢æˆ·ç«¯åœæ­¢å®Œæˆ: {self.client_id}")
            
        except Exception as e:
            self.logger.error(f"å®¢æˆ·ç«¯åœæ­¢å¤±è´¥: {self.client_id}: {e}")
    
    def handle_model_distribution(self, message_data: Dict[str, Any]) -> Any:
        """å¤„ç†å…¨å±€æ¨¡å‹åˆ†å‘ï¼ˆæ”¯æŒå¤šæ¨¡å‹ï¼‰"""
        try:
            round_id = message_data.get('metadata', {}).get('round_id', -1)
            self.logger.info(f"ğŸ“¥ [æ¨¡å‹ä¸‹å‘] Round {round_id} - æ¥æ”¶å…¨å±€æ¨¡å‹ï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒä¸è¯„ä¼°")
            
            # æå–å¤šä¸ªæ¨¡å‹æ•°æ®
            models_data = message_data.get('data', {}).get('models', {})
            if not models_data:
                # å…¼å®¹å•æ¨¡å‹æ ¼å¼
                model_state = message_data.get('data', {}).get('model_state')
                if model_state:
                    models_data = {"primary_model": model_state}
            
            if not models_data:
                self.logger.warning("æ¶ˆæ¯ä¸­æ²¡æœ‰æ¨¡å‹çŠ¶æ€æ•°æ®")
                return {"status": "error", "message": "No model states"}
            
            updated_models = []
            
            # æ›´æ–°å„ä¸ªlearnerçš„æ¨¡å‹
            for model_key, model_state in models_data.items():
                learner_info = self._find_learner_for_model(model_key)
                
                if learner_info:
                    try:
                        if hasattr(learner_info.learner_instance, 'update_model'):
                            learner_info.learner_instance.update_model(model_state)
                        else:
                            # å…¼å®¹æ€§å¤„ç†
                            model = learner_info.learner_instance.get_model()
                            if hasattr(model, 'load_state_dict'):
                                model.load_state_dict(model_state)
                        
                        self.received_global_models[learner_info.learner_id] = learner_info.learner_instance.get_model()
                        updated_models.append(learner_info.learner_id)
                        
                    except Exception as e:
                        self.logger.error(f"æ›´æ–°learneræ¨¡å‹å¤±è´¥ {learner_info.learner_id}: {e}")
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°æ¨¡å‹keyå¯¹åº”çš„learner: {model_key}")
            
            # å‘å¸ƒæ¨¡å‹æ¥æ”¶äº‹ä»¶
            self.context.publish_event("global_models_received", {
                "client_id": self.client_id,
                "round_id": round_id,
                "updated_models": updated_models,
                "timestamp": time.time()
            })
            
            return {"status": "success", "round_id": round_id, "updated_models": updated_models}
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ¨¡å‹åˆ†å‘å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}
    
    def handle_model_update(self, message_data: Dict[str, Any]) -> Any:
        """å¤„ç†æ¨¡å‹æ›´æ–°è¯·æ±‚ï¼ˆå®¢æˆ·ç«¯ä¸€èˆ¬ä¸å¤„ç†æ­¤æ¶ˆæ¯ï¼‰"""
        self.logger.warning("å¤šlearnerå®¢æˆ·ç«¯æ”¶åˆ°model_updateæ¶ˆæ¯ - éé¢„æœŸ")
        return {"status": "ignored"}
    
    def handle_training_trigger(self, message_data: Dict[str, Any]) -> Any:
        """å¤„ç†è®­ç»ƒè§¦å‘ï¼ˆå¤šlearnerè®­ç»ƒï¼‰"""
        try:
            training_params = message_data.get('data', {})
            round_id = message_data.get('metadata', {}).get('round_id', -1)
            
            self.logger.info(f"è§¦å‘å¤šlearnerè®­ç»ƒ round {round_id}")
            
            # æ£€æŸ¥å½“å‰çŠ¶æ€æ˜¯å¦å…è®¸å¼€å§‹è®­ç»ƒ
            current_coordination_state = self.hierarchical_state_manager.get_coordination_state()
            if current_coordination_state not in [ClientLifecycleState.READY, ClientLifecycleState.REGISTERED]:
                self.logger.warning(f"å½“å‰çŠ¶æ€ä¸å…è®¸å¼€å§‹è®­ç»ƒ: {current_coordination_state}")
                return {"status": "error", "message": f"Invalid state for training: {current_coordination_state}"}
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒåœ¨è¿›è¡Œ
            with self.training_lock:
                if self.is_training:
                    self.logger.warning("è®­ç»ƒå·²åœ¨è¿›è¡Œä¸­ï¼Œå¿½ç•¥æ–°çš„è®­ç»ƒè§¦å‘")
                    return {"status": "error", "message": "Training already in progress"}
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> TRAINING
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.TRAINING,
                {
                    "action": "multi_learner_training_triggered",
                    "round_id": round_id,
                    "timestamp": time.time()
                }
            )
            
            # å¼‚æ­¥å¯åŠ¨å¤šlearnerè®­ç»ƒ
            self._start_multi_learner_training_async(training_params, round_id)
            
            return {"status": "multi_learner_training_started", "round_id": round_id}
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¤šlearnerè®­ç»ƒè§¦å‘å¤±è´¥: {e}")
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢åˆ°é”™è¯¯çŠ¶æ€
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.ERROR,
                {
                    "action": "training_trigger_failed",
                    "error": str(e),
                    "timestamp": time.time()
                }
            )
            
            return {"status": "error", "message": str(e)}
    
    def handle_task_notification(self, message_data: Dict[str, Any]) -> Any:
        """å¤„ç†ä»»åŠ¡é€šçŸ¥ï¼ˆå¤šlearneræŒç»­å­¦ä¹ åœºæ™¯ï¼‰"""
        try:
            task_info = message_data.get('data', {})
            self.logger.debug(f"æ¥æ”¶å¤šlearnerä»»åŠ¡é€šçŸ¥: {task_info}")
            
            # å¤„ç†æ–°ä»»åŠ¡ï¼ˆé€šçŸ¥æ‰€æœ‰ç›¸å…³learnerï¼‰
            self._handle_multi_learner_new_task(task_info)
            
            return {"status": "multi_learner_task_received"}
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¤šlearnerä»»åŠ¡é€šçŸ¥å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}
    
    # ===== å¤šlearnerç‰¹æœ‰æ–¹æ³• =====
    
    def _start_multi_learner_training_async(self, training_params: Dict[str, Any], round_id: int) -> None:
        """å¼‚æ­¥å¯åŠ¨å¤šlearnerè®­ç»ƒ"""
        def multi_learner_training_worker():
            try:
                with self.training_lock:
                    self.current_round = round_id
                    self.is_training = True
                
                self.logger.info(f"å¼€å§‹æ‰§è¡Œå¤šlearnerè®­ç»ƒ round {round_id}")
                
                # æ‰§è¡Œå¤šlearnerè®­ç»ƒ
                result = self._execute_multi_learner_training(round_id, training_params)
                
                # å‘é€ç»“æœ
                self._send_multi_learner_training_result(result)
                
                self.logger.debug(f"å¤šlearnerè®­ç»ƒå®Œæˆ round {round_id}")
                
            except Exception as e:
                self.logger.error(f"å¤šlearnerè®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
                self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                
                # åè°ƒå±‚çŠ¶æ€è½¬æ¢åˆ°é”™è¯¯çŠ¶æ€
                self.hierarchical_state_manager.transition_coordination_state(
                    ClientLifecycleState.ERROR,
                    {
                        "action": "training_execution_failed",
                        "error": str(e),
                        "round_id": round_id,
                        "timestamp": time.time()
                    }
                )
            finally:
                with self.training_lock:
                    self.is_training = False
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(
            target=multi_learner_training_worker, 
            name=f"TrainingWorker-{self.client_id}-R{round_id}",
            daemon=True
        )
        self.training_thread.start()
    
    def _execute_multi_learner_training(self, round_id: int, training_params: Dict[str, Any] = None) -> MultiLearnerTrainingResult:
        """æ‰§è¡Œå¤šlearnerè®­ç»ƒ"""
        try:
            start_time = time.time()
            
            if not self.dataloaders:
                raise FederationError("No dataloaders available for training")
            
            if not self.enhanced_training_engine:
                raise FederationError("Training engine not initialized")
            
            self.logger.debug(f"å¼€å§‹å¤šlearnerè®­ç»ƒ round {round_id}")
            
            # ç¡®ä¿è®­ç»ƒå¼•æ“å¤„äºå¯æ‰§è¡ŒçŠ¶æ€
            current_state = self.enhanced_training_engine.training_state
            if current_state == TrainingPhaseState.PREPARING:
                self.logger.debug(f"è®­ç»ƒå¼•æ“çŠ¶æ€ä¸º {current_state}ï¼Œç›´æ¥æ‰§è¡Œè®­ç»ƒ")
            elif current_state == TrainingPhaseState.RUNNING:
                self.logger.debug(f"è®­ç»ƒå¼•æ“çŠ¶æ€ä¸º {current_state}ï¼Œå‡†å¤‡æ‰§è¡Œè®­ç»ƒ")
            else:
                self.logger.debug(f"è®­ç»ƒå¼•æ“çŠ¶æ€ä¸º {current_state}ï¼Œéœ€è¦åˆå§‹åŒ–...")
                
                # å¦‚æœçŠ¶æ€éœ€è¦é‡æ–°åˆå§‹åŒ–ï¼ˆUNINITIALIZEDæˆ–FAILEDï¼‰
                if current_state in [TrainingPhaseState.UNINITIALIZED, TrainingPhaseState.FAILED]:
                    self.enhanced_training_engine.initialize_training()
                # å¦‚æœçŠ¶æ€æ˜¯FINISHEDï¼Œå…ˆè½¬æ¢ä¸ºPREPARINGçŠ¶æ€
                elif current_state == TrainingPhaseState.FINISHED:
                    # ä»FINISHEDçŠ¶æ€å¯ä»¥è½¬æ¢åˆ°PREPARINGï¼Œç„¶åç”±execute_training_planè‡ªåŠ¨å¤„ç†
                    self.enhanced_training_engine.state_manager.transition_to(
                        TrainingPhaseState.PREPARING,
                        {
                            "action": "reset_for_next_round",
                            "timestamp": time.time()
                        }
                    )
                    self.logger.debug("çŠ¶æ€å·²è½¬æ¢ä¸ºPREPARINGï¼Œç”±execute_training_planå¤„ç†åç»­çŠ¶æ€è½¬æ¢")
                else:
                    # å…¶ä»–çŠ¶æ€ï¼Œå°è¯•ç›´æ¥åˆå§‹åŒ–
                    self.enhanced_training_engine.initialize_training()
            
            # å§”æ‰˜ç»™å¢å¼ºè®­ç»ƒå¼•æ“æ‰§è¡Œï¼ˆè®­ç»ƒå¼•æ“ç®¡ç†æ§åˆ¶å±‚çŠ¶æ€ï¼‰
            phase_results = self.enhanced_training_engine.execute_training_plan()
            
            total_training_time = time.time() - start_time
            
            # èšåˆå¤šlearnerç»“æœ
            aggregated_result = self._aggregate_multi_learner_results(
                round_id, phase_results, total_training_time
            )
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append(aggregated_result)
            
            self.logger.debug(f"å¤šlearnerè®­ç»ƒå®Œæˆ round {round_id}, è€—æ—¶ {total_training_time:.2f}s")
            return aggregated_result
            
        except Exception as e:
            self.logger.error(f"å¤šlearnerè®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
            raise FederationError(f"Multi-learner training failed: {e}")
    
    def _aggregate_multi_learner_results(self, 
                                       round_id: int, 
                                       phase_results: Dict[str, PhaseResult], 
                                       total_training_time: float) -> MultiLearnerTrainingResult:
        """èšåˆå¤šlearnerè®­ç»ƒç»“æœ"""
        try:
            # 1. æå–å’Œèšåˆæ¨¡å‹æ›´æ–°
            aggregated_model_update = self._aggregate_model_updates(phase_results)
            
            # 2. èšåˆè®­ç»ƒæŒ‡æ ‡
            aggregated_metrics = self._aggregate_training_metrics(phase_results)
            
            # 3. èšåˆè¯„ä¼°ç»“æœ
            aggregated_evaluation_results = self._aggregate_evaluation_results(phase_results)
            
            # 4. è®¡ç®—learnerè´¡çŒ®æƒé‡
            learner_contributions = self._calculate_learner_contributions(phase_results)
            
            # 5. è®¡ç®—æ€»æ ·æœ¬æ•°
            total_samples = self._calculate_total_samples()
            
            # 6. æ„å»ºèšåˆç»“æœ
            aggregated_result = MultiLearnerTrainingResult(
                client_id=self.client_id,
                round_id=round_id,
                phase_results=phase_results,
                aggregated_model_update=aggregated_model_update,
                total_samples=total_samples,
                training_metrics=aggregated_metrics,
                evaluation_results=aggregated_evaluation_results,
                total_training_time=total_training_time,
                learner_contributions=learner_contributions,
                metadata={
                    "phase_count": len(phase_results),
                    "learner_count": len(self.learners_info),
                    "aggregation_method": "weighted_average",
                    "timestamp": time.time()
                }
            )
            
            return aggregated_result
            
        except Exception as e:
            self.logger.error(f"èšåˆå¤šlearnerç»“æœå¤±è´¥: {e}")
            raise
    
    def _aggregate_model_updates(self, phase_results: Dict[str, PhaseResult]) -> Dict[str, torch.Tensor]:
        """èšåˆæ¨¡å‹æ›´æ–°"""
        try:
            aggregated_update = {}
            total_weight = 0.0
            
            for phase_name, phase_result in phase_results.items():
                if not phase_result.success or not phase_result.final_state:
                    continue
                
                # è·å–é˜¶æ®µçš„æ¨¡å‹æ›´æ–°
                phase_model_update = phase_result.final_state.get('model_update', {})
                if not phase_model_update:
                    # å°è¯•ä»learnerè·å–æ¨¡å‹å‚æ•°
                    learner_info = self._get_learner_info_by_phase(phase_name)
                    if learner_info:
                        try:
                            model = learner_info.learner_instance.get_model()
                            if hasattr(model, 'state_dict'):
                                phase_model_update = {k: v.clone() for k, v in model.state_dict().items()}
                        except Exception as e:
                            self.logger.warning(f"è·å–é˜¶æ®µ{phase_name}æ¨¡å‹å‚æ•°å¤±è´¥: {e}")
                            continue
                    else:
                        continue
                
                # è®¡ç®—é˜¶æ®µæƒé‡ï¼ˆåŸºäºè®­ç»ƒæ—¶é—´å’ŒæˆåŠŸçš„epochæ•°ï¼‰
                phase_weight = len(phase_result.executed_epochs) * max(phase_result.execution_time, 1.0)
                total_weight += phase_weight
                
                # åŠ æƒèšåˆæ¨¡å‹å‚æ•°
                for param_name, param_tensor in phase_model_update.items():
                    if not isinstance(param_tensor, torch.Tensor):
                        continue
                        
                    if param_name not in aggregated_update:
                        aggregated_update[param_name] = param_tensor.clone() * phase_weight
                    else:
                        aggregated_update[param_name] += param_tensor * phase_weight
            
            # å½’ä¸€åŒ–
            if total_weight > 0:
                for param_name in aggregated_update:
                    aggregated_update[param_name] /= total_weight
            
            self.logger.info(f"èšåˆæ¨¡å‹æ›´æ–°å®Œæˆ: {len(phase_results)}ä¸ªé˜¶æ®µ, {len(aggregated_update)}ä¸ªå‚æ•°")
            return aggregated_update
            
        except Exception as e:
            self.logger.error(f"èšåˆæ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
            return {}
    
    def _aggregate_training_metrics(self, phase_results: Dict[str, PhaseResult]) -> Dict[str, Any]:
        """èšåˆè®­ç»ƒæŒ‡æ ‡"""
        try:
            aggregated_metrics = {
                "total_phases": len(phase_results),
                "successful_phases": 0,
                "total_epochs": 0,
                "average_loss": 0.0,
                "average_accuracy": 0.0,
                "phase_metrics": {}
            }
            
            total_loss = 0.0
            total_accuracy = 0.0
            loss_count = 0
            accuracy_count = 0
            
            for phase_name, phase_result in phase_results.items():
                if phase_result.success:
                    aggregated_metrics["successful_phases"] += 1
                
                aggregated_metrics["total_epochs"] += len(phase_result.executed_epochs)
                
                # èšåˆæ¯ä¸ªé˜¶æ®µçš„æœ€ç»ˆæŒ‡æ ‡
                final_metrics = phase_result.get_final_metrics()
                aggregated_metrics["phase_metrics"][phase_name] = final_metrics
                
                # ç´¯ç§¯æŸå¤±å’Œå‡†ç¡®ç‡
                if "loss" in final_metrics and isinstance(final_metrics["loss"], (int, float)):
                    total_loss += final_metrics["loss"]
                    loss_count += 1
                
                if "accuracy" in final_metrics and isinstance(final_metrics["accuracy"], (int, float)):
                    total_accuracy += final_metrics["accuracy"]
                    accuracy_count += 1
            
            # è®¡ç®—å¹³å‡å€¼
            if loss_count > 0:
                aggregated_metrics["average_loss"] = total_loss / loss_count
            
            if accuracy_count > 0:
                aggregated_metrics["average_accuracy"] = total_accuracy / accuracy_count
            
            return aggregated_metrics
            
        except Exception as e:
            self.logger.error(f"èšåˆè®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _aggregate_evaluation_results(self, phase_results: Dict[str, PhaseResult]) -> Dict[str, Any]:
        """èšåˆè¯„ä¼°ç»“æœ"""
        try:
            aggregated_evaluation = {
                "total_evaluation_tasks": 0,
                "successful_evaluations": 0,
                "phase_evaluations": {}
            }
            
            for phase_name, phase_result in phase_results.items():
                if not phase_result.success:
                    continue
                
                # ä»é˜¶æ®µç»“æœä¸­æå–è¯„ä¼°æ•°æ®
                evaluation_data = phase_result.metrics.get("evaluation", {})
                if evaluation_data:
                    aggregated_evaluation["phase_evaluations"][phase_name] = evaluation_data
                    aggregated_evaluation["total_evaluation_tasks"] += len(evaluation_data)
                    aggregated_evaluation["successful_evaluations"] += len(evaluation_data)
                    
                    self.logger.debug(f"ğŸ“Š æå–é˜¶æ®µ {phase_name} è¯„ä¼°ç»“æœ: {list(evaluation_data.keys())}")
            
            if aggregated_evaluation["total_evaluation_tasks"] > 0:
                self.logger.info(f"ğŸ“Š [è¯„ä¼°èšåˆ] èšåˆè¯„ä¼°ç»“æœå®Œæˆ: {aggregated_evaluation['total_evaluation_tasks']} ä¸ªè¯„ä¼°ä»»åŠ¡")
            else:
                self.logger.debug("ğŸ“Š [è¯„ä¼°èšåˆ] æœªå‘ç°è¯„ä¼°ç»“æœ")
            
            return aggregated_evaluation
            
        except Exception as e:
            self.logger.error(f"âŒ [è¯„ä¼°èšåˆ] èšåˆè¯„ä¼°ç»“æœå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _calculate_learner_contributions(self, phase_results: Dict[str, PhaseResult]) -> Dict[str, float]:
        """è®¡ç®—learnerè´¡çŒ®æƒé‡"""
        try:
            contributions = {}
            total_contribution = 0.0
            
            for phase_name, phase_result in phase_results.items():
                if not phase_result.success:
                    contributions[phase_name] = 0.0
                    continue
                
                # æ ¹æ®æ‰§è¡Œçš„epochæ•°å’Œæ‰§è¡Œæ—¶é—´è®¡ç®—è´¡çŒ®
                epoch_contribution = len(phase_result.executed_epochs)
                time_contribution = max(phase_result.execution_time, 1.0)
                
                # ç®€å•çš„è´¡çŒ®è®¡ç®—ï¼šepochæ•° Ã— æ‰§è¡Œæ—¶é—´
                phase_contribution = epoch_contribution * time_contribution
                contributions[phase_name] = phase_contribution
                total_contribution += phase_contribution
            
            # å½’ä¸€åŒ–ä¸ºç™¾åˆ†æ¯”
            if total_contribution > 0:
                for phase_name in contributions:
                    contributions[phase_name] = contributions[phase_name] / total_contribution
            
            return contributions
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—learnerè´¡çŒ®å¤±è´¥: {e}")
            return {}
    
    def _calculate_total_samples(self) -> int:
        """è®¡ç®—æ€»æ ·æœ¬æ•°"""
        try:
            # ä¼˜å…ˆä»ä¸Šä¸‹æ–‡è·å–
            data_info = self.context.get_state(f"client_{self.client_id}_multi_data_info", scope="client")
            if data_info and 'total_samples' in data_info:
                return data_info['total_samples']
            
            # ä»DataLoaderæ¨æ–­
            total_samples = 0
            for dataloader in self.dataloaders.values():
                if hasattr(dataloader, 'dataset') and hasattr(dataloader.dataset, '__len__'):
                    total_samples += len(dataloader.dataset)
            
            return total_samples
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æ€»æ ·æœ¬æ•°å¤±è´¥: {e}")
            return 0
    
    def _create_evaluation_summary(self, evaluation_results: Dict[str, Any]) -> str:
        """åˆ›å»ºè¯„ä¼°ç»“æœæ‘˜è¦å­—ç¬¦ä¸²"""
        try:
            if not evaluation_results or not evaluation_results.get("phase_evaluations"):
                return ""
            
            summary_parts = []
            total_tasks = evaluation_results.get("total_evaluation_tasks", 0)
            
            for phase_name, phase_eval in evaluation_results.get("phase_evaluations", {}).items():
                if isinstance(phase_eval, dict):
                    accuracy_results = []
                    loss_results = []
                    
                    for task_name, task_result in phase_eval.items():
                        if isinstance(task_result, dict):
                            accuracy = task_result.get("accuracy")
                            loss = task_result.get("loss")
                            
                            if accuracy is not None:
                                accuracy_results.append(f"{accuracy:.3f}")
                            if loss is not None:
                                loss_results.append(f"{loss:.3f}")
                    
                    if accuracy_results or loss_results:
                        phase_summary = f"{phase_name}("
                        if accuracy_results:
                            phase_summary += f"acc:{','.join(accuracy_results)}"
                        if loss_results:
                            if accuracy_results:
                                phase_summary += f", "
                            phase_summary += f"loss:{','.join(loss_results)}"
                        phase_summary += ")"
                        summary_parts.append(phase_summary)
            
            if summary_parts:
                return f"{total_tasks}ä¸ªè¯„ä¼°ä»»åŠ¡ - {'; '.join(summary_parts)}"
            else:
                return f"{total_tasks}ä¸ªè¯„ä¼°ä»»åŠ¡"
                
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºè¯„ä¼°æ‘˜è¦å¤±è´¥: {e}")
            return "è¯„ä¼°ç»“æœæ‘˜è¦åˆ›å»ºå¤±è´¥"
    
    def _send_multi_learner_training_result(self, result: MultiLearnerTrainingResult) -> None:
        """å‘é€å¤šlearnerè®­ç»ƒç»“æœåˆ°æœåŠ¡ç«¯"""
        try:
            result_data = {
                "client_id": result.client_id,
                "round_id": result.round_id,
                "client_type": "multi_learner",
                "aggregated_model_update": result.aggregated_model_update,
                "total_samples": result.total_samples,
                "training_metrics": result.training_metrics,
                "evaluation_results": result.evaluation_results,  # æ·»åŠ è¯„ä¼°ç»“æœ
                "total_training_time": result.total_training_time,
                "learner_contributions": result.learner_contributions,
                "metadata": result.metadata
            }
            
            # è®°å½•å³å°†ä¸Šä¼ çš„è¯„ä¼°ç»“æœæ‘˜è¦
            evaluation_summary = self._create_evaluation_summary(result.evaluation_results)
            if evaluation_summary:
                self.logger.info(f"ğŸ“¤ [æ¨¡å‹ä¸Šä¼ ] Round {result.round_id} - ä¸Šä¼ æ¨¡å‹ä¸è¯„ä¼°ç»“æœ: {evaluation_summary}")
            else:
                self.logger.info(f"ğŸ“¤ [æ¨¡å‹ä¸Šä¼ ] Round {result.round_id} - ä¸Šä¼ æ¨¡å‹ (æ— è¯„ä¼°ç»“æœ)")
            
            # å‘é€èšåˆçš„æ¨¡å‹æ›´æ–°æ¶ˆæ¯
            self.send_message(
                target="server",
                message_type=MessageType.MODEL_UPDATE,
                data=result_data,
                metadata={
                    "round_id": result.round_id,
                    "client_id": result.client_id,
                    "client_type": "multi_learner"
                }
            )
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢ï¼šTRAINING -> READYï¼ˆå‡†å¤‡ä¸‹ä¸€è½®ï¼‰
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.READY,
                {
                    "action": "multi_learner_training_result_sent",
                    "round_id": result.round_id,
                    "timestamp": time.time()
                }
            )
            
            self.logger.info(f"å¤šlearnerè®­ç»ƒç»“æœå·²å‘é€ round {result.round_id}")
            
        except Exception as e:
            self.logger.error(f"å‘é€å¤šlearnerè®­ç»ƒç»“æœå¤±è´¥: {e}")
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢åˆ°é”™è¯¯çŠ¶æ€
            self.hierarchical_state_manager.transition_coordination_state(
                ClientLifecycleState.ERROR,
                {
                    "action": "send_result_failed",
                    "error": str(e),
                    "timestamp": time.time()
                }
            )
    
    # ===== ç»„ä»¶åˆå§‹åŒ–æ–¹æ³• =====
    
    def _initialize_multi_learner_components(self) -> None:
        """åˆå§‹åŒ–å¤šlearnerç»„ä»¶"""
        try:
            self.logger.debug("åˆå§‹åŒ–å¤šlearnerç»„ä»¶...")
            
            # 1. åˆ›å»ºæ‰€æœ‰learner
            learners_config = self.client_config.get('learners', {})
            if not learners_config:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°learnersé…ç½®ï¼Œå°†åˆ›å»ºé»˜è®¤learner")
                learners_config = self._create_default_learners_config()
            
            for learner_id, learner_config in learners_config.items():
                try:
                    self.logger.debug(f"å¼€å§‹åˆ›å»ºlearner: {learner_id}, é…ç½®: {learner_config}")
                    learner_info = self._create_learner_info(learner_id, learner_config)
                    self.learners_info[learner_id] = learner_info
                    self.logger.debug(f"åˆ›å»ºlearneræˆåŠŸ: {learner_id} ({learner_info.learner_type})")
                except Exception as e:
                    self.logger.error(f"åˆ›å»ºlearnerå¤±è´¥ {learner_id}: {e}")
                    self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            
            # 2. åˆ›å»ºæ‰€æœ‰dataloader
            dataloaders_config = self.client_config.get('dataloaders', {})
            if not dataloaders_config:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°dataloadersé…ç½®ï¼Œå°†åˆ›å»ºé»˜è®¤dataloader")
                dataloaders_config = self._create_default_dataloaders_config()
            
            for dataloader_id, dataloader_config in dataloaders_config.items():
                try:
                    dataloader = self._create_dataloader(dataloader_id, dataloader_config)
                    self.dataloaders[dataloader_id] = dataloader
                    self.logger.info(f"åˆ›å»ºdataloader: {dataloader_id}")
                except Exception as e:
                    self.logger.error(f"åˆ›å»ºdataloaderå¤±è´¥ {dataloader_id}: {e}")
            
            if not self.learners_info:
                raise FederationError("No learners created successfully")
            
            if not self.dataloaders:
                self.logger.warning("No dataloaders created, using mock data")
                self.dataloaders["default"] = self._create_mock_data()
            
            self.logger.debug(f"å¤šlearnerç»„ä»¶åˆå§‹åŒ–å®Œæˆ: {len(self.learners_info)} learners, {len(self.dataloaders)} dataloaders")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¤šlearnerç»„ä»¶å¤±è´¥: {e}")
            raise
    
    def _create_default_learners_config(self) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤learneré…ç½®"""
        return {
            "default_learner": {
                "class": "l2p",
                "model": {
                    "type": "SimpleMLP",
                    "input_dim": 784,
                    "hidden_dims": [128, 64],
                    "output_dim": 10
                },
                "optimizer": {
                    "type": "Adam",
                    "lr": 0.001
                },
                "dataloader": "default",
                "scheduler": "default_scheduler",
                "priority": 0,
                "enabled": True
            }
        }
    
    def _create_default_dataloaders_config(self) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤dataloaderé…ç½®"""
        return {
            "default": {
                "batch_size": 32,
                "shuffle": True,
                "num_workers": 0,
                "drop_last": False
            }
        }
    
    def _create_learner_info(self, learner_id: str, learner_config: Dict[str, Any]) -> LearnerInfo:
        """åˆ›å»ºlearnerä¿¡æ¯"""
        try:
            # åˆ›å»ºlearnerå®ä¾‹
            learner_instance = self._create_single_learner(learner_config, self.context)
            
            # æ„å»ºlearnerä¿¡æ¯
            learner_info = LearnerInfo(
                learner_id=learner_id,
                learner_type=learner_config.get('type', 'UnknownLearner'),
                learner_instance=learner_instance,
                dataloader_id=learner_config.get('dataloader', f"{learner_id}_dataloader"),
                scheduler_id=learner_config.get('scheduler', f"{learner_id}_scheduler"),
                priority=learner_config.get('priority', 0),
                is_active=learner_config.get('enabled', True)
            )
            
            return learner_info
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºlearnerä¿¡æ¯å¤±è´¥ {learner_id}: {e}")
            raise
    
    def _create_single_learner(self, learner_config: Dict[str, Any], context: ExecutionContext) -> BaseLearner:
        """åˆ›å»ºå•ä¸ªlearner"""
        try:
            # å°è¯•ä½¿ç”¨ç»„ä»¶æ³¨å†Œè¡¨åˆ›å»ºlearner
            try:
                from ...registry.component_composer import ComponentComposer
                from ...registry import registry
                
                composer = ComponentComposer(registry)
                config = OmegaConf.create({'learner': learner_config})
                learner = composer.create_learner(config, context)
                return learner
            except Exception as e:
                self.logger.warning(f"ä½¿ç”¨ç»„ä»¶æ³¨å†Œè¡¨åˆ›å»ºlearnerå¤±è´¥: {e}, å°è¯•åˆ›å»ºmock learner")
                return self._create_mock_learner(learner_config)
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºlearnerå¤±è´¥: {e}")
            raise
    
    def _create_mock_learner(self, learner_config: Dict[str, Any]) -> BaseLearner:
        """åˆ›å»ºæ¨¡æ‹Ÿlearner"""
        class MockLearner:
            def __init__(self, config):
                self.config = config
                self.metrics = {"loss": 1.0, "accuracy": 0.0}
                self.model = self._create_mock_model()
            
            def _create_mock_model(self):
                """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹"""
                import torch.nn as nn
                return nn.Sequential(
                    nn.Linear(784, 128),
                    nn.ReLU(),
                    nn.Linear(128, 10)
                )
            
            def get_model(self):
                return self.model
            
            def train_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
                # æ¨¡æ‹Ÿè®­ç»ƒ
                import time
                time.sleep(0.1)
                self.metrics["loss"] = max(0.1, self.metrics["loss"] * 0.95)
                self.metrics["accuracy"] = min(0.95, self.metrics["accuracy"] + 0.02)
                return self.metrics.copy()
            
            def get_state(self) -> Dict[str, Any]:
                return {"metrics": self.metrics}
            
            def set_state(self, state: Dict[str, Any]) -> None:
                if "metrics" in state:
                    self.metrics.update(state["metrics"])
            
            def update_model(self, model_state):
                if hasattr(self.model, 'load_state_dict'):
                    self.model.load_state_dict(model_state)
        
        return MockLearner(learner_config)
    
    def _create_dataloader(self, dataloader_id: str, dataloader_config: Dict[str, Any]) -> DataLoader:
        """åˆ›å»ºdataloader"""
        try:
            # å°è¯•ä½¿ç”¨DataLoaderFactory
            try:
                from ...config.config_manager import DataLoaderFactory
                
                factory = DataLoaderFactory({dataloader_id: dataloader_config})
                dataloader = factory.create_dataloader(dataloader_id, dataloader_config)
                return dataloader
            except Exception as e:
                self.logger.warning(f"ä½¿ç”¨DataLoaderFactoryåˆ›å»ºdataloaderå¤±è´¥ {dataloader_id}: {e}, ä½¿ç”¨mockæ•°æ®")
                return self._create_mock_data()
            
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºdataloaderå¤±è´¥ {dataloader_id}: {e}, ä½¿ç”¨mockæ•°æ®")
            return self._create_mock_data()
    
    def _create_mock_data(self) -> DataLoader:
        """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
        try:
            # ç®€å•çš„æ¨¡æ‹Ÿæ•°æ®
            num_samples = 100
            input_dim = 784
            num_classes = 10
            
            X = torch.randn(num_samples, input_dim)
            y = torch.randint(0, num_classes, (num_samples,))
            
            dataset = TensorDataset(X, y)
            dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
            
            self.logger.debug(f"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†: {num_samples} samples")
            return dataloader
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®å¤±è´¥: {e}")
            raise
    
    def _build_enhanced_training_config(self, client_config: DictConfig) -> Dict[str, Any]:
        """æ„å»ºå¢å¼ºè®­ç»ƒå¼•æ“é…ç½®"""
        enhanced_config = {
            "project": {
                "name": f"client_{self.client_id}_multi_learner",
                "output_dir": f"./outputs/client_{self.client_id}"
            },
            
            # ä»å®¢æˆ·ç«¯é…ç½®ä¸­æå–ç›¸å…³é…ç½®
            "dataloaders": client_config.get('dataloaders', {}),
            "learners": client_config.get('learners', {}),
            "schedulers": client_config.get('schedulers', {}),
            "training_plan": client_config.get('training_plan', {}),
            "state_transfer": client_config.get('state_transfer', {}),
            "hooks": client_config.get('hooks', {}),
            "system": client_config.get('system', {}),
            
            # æ·»åŠ è¯„ä¼°ç›¸å…³é…ç½®
            "test_datas": client_config.get('test_datas', {}),
            "evaluators": client_config.get('evaluators', {}),
            "evaluation": client_config.get('evaluation', {})
        }
        
        # å¦‚æœæ²¡æœ‰è®­ç»ƒè®¡åˆ’ï¼Œåˆ›å»ºé»˜è®¤çš„
        if not enhanced_config.get('training_plan'):
            enhanced_config['training_plan'] = self._create_default_training_plan()
        
        return enhanced_config
    
    def _create_default_training_plan(self) -> Dict[str, Any]:
        """åˆ›å»ºé»˜è®¤è®­ç»ƒè®¡åˆ’"""
        self.logger.debug(f"å¼€å§‹åˆ›å»ºé»˜è®¤è®­ç»ƒè®¡åˆ’ï¼Œå½“å‰learners_info: {list(self.learners_info.keys())}")
        
        learner_ids = list(self.learners_info.keys())
        
        if not learner_ids:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„learnersç”¨äºåˆ›å»ºè®­ç»ƒè®¡åˆ’ï¼")
            raise FederationError("No learners available for training plan")
        
        # åˆ›å»ºç®€å•çš„é¡ºåºè®­ç»ƒè®¡åˆ’
        phases = []
        epoch_count = 1
        epochs_per_phase = 5
        
        for i, learner_id in enumerate(learner_ids):
            learner_info = self.learners_info[learner_id]
            self.logger.debug(f"ä¸ºlearner {learner_id} åˆ›å»ºè®­ç»ƒé˜¶æ®µï¼Œscheduler={learner_info.scheduler_id}")
            
            phase_epochs = list(range(epoch_count, epoch_count + epochs_per_phase))
            
            phase = {
                "name": f"phase_{learner_id}",
                "description": f"Training phase for {learner_id}",
                "epochs": phase_epochs,
                "learner": learner_id,
                "scheduler": learner_info.scheduler_id,
                "priority": learner_info.priority,
                "execution_mode": "sequential"
            }
            
            # æ·»åŠ ç»§æ‰¿å…³ç³»ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªé˜¶æ®µï¼‰
            if i > 0:
                phase["inherit_from"] = [phases[i-1]["name"]]
            
            phases.append(phase)
            epoch_count += epochs_per_phase
        
        training_plan = {
            "total_epochs": epoch_count - 1,
            "execution_strategy": "sequential",
            "phases": phases
        }
        
        self.logger.debug(f"åˆ›å»ºçš„è®­ç»ƒè®¡åˆ’: {training_plan}")
        return training_plan
    
    # ===== è¾…åŠ©æ–¹æ³• =====
    
    def _find_learner_for_model(self, model_key: str) -> Optional[LearnerInfo]:
        """æ ¹æ®æ¨¡å‹keyæ‰¾åˆ°å¯¹åº”çš„learner"""
        # å°è¯•ç›´æ¥åŒ¹é…learner_id
        if model_key in self.learners_info:
            return self.learners_info[model_key]
        
        # å°è¯•æ ¹æ®æ¨¡å‹keyçš„æ˜ å°„è§„åˆ™æŸ¥æ‰¾
        model_mappings = self.client_config.get('model_mappings', {})
        if model_key in model_mappings:
            learner_id = model_mappings[model_key]
            return self.learners_info.get(learner_id)
        
        # é»˜è®¤ç­–ç•¥ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªlearnerï¼Œä½¿ç”¨å®ƒ
        if len(self.learners_info) == 1:
            return list(self.learners_info.values())[0]
        
        # é»˜è®¤ç­–ç•¥ï¼šæŸ¥æ‰¾ä¸»learner
        for learner_info in self.learners_info.values():
            if "primary" in learner_info.learner_id.lower() or "default" in learner_info.learner_id.lower():
                return learner_info
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        if self.learners_info:
            return list(self.learners_info.values())[0]
        
        return None
    
    def _get_learner_info_by_phase(self, phase_name: str) -> Optional[LearnerInfo]:
        """æ ¹æ®é˜¶æ®µåç§°è·å–learnerä¿¡æ¯"""
        # å°è¯•ä»é˜¶æ®µåç§°æå–learner_id
        for learner_id in self.learners_info:
            if learner_id in phase_name:
                return self.learners_info[learner_id]
        
        # å¦‚æœæå–ä¸åˆ°ï¼Œè¿”å›é»˜è®¤learner
        return self._find_learner_for_model("default")
    
    def _load_multi_learner_data(self) -> None:
        """åŠ è½½å¤šlearneræ•°æ®"""
        try:
            dataset_config = self.client_config.get('dataset', {})
            
            if not dataset_config:
                self.logger.warning("æ²¡æœ‰æ•°æ®é›†é…ç½®ï¼Œæ•°æ®å°†åœ¨dataloaderä¸­åŠ è½½")
                return
            
            # ä¸ºæ¯ä¸ªdataloaderåŠ è½½æ•°æ®ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åŠ è½½çš„è¯ï¼‰
            total_samples = 0
            for dataloader_id, dataloader in self.dataloaders.items():
                if hasattr(dataloader, 'dataset') and hasattr(dataloader.dataset, '__len__'):
                    num_samples = len(dataloader.dataset)
                    total_samples += num_samples
                    self.logger.debug(f"Dataloader {dataloader_id} å·²æœ‰ {num_samples} æ ·æœ¬")
                else:
                    self.logger.debug(f"Dataloader {dataloader_id} éœ€è¦æ•°æ®åŠ è½½")
            
            # å­˜å‚¨æ•°æ®é›†ä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡
            self.context.set_state(f"client_{self.client_id}_multi_data_info", {
                "total_samples": total_samples,
                "dataloader_count": len(self.dataloaders),
                "learner_count": len(self.learners_info),
                "dataset_name": dataset_config.get('name', 'unknown')
            }, scope="client")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½å¤šlearneræ•°æ®å¤±è´¥: {e}")
    
    def _register_to_server(self) -> bool:
        """å‘æœåŠ¡ç«¯æ³¨å†Œï¼ˆåŒ…å«å¤šlearnerä¿¡æ¯ï¼‰"""
        try:
            registration_data = {
                "client_id": self.client_id,
                "client_type": "multi_learner",
                "learners": self._get_learners_capabilities(),
                "data_info": self.context.get_state(f"client_{self.client_id}_multi_data_info", scope="client")
            }
            
            # å‘é€æ³¨å†Œæ¶ˆæ¯
            response = self.send_message(
                target="server",
                message_type=MessageType.REGISTRATION,
                data=registration_data,
                expect_response=True,
                timeout=30.0
            )
            
            if response and response.get('status') == 'registered':
                self.logger.debug(f"å¤šlearnerå®¢æˆ·ç«¯æ³¨å†ŒæˆåŠŸ: {self.client_id}")
                return True
            else:
                self.logger.warning(f"æ³¨å†Œå“åº”: {response}")
                return False
                
        except Exception as e:
            self.logger.warning(f"æ³¨å†Œå¤±è´¥: {e}")
            return False
    
    def _get_learners_capabilities(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰learnerçš„èƒ½åŠ›"""
        capabilities = {}
        
        for learner_id, learner_info in self.learners_info.items():
            learner_instance = learner_info.learner_instance
            
            try:
                model = learner_instance.get_model()
                model_name = type(model).__name__ if model else "Unknown"
            except:
                model_name = "Unknown"
            
            capabilities[learner_id] = {
                "learner_type": learner_info.learner_type,
                "model_architecture": model_name,
                "device": str(getattr(learner_instance, 'device', 'cpu')),
                "supported_tasks": getattr(learner_instance, 'supported_tasks', []),
                "priority": learner_info.priority,
                "is_active": learner_info.is_active,
                "dataloader_id": learner_info.dataloader_id,
                "scheduler_id": learner_info.scheduler_id
            }
        
        return capabilities
    
    def _handle_multi_learner_new_task(self, task_info: Dict[str, Any]) -> None:
        """å¤„ç†å¤šlearneræ–°ä»»åŠ¡"""
        try:
            task_id = task_info.get('task_id')
            task_type = task_info.get('task_type', 'classification')
            
            self.logger.debug(f"å¤„ç†å¤šlearneræ–°ä»»åŠ¡: {task_id} (type: {task_type})")
            
            # é€šçŸ¥æ‰€æœ‰ç›¸å…³learnerå‡†å¤‡æ–°ä»»åŠ¡
            affected_learners = []
            for learner_id, learner_info in self.learners_info.items():
                if hasattr(learner_info.learner_instance, 'prepare_for_task'):
                    try:
                        learner_info.learner_instance.prepare_for_task(task_info)
                        affected_learners.append(learner_id)
                    except Exception as e:
                        self.logger.warning(f"learner {learner_id} å‡†å¤‡æ–°ä»»åŠ¡å¤±è´¥: {e}")
            
            # å‘å¸ƒä»»åŠ¡æ¥æ”¶äº‹ä»¶
            self.context.publish_event("multi_learner_new_task_received", {
                "client_id": self.client_id,
                "task_id": task_id,
                "task_type": task_type,
                "affected_learners": affected_learners,
                "timestamp": time.time()
            })
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¤šlearneræ–°ä»»åŠ¡å¤±è´¥: {e}")
    
    def _register_state_callbacks(self):
        """æ³¨å†ŒçŠ¶æ€å˜åŒ–å›è°ƒ"""
        try:
            # æ³¨å†Œåè°ƒå±‚çŠ¶æ€å›è°ƒ
            self.hierarchical_state_manager.register_coordination_callback(
                self._on_coordination_state_change,
                callback_id="client_coordination_callback"
            )
            
            # æ³¨å†Œæ§åˆ¶å±‚çŠ¶æ€å›è°ƒ
            self.hierarchical_state_manager.register_control_callback(
                self._on_control_state_change,
                callback_id="client_control_callback"
            )
            
            self.logger.debug("çŠ¶æ€å›è°ƒæ³¨å†Œå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ³¨å†ŒçŠ¶æ€å›è°ƒå¤±è´¥: {e}")
    
    def _on_coordination_state_change(self, old_state: ClientLifecycleState, 
                                    new_state: ClientLifecycleState, 
                                    metadata: Dict[str, Any]):
        """åè°ƒå±‚çŠ¶æ€å˜åŒ–å›è°ƒ"""
        self.logger.debug(
            f"å®¢æˆ·ç«¯åè°ƒå±‚çŠ¶æ€å˜åŒ–: {self.client_id} {old_state.name} -> {new_state.name}"
        )
        
        # å‘å¸ƒäº‹ä»¶åˆ°æ‰§è¡Œä¸Šä¸‹æ–‡
        self.context.publish_event("client_coordination_state_changed", {
            "client_id": self.client_id,
            "old_state": old_state.name,
            "new_state": new_state.name,
            "metadata": metadata,
            "timestamp": metadata.get("timestamp", time.time())
        })
    
    def _on_control_state_change(self, old_state: TrainingPhaseState,
                               new_state: TrainingPhaseState,
                               metadata: Dict[str, Any]):
        """æ§åˆ¶å±‚çŠ¶æ€å˜åŒ–å›è°ƒ"""
        self.logger.debug(
            f"å®¢æˆ·ç«¯æ§åˆ¶å±‚çŠ¶æ€å˜åŒ–: {self.client_id} {old_state.name} -> {new_state.name}"
        )
        
        # æ ¹æ®æ§åˆ¶å±‚çŠ¶æ€æ‰§è¡Œç›¸åº”æ“ä½œ
        if new_state == TrainingPhaseState.FINISHED:
            self.logger.debug("è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡å‘é€ç»“æœ")
        elif new_state == TrainingPhaseState.FAILED:
            self.logger.error("è®­ç»ƒå¤±è´¥")
    
    def _register_hooks(self, hooks_config: Dict[str, Any]) -> None:
        """æ³¨å†ŒHookåˆ°å¢å¼ºè®­ç»ƒå¼•æ“"""
        try:
            if not hooks_config:
                self.logger.debug("æ²¡æœ‰é…ç½®hooks")
                return
            
            # Hookæ³¨å†Œé€»è¾‘å¯ä»¥å§”æ‰˜ç»™enhanced_training_engineå¤„ç†
            if self.enhanced_training_engine and hasattr(self.enhanced_training_engine, 'register_training_hooks'):
                # è¿™é‡Œå¯ä»¥æ ¹æ®hooks_configåˆ›å»ºå…·ä½“çš„hookå®ä¾‹
                self.logger.debug("Hookæ³¨å†Œå§”æ‰˜ç»™å¢å¼ºè®­ç»ƒå¼•æ“")
            else:
                self.logger.debug("è®­ç»ƒå¼•æ“ä¸æ”¯æŒhookæ³¨å†Œ")
            
        except Exception as e:
            self.logger.error(f"æ³¨å†Œhookså¤±è´¥: {e}")
    
    def _create_execution_context(self, config: DictConfig) -> ExecutionContext:
        """åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡"""
        try:
            context_config = config.get('context', {})
            experiment_id = f"multi_learner_client_experiment_{self.client_id}"
            
            context = ExecutionContext(
                config=OmegaConf.create(context_config),
                experiment_id=experiment_id
            )
            
            # å­˜å‚¨å®Œæ•´é…ç½®
            context._client_config = config
            
            return context
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            raise
    
    # ===== å…¬å…±æ¥å£æ–¹æ³• =====
    
    def get_client_status(self) -> Dict[str, Any]:
        """è·å–å¤šlearnerå®¢æˆ·ç«¯çŠ¶æ€"""
        try:
            overall_status = self.hierarchical_state_manager.get_overall_status()
            
            status = {
                "client_id": self.client_id,
                "client_type": "multi_learner",
                "current_round": self.current_round,
                "is_training": self.is_training,
                "is_running": self.is_running(),
                "is_connected": self.is_connected(),
                "learner_count": len(self.learners_info),
                "dataloader_count": len(self.dataloaders),
                "active_learners": [
                    learner_id for learner_id, info in self.learners_info.items() 
                    if info.is_active
                ],
                "training_history_length": len(self.training_history),
                "state_management": overall_status,
                "training_thread_alive": self.training_thread.is_alive() if self.training_thread else False
            }
            
            return status
        except Exception as e:
            self.logger.error(f"è·å–å®¢æˆ·ç«¯çŠ¶æ€å¤±è´¥: {e}")
            return {
                "client_id": self.client_id,
                "error": str(e),
                "timestamp": time.time()
            }
    
    def cleanup_client(self) -> None:
        """æ¸…ç†å¤šlearnerå®¢æˆ·ç«¯èµ„æº"""
        try:
            self.logger.info(f"æ¸…ç†å¤šlearnerå®¢æˆ·ç«¯: {self.client_id}")
            
            # åè°ƒå±‚çŠ¶æ€è½¬æ¢åˆ°å®ŒæˆçŠ¶æ€
            try:
                current_state = self.hierarchical_state_manager.get_coordination_state()
                if current_state != ClientLifecycleState.COMPLETED:
                    self.hierarchical_state_manager.transition_coordination_state(
                        ClientLifecycleState.COMPLETED,
                        {
                            "action": "multi_learner_client_cleanup",
                            "timestamp": time.time()
                        }
                    )
            except Exception as e:
                self.logger.warning(f"è½¬æ¢åˆ°å®ŒæˆçŠ¶æ€å¤±è´¥: {e}")
            
            # åœæ­¢é€šä¿¡
            self.stop()
            
            # åœæ­¢è®­ç»ƒ
            with self.training_lock:
                if self.is_training and self.training_thread and self.training_thread.is_alive():
                    if self.enhanced_training_engine:
                        self.enhanced_training_engine.stop_training()
                    self.is_training = False
                    self.training_thread.join(timeout=10)
            
            # é‡ç½®å®¢æˆ·ç«¯çŠ¶æ€
            self.current_round = 0
            self.is_training = False
            self.received_global_models.clear()
            self.training_history.clear()
            
            # æ¸…ç†æ‰€æœ‰learner
            for learner_info in self.learners_info.values():
                if hasattr(learner_info.learner_instance, 'cleanup'):
                    try:
                        learner_info.learner_instance.cleanup()
                    except Exception as e:
                        self.logger.warning(f"æ¸…ç†learnerå¤±è´¥ {learner_info.learner_id}: {e}")
            
            # æ¸…ç†å¢å¼ºè®­ç»ƒå¼•æ“
            if self.enhanced_training_engine and hasattr(self.enhanced_training_engine, 'cleanup_training_environment'):
                try:
                    self.enhanced_training_engine.cleanup_training_environment()
                except Exception as e:
                    self.logger.warning(f"æ¸…ç†è®­ç»ƒå¼•æ“å¤±è´¥: {e}")
            
            # æ¸…ç†å±‚çº§çŠ¶æ€ç®¡ç†å™¨
            try:
                self.hierarchical_state_manager.cleanup()
            except Exception as e:
                self.logger.warning(f"æ¸…ç†çŠ¶æ€ç®¡ç†å™¨å¤±è´¥: {e}")
            
            # æ¸…ç†æ•°æ®ç»“æ„
            self.learners_info.clear()
            self.dataloaders.clear()
            
            # æ¸…ç†ä¸Šä¸‹æ–‡çŠ¶æ€
            try:
                self.context.clear_scope("client")
            except Exception as e:
                self.logger.warning(f"æ¸…ç†contextå¤±è´¥: {e}")
            
            self.logger.info(f"å¤šlearnerå®¢æˆ·ç«¯æ¸…ç†å®Œæˆ: {self.client_id}")
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†å¤šlearnerå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    @classmethod
    def create_from_config(cls, config: DictConfig) -> 'MultiLearnerFederatedClient':
        """ä»é…ç½®åˆ›å»ºå¤šlearnerå®¢æˆ·ç«¯å®ä¾‹"""
        try:
            client_id = config.get('client', {}).get('id', 'multi_learner_client_0')
            logger.debug(f"ä»é…ç½®åˆ›å»ºå¤šlearnerå®¢æˆ·ç«¯: {client_id}")
            
            client = cls(client_id, config)
            
            logger.debug(f"å¤šlearnerå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ: {client_id}")
            return client
            
        except Exception as e:
            logger.error(f"ä»é…ç½®åˆ›å»ºå¤šlearnerå®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise FederationError(f"Multi-learner client creation failed: {e}")


# å‘åå…¼å®¹çš„åˆ«å
MultiLearnerClient = MultiLearnerFederatedClient