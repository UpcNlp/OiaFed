# fedcl/engine/enhanced_training_engine.py
"""
é‡æ„åçš„å¢å¼ºè®­ç»ƒå¼•æ“

çŠ¶æ€ç®¡ç†é‡æ„ï¼š
- åªè´Ÿè´£æ§åˆ¶å±‚çŠ¶æ€ç®¡ç†ï¼ˆTrainingPhaseStateï¼‰
- æ¥å—å¤–éƒ¨ä¼ å…¥çš„çŠ¶æ€ç®¡ç†å™¨
- ä¸åè°ƒå±‚çŠ¶æ€ç®¡ç†è§£è€¦
- ä¸“æ³¨äºè®­ç»ƒè¿‡ç¨‹æ§åˆ¶
- ä¿æŒæ‰€æœ‰åŸæœ‰åŠŸèƒ½
"""

import logging
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass
from pathlib import Path
import traceback

from ..federation.state.state_enums import TrainingPhaseState
from ..federation.state.state_manager import StateManager
from ..core.execution_context import ExecutionContext


@dataclass
class PhaseConfig:
    """è®­ç»ƒé˜¶æ®µé…ç½®"""
    name: str
    description: str
    epochs: List[int]
    learner_id: str
    scheduler_id: str
    inherit_from: Optional[List[str]] = None
    priority: int = 0
    execution_mode: str = "sequential"  # sequential, parallel, hybrid


@dataclass
class PhaseResult:
    """é˜¶æ®µæ‰§è¡Œç»“æœ"""
    phase_name: str
    executed_epochs: List[int]
    metrics: Dict[str, List[float]]
    final_state: Dict[str, Any]
    exported_knowledge: Optional[Dict[str, Any]]
    execution_time: float
    memory_usage: Dict[str, float]
    error_info: Optional[Exception] = None
    success: bool = True
    
    def get_final_metrics(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆæŒ‡æ ‡"""
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if hasattr(self, 'logger'):
            self.logger.debug(f"get_final_metrics called, metrics keys: {list(self.metrics.keys())}")
            for key, value in self.metrics.items():
                self.logger.debug(f"  metric '{key}': type={type(value)}, value={value}")
        
        final_metrics = {}
        for metric_name, metric_values in self.metrics.items():
            if metric_values:
                # ç¡®ä¿metric_valuesæ˜¯åˆ—è¡¨å¹¶ä¸”æœ‰å…ƒç´ 
                if isinstance(metric_values, list) and len(metric_values) > 0:
                    final_metrics[metric_name] = metric_values[-1]  # å–æœ€åä¸€ä¸ªå€¼
                elif isinstance(metric_values, (int, float, str)):
                    # å¦‚æœæ˜¯å•ä¸€å€¼ï¼Œç›´æ¥ä½¿ç”¨
                    final_metrics[metric_name] = metric_values
                else:
                    # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
                    try:
                        if hasattr(metric_values, '__getitem__') and len(metric_values) > 0:
                            final_metrics[metric_name] = metric_values[-1]
                    except (TypeError, IndexError, KeyError):
                        # å¦‚æœæ— æ³•è·å–æœ€åä¸€ä¸ªå€¼ï¼Œè·³è¿‡è¿™ä¸ªæŒ‡æ ‡
                        continue
        return final_metrics


@dataclass
class TrainingPlan:
    """å®Œæ•´è®­ç»ƒè®¡åˆ’"""
    total_epochs: int
    phases: List[PhaseConfig]
    execution_strategy: str = "sequential"  # sequential, parallel, hybrid
    
    def validate(self) -> bool:
        """éªŒè¯è®­ç»ƒè®¡åˆ’çš„æœ‰æ•ˆæ€§"""
        if not self.phases:
            raise ValueError("Training plan must have at least one phase")
            
        all_epochs = set()
        for phase in self.phases:
            for epoch in phase.epochs:
                if epoch in all_epochs:
                    raise ValueError(f"Epoch {epoch} appears in multiple phases")
                all_epochs.add(epoch)
        
        expected_epochs = set(range(1, self.total_epochs + 1))
        if all_epochs != expected_epochs:
            missing = expected_epochs - all_epochs
            extra = all_epochs - expected_epochs
            raise ValueError(f"Invalid epoch coverage. Missing: {missing}, Extra: {extra}")
        
        return True


class TrainingEngineError(Exception):
    """è®­ç»ƒå¼•æ“é”™è¯¯å¼‚å¸¸"""
    def __init__(self, message: str, context: Dict[str, Any] = None):
        super().__init__(message)
        self.context = context or {}


class RefactoredEnhancedTrainingEngine:
    """
    é‡æ„åçš„å¢å¼ºè®­ç»ƒå¼•æ“
    
    é‡æ„åçš„èŒè´£ï¼š
    - åªç®¡ç†æ§åˆ¶å±‚çŠ¶æ€ï¼ˆTrainingPhaseStateï¼‰
    - æ¥å—å¤–éƒ¨ä¼ å…¥çš„çŠ¶æ€ç®¡ç†å™¨
    - æ‰§è¡Œå¤šlearnerè®­ç»ƒè®¡åˆ’
    - ç®¡ç†è®­ç»ƒè¿‡ç¨‹å’Œé˜¶æ®µè½¬æ¢
    - æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
    
    çŠ¶æ€ç®¡ç†å˜æ›´ï¼š
    - ç§»é™¤è‡ªå·±åˆ›å»ºçš„çŠ¶æ€ç®¡ç†å™¨
    - ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„control_state_manager
    - åªå…³æ³¨è®­ç»ƒè¿‡ç¨‹çŠ¶æ€è½¬æ¢
    - ä¸åè°ƒå±‚çŠ¶æ€ç®¡ç†å®Œå…¨è§£è€¦
    """
    
    def __init__(self, 
                 context: ExecutionContext,
                 config: Dict[str, Any],
                 control_state_manager: StateManager):
        """
        åˆå§‹åŒ–é‡æ„åçš„è®­ç»ƒå¼•æ“
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            config_dict: é…ç½®å­—å…¸
            control_state_manager: å¤–éƒ¨ä¼ å…¥çš„æ§åˆ¶å±‚çŠ¶æ€ç®¡ç†å™¨
        """
        try:
            self.context = context
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
            
            # é‡æ„ï¼šä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„çŠ¶æ€ç®¡ç†å™¨ï¼Œè€Œä¸æ˜¯è‡ªå·±åˆ›å»º
            self.state_manager = control_state_manager
            
            # é…ç½®ç®¡ç†
            self.config_manager = self._create_config_manager(config)
            training_config = self.config_manager.get_training_config() if self.config_manager else {}
            
            # å½“å‰é˜¶æ®µ
            self._current_phase: Optional[str] = None
            self._training_start_time: Optional[float] = None
            
            # å¤šlearnerç›¸å…³å±æ€§
            self.learners: Dict[str, Any] = {}
            self.dataloaders: Dict[str, Any] = {}
            self.scheduler_manager: Optional[Any] = None
            self.training_plan: Optional[TrainingPlan] = None
            self.phase_results: Dict[str, PhaseResult] = {}
            
            # è¯„ä¼°ç›¸å…³å±æ€§
            self.test_dataloaders: Dict[str, Any] = {}
            self.evaluators: Dict[str, Any] = {}
            self.evaluation_engine: Optional[Any] = None
            
            # é’©å­ç³»ç»Ÿ
            self.hook_executor: Optional[Any] = None
            self.registered_hooks: List[Any] = []
            
            # å¹¶å‘æ§åˆ¶
            self.executor: Optional[ThreadPoolExecutor] = None
            self.max_concurrent_schedulers = training_config.get("max_concurrent_schedulers", 2) if training_config else 2
            
            # è®­ç»ƒæ§åˆ¶
            self.training_stopped = False
            self.training_paused = False
            self.training_lock = threading.RLock()
            
            # ç»Ÿè®¡ä¿¡æ¯
            self.stats = {
                'total_phases_executed': 0,
                'successful_phases': 0,
                'failed_phases': 0,
                'total_epochs_executed': 0,
                'total_training_time': 0.0,
                'last_training_start': None,
                'created_at': time.time()
            }
            
            # æ³¨å†Œæ§åˆ¶å±‚çŠ¶æ€å›è°ƒ
            self._register_control_state_callbacks()
            
            self.logger.debug("RefactoredEnhancedTrainingEngineåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise TrainingEngineError(f"Training engine initialization failed: {e}")
    
    @property
    def current_phase(self) -> Optional[str]:
        """è·å–å½“å‰è®­ç»ƒé˜¶æ®µ"""
        return self._current_phase
    
    @property
    def is_running(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        current_state = self.state_manager.get_current_state()
        return current_state in [
            TrainingPhaseState.RUNNING,
            TrainingPhaseState.PHASE_TRANSITION,
            TrainingPhaseState.EPOCH_EXECUTING,
            TrainingPhaseState.EVALUATING,
            TrainingPhaseState.AGGREGATING
        ]
    
    @property
    def is_paused(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æš‚åœ"""
        current_state = self.state_manager.get_current_state()
        return current_state == TrainingPhaseState.PAUSED
    
    @property
    def training_state(self) -> TrainingPhaseState:
        """è·å–è®­ç»ƒçŠ¶æ€"""
        return self.state_manager.get_current_state()
    
    def initialize_training(self) -> None:
        """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ"""
        try:
            self.logger.debug("åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
            
            current_state = self.state_manager.get_current_state()
            if current_state not in [TrainingPhaseState.UNINITIALIZED, TrainingPhaseState.FAILED]:
                self.logger.warning(f"ä»çŠ¶æ€ {current_state} é‡æ–°åˆå§‹åŒ–è®­ç»ƒ")
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> INITIALIZING
            self.state_manager.transition_to(
                TrainingPhaseState.INITIALIZING,
                {
                    "action": "training_initialization_started",
                    "timestamp": time.time()
                }
            )
            
            if not self.config_manager:
                raise TrainingEngineError("Configuration manager not initialized")
                
            # éªŒè¯å¹¶è§£æé…ç½®
            self._parse_training_plan()
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šINITIALIZING -> PREPARING
            self.state_manager.transition_to(
                TrainingPhaseState.PREPARING,
                {
                    "action": "preparing_training_components", 
                    "timestamp": time.time()
                }
            )
            
            # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
            self._initialize_dataloaders()
            self._initialize_learners()
            self._initialize_schedulers()
            self._initialize_evaluation_components()
            
            # åˆå§‹åŒ–å¹¶å‘æ‰§è¡Œå™¨
            if self.training_plan and self.training_plan.execution_strategy in ["parallel", "hybrid"]:
                self.executor = ThreadPoolExecutor(max_workers=self.max_concurrent_schedulers)
                self.logger.debug(f"å¹¶å‘æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§workeræ•°: {self.max_concurrent_schedulers}")
            
            # æ‰§è¡Œåˆå§‹åŒ–Hook
            self._execute_hooks("before_experiment",
                              training_plan=self.training_plan,
                              learners=list(self.learners.keys()),
                              dataloaders=list(self.dataloaders.keys()))
            
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.stats['last_training_start'] = time.time()
            self.training_stopped = False
            self.training_paused = False
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šPREPARING -> RUNNING (å‡†å¤‡å°±ç»ª)
            self.state_manager.transition_to(
                TrainingPhaseState.RUNNING,
                {
                    "action": "training_environment_ready",
                    "timestamp": time.time(),
                    "total_phases": len(self.training_plan.phases) if self.training_plan else 0
                }
            )
            
            self.logger.debug("è®­ç»ƒç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> FAILED
            self.state_manager.transition_to(
                TrainingPhaseState.FAILED,
                {
                    "action": "training_initialization_failed",
                    "error": str(e),
                    "timestamp": time.time()
                }
            )
            
            self.logger.error(f"è®­ç»ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            self._execute_hooks("on_error", error=e)
            raise TrainingEngineError(f"Training initialization failed: {e}")
    
    def execute_training_plan(self) -> Dict[str, PhaseResult]:
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒè®¡åˆ’
        
        Returns:
            Dict[str, PhaseResult]: å„é˜¶æ®µæ‰§è¡Œç»“æœ
        """
        try:
            self.logger.debug("å¼€å§‹æ‰§è¡Œè®­ç»ƒè®¡åˆ’")
            
            current_state = self.state_manager.get_current_state()
            
            # å¦‚æœçŠ¶æ€æ˜¯PREPARINGï¼Œè‡ªåŠ¨è½¬æ¢åˆ°RUNNING
            if current_state == TrainingPhaseState.PREPARING:
                self.logger.debug("è®­ç»ƒå¼•æ“ä»PREPARINGçŠ¶æ€è½¬æ¢åˆ°RUNNINGçŠ¶æ€")
                self.state_manager.transition_to(
                    TrainingPhaseState.RUNNING,
                    {
                        "action": "auto_transition_to_running",
                        "timestamp": time.time()
                    }
                )
            elif current_state != TrainingPhaseState.RUNNING:
                raise TrainingEngineError(
                    f"æ— æ³•åœ¨çŠ¶æ€ {current_state} ä¸‹æ‰§è¡Œè®­ç»ƒè®¡åˆ’",
                    {"current_state": current_state}
                )
            
            if not self.training_plan:
                self.logger.error("è®­ç»ƒè®¡åˆ’æœªåˆå§‹åŒ–")
                raise TrainingEngineError("Training plan not initialized")
            
            # æ·»åŠ è¯¦ç»†çš„è®­ç»ƒè®¡åˆ’æ—¥å¿—
            phases_count = len(self.training_plan.phases) if self.training_plan.phases else 0
            self.logger.debug(f"è®­ç»ƒè®¡åˆ’è¯¦æƒ…: æ€»epoch={self.training_plan.total_epochs}, é˜¶æ®µæ•°={phases_count}, æ‰§è¡Œç­–ç•¥={self.training_plan.execution_strategy}")
            
            if phases_count == 0:
                self.logger.warning("è®­ç»ƒè®¡åˆ’ä¸­æ²¡æœ‰é˜¶æ®µï¼")
                # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ç©ºç»“æœ
                self.phase_results.clear()
                return self.phase_results
            
            # è¾“å‡ºæ¯ä¸ªé˜¶æ®µçš„è¯¦ç»†ä¿¡æ¯
            for i, phase in enumerate(self.training_plan.phases):
                self.logger.debug(f"é˜¶æ®µ {i+1}: {phase.name}, learner={phase.learner_id}, epochs={phase.epochs}, scheduler={phase.scheduler_id}")
            
            self._training_start_time = time.time()
            
            # æ¸…ç©ºä¹‹å‰çš„ç»“æœ
            self.phase_results.clear()
            
            # æ‰§è¡Œè®­ç»ƒè®¡åˆ’å‰Hook
            self._execute_hooks("before_task", training_plan=self.training_plan)
            
            # æ ¹æ®æ‰§è¡Œç­–ç•¥æ‰§è¡Œè®­ç»ƒè®¡åˆ’
            if self.training_plan.execution_strategy == "sequential":
                self._execute_sequential_plan()
            elif self.training_plan.execution_strategy == "parallel":
                self._execute_parallel_plan()
            elif self.training_plan.execution_strategy == "hybrid":
                self._execute_hybrid_plan()
            else:
                raise TrainingEngineError(f"æœªçŸ¥çš„æ‰§è¡Œç­–ç•¥: {self.training_plan.execution_strategy}")
            
            # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
            total_time = time.time() - self._training_start_time
            self.stats['total_training_time'] += total_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            successful_phases = [r for r in self.phase_results.values() if r.success]
            self.stats['successful_phases'] += len(successful_phases)
            self.stats['failed_phases'] += len(self.phase_results) - len(successful_phases)
            self.stats['total_epochs_executed'] += sum(len(r.executed_epochs) for r in self.phase_results.values())
            
            # æ‰§è¡Œè®­ç»ƒè®¡åˆ’åHook
            self._execute_hooks("after_task",
                              phase_results=self.phase_results,
                              total_time=total_time)
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šRUNNING -> FINISHED
            self.state_manager.transition_to(
                TrainingPhaseState.FINISHED,
                {
                    "action": "training_plan_å®Œæˆ",
                    "total_time": total_time,
                    "successful_phases": len(successful_phases),
                    "total_phases": len(self.phase_results),
                    "timestamp": time.time()
                }
            )
            
            self.logger.debug(f"è®­ç»ƒè®¡åˆ’æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {total_time:.2f}s")
            
            return self.phase_results
            
        except Exception as e:
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> FAILED
            self.state_manager.transition_to(
                TrainingPhaseState.FAILED,
                {
                    "action": "training_plan_execution_failed",
                    "error": str(e),
                    "timestamp": time.time()
                }
            )
            
            self.logger.error(f"è®­ç»ƒè®¡åˆ’æ‰§è¡Œå¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            self._execute_hooks("on_error", error=e)
            raise TrainingEngineError(f"Training plan execution failed: {e}")
    
    def pause_training(self) -> None:
        """æš‚åœè®­ç»ƒ"""
        with self.training_lock:
            current_state = self.state_manager.get_current_state()
            
            if current_state not in [TrainingPhaseState.RUNNING, TrainingPhaseState.EPOCH_EXECUTING]:
                raise TrainingEngineError(
                    f"æ— æ³•åœ¨çŠ¶æ€ {current_state} ä¸‹æš‚åœè®­ç»ƒ",
                    {"current_state": current_state}
                )
            
            self.training_paused = True
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> PAUSED
            self.state_manager.transition_to(
                TrainingPhaseState.PAUSED,
                {
                    "action": "training_paused",
                    "previous_state": current_state,
                    "timestamp": time.time()
                }
            )
            
            self.logger.debug("è®­ç»ƒå·²æš‚åœ")
    
    def resume_training(self) -> None:
        """æ¢å¤è®­ç»ƒ"""
        with self.training_lock:
            current_state = self.state_manager.get_current_state()
            
            if current_state != TrainingPhaseState.PAUSED:
                raise TrainingEngineError(
                    f"æ— æ³•åœ¨çŠ¶æ€ {current_state} ä¸‹æ¢å¤è®­ç»ƒ",
                    {"current_state": current_state}
                )
            
            self.training_paused = False
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šPAUSED -> RUNNING
            self.state_manager.transition_to(
                TrainingPhaseState.RUNNING,
                {
                    "action": "training_resumed",
                    "timestamp": time.time()
                }
            )
            
            self.logger.debug("è®­ç»ƒå·²æ¢å¤")
    
    def stop_training(self) -> None:
        """åœæ­¢è®­ç»ƒ"""
        with self.training_lock:
            current_state = self.state_manager.get_current_state()
            
            if current_state in [TrainingPhaseState.FINISHED, TrainingPhaseState.FAILED]:
                self.logger.debug(f"è®­ç»ƒå·²åœ¨ç»ˆæ­¢çŠ¶æ€: {current_state}")
                return
            
            self.training_stopped = True
            self.training_paused = False
            
            # æ¸…ç†èµ„æº
            if self.executor:
                try:
                    self.executor.shutdown(wait=True)
                    self.executor = None
                    self.logger.debug("å¹¶å‘æ‰§è¡Œå™¨å·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­å¹¶å‘æ‰§è¡Œå™¨å¤±è´¥: {e}")
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šå½“å‰çŠ¶æ€ -> FINISHED
            self.state_manager.transition_to(
                TrainingPhaseState.FINISHED,
                {
                    "action": "training_å·²åœæ­¢",
                    "previous_state": current_state,
                    "timestamp": time.time()
                }
            )
            
            self.logger.debug(f"è®­ç»ƒå·²åœæ­¢ (ä¹‹å‰çŠ¶æ€: {current_state})")
    
    def _execute_sequential_plan(self) -> None:
        """é¡ºåºæ‰§è¡Œè®­ç»ƒè®¡åˆ’"""
        self.logger.debug("é¡ºåºæ‰§è¡Œè®­ç»ƒè®¡åˆ’")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºé˜¶æ®µ
        sorted_phases = sorted(self.training_plan.phases, key=lambda p: p.priority)
        
        for i, phase_config in enumerate(sorted_phases):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢è®­ç»ƒ
            if self.training_stopped:
                self.logger.debug("è®­ç»ƒè¢«åœæ­¢ï¼Œä¸­æ–­é˜¶æ®µæ‰§è¡Œ")
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœè®­ç»ƒ
            while self.training_paused and not self.training_stopped:
                self.logger.debug("è®­ç»ƒå·²æš‚åœï¼Œç­‰å¾…æ¢å¤...")
                time.sleep(1)
            
            if self.training_stopped:
                break
            
            self.logger.debug(f"æ‰§è¡Œé˜¶æ®µ {i+1}/{len(sorted_phases)}: {phase_config.name}")
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šRUNNING -> PHASE_TRANSITION
            self.state_manager.transition_to(
                TrainingPhaseState.PHASE_TRANSITION,
                {
                    "action": "phase_start",
                    "phase_name": phase_config.name,
                    "phase_index": i,
                    "total_phases": len(sorted_phases),
                    "timestamp": time.time()
                }
            )
            
            # æ‰§è¡Œé˜¶æ®µå‰Hook
            self._execute_hooks("before_phase",
                              phase_config=phase_config,
                              phase_name=phase_config.name)
            
            try:
                # æ‰§è¡Œå•ä¸ªé˜¶æ®µ
                phase_result = self._execute_single_phase(phase_config)
                self.phase_results[phase_config.name] = phase_result
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['total_phases_executed'] += 1
                if phase_result.success:
                    self.logger.debug(f"é˜¶æ®µ {phase_config.name} æ‰§è¡ŒæˆåŠŸ")
                else:
                    self.logger.warning(f"é˜¶æ®µ {phase_config.name} æ‰§è¡Œå¤±è´¥")
                
                # æ‰§è¡Œé˜¶æ®µåHook
                self._execute_hooks("after_phase",
                                  phase_config=phase_config,
                                  phase_result=phase_result)
                
                # å¤„ç†çŠ¶æ€ä¼ é€’
                self._handle_state_transfer(phase_config, phase_result)
                
            except Exception as e:
                self.logger.error(f"é˜¶æ®µ {phase_config.name} æ‰§è¡Œå¼‚å¸¸: {e}")
                
                # åˆ›å»ºå¤±è´¥ç»“æœ
                phase_result = PhaseResult(
                    phase_name=phase_config.name,
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    exported_knowledge=None,
                    execution_time=0.0,
                    memory_usage={},
                    error_info=e,
                    success=False
                )
                self.phase_results[phase_config.name] = phase_result
                self.stats['total_phases_executed'] += 1
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šPHASE_TRANSITION -> RUNNING (å‡†å¤‡ä¸‹ä¸€é˜¶æ®µ)
            if i < len(sorted_phases) - 1 and not self.training_stopped:  # ä¸æ˜¯æœ€åä¸€ä¸ªé˜¶æ®µ
                self.state_manager.transition_to(
                    TrainingPhaseState.RUNNING,
                    {
                        "action": "phase_completed_continue",
                        "å®Œæˆ_phase": phase_config.name,
                        "timestamp": time.time()
                    }
                )
    
    def _execute_parallel_plan(self) -> None:
        """å¹¶è¡Œæ‰§è¡Œè®­ç»ƒè®¡åˆ’"""
        self.logger.debug("å¹¶è¡Œæ‰§è¡Œè®­ç»ƒè®¡åˆ’")
        
        if not self.executor:
            raise TrainingEngineError("ThreadPoolExecutor not initialized for parallel execution")
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„é˜¶æ®µ
        priority_groups = self._group_phases_by_priority()
        
        for priority, phases in priority_groups.items():
            if self.training_stopped:
                break
                
            self.logger.debug(f"å¹¶è¡Œæ‰§è¡Œ {len(phases)} ä¸ªä¼˜å…ˆçº§ä¸º {priority} çš„é˜¶æ®µ")
            
            # å¹¶è¡Œæ‰§è¡ŒåŒä¼˜å…ˆçº§çš„é˜¶æ®µ
            futures = []
            for phase_config in phases:
                if self.training_stopped:
                    break
                future = self.executor.submit(self._execute_single_phase, phase_config)
                futures.append((phase_config, future))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for phase_config, future in futures:
                if self.training_stopped:
                    # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                    future.cancel()
                    continue
                    
                try:
                    phase_result = future.result(timeout=3600)  # 1å°æ—¶è¶…æ—¶
                    self.phase_results[phase_config.name] = phase_result
                    self.stats['total_phases_executed'] += 1
                    
                    if phase_result.success:
                        self.logger.debug(f"é˜¶æ®µ {phase_config.name} å¹¶è¡Œæ‰§è¡ŒæˆåŠŸ")
                    else:
                        self.logger.warning(f"é˜¶æ®µ {phase_config.name} å¹¶è¡Œæ‰§è¡Œå¤±è´¥")
                    
                    self._handle_state_transfer(phase_config, phase_result)
                    
                except Exception as e:
                    self.logger.error(f"é˜¶æ®µ {phase_config.name} å¹¶è¡Œæ‰§è¡Œå¼‚å¸¸: {e}")
                    self.phase_results[phase_config.name] = PhaseResult(
                        phase_name=phase_config.name,
                        executed_epochs=[],
                        metrics={},
                        final_state={},
                        exported_knowledge=None,
                        execution_time=0.0,
                        memory_usage={},
                        error_info=e,
                        success=False
                    )
                    self.stats['total_phases_executed'] += 1
    
    def _execute_hybrid_plan(self) -> None:
        """æ··åˆæ‰§è¡Œè®­ç»ƒè®¡åˆ’"""
        self.logger.debug("æ··åˆæ¨¡å¼æ‰§è¡Œè®­ç»ƒè®¡åˆ’")
        
        # æ ¹æ®é˜¶æ®µé…ç½®å†³å®šæ‰§è¡Œæ¨¡å¼
        for phase_config in sorted(self.training_plan.phases, key=lambda p: p.priority):
            if self.training_stopped:
                break
                
            if phase_config.execution_mode == "parallel":
                # å¹¶è¡Œæ‰§è¡Œå•ä¸ªé˜¶æ®µï¼ˆå¦‚æœæœ‰å¤šä¸ªlearnerï¼‰
                self.logger.debug(f"å¹¶è¡Œæ¨¡å¼æ‰§è¡Œé˜¶æ®µ: {phase_config.name}")
                self._execute_phase_parallel(phase_config)
            else:
                # é¡ºåºæ‰§è¡Œ
                self.logger.debug(f"é¡ºåºæ¨¡å¼æ‰§è¡Œé˜¶æ®µ: {phase_config.name}")
                phase_result = self._execute_single_phase(phase_config)
                self.phase_results[phase_config.name] = phase_result
                self.stats['total_phases_executed'] += 1
                self._handle_state_transfer(phase_config, phase_result)
    
    def _execute_phase_parallel(self, phase_config: PhaseConfig) -> None:
        """å¹¶è¡Œæ‰§è¡Œå•ä¸ªé˜¶æ®µ"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¹¶è¡Œé€»è¾‘
        # ç›®å‰ç®€åŒ–ä¸ºé¡ºåºæ‰§è¡Œ
        phase_result = self._execute_single_phase(phase_config)
        self.phase_results[phase_config.name] = phase_result
        self.stats['total_phases_executed'] += 1
        self._handle_state_transfer(phase_config, phase_result)
    
    def _execute_single_phase(self, phase_config: PhaseConfig) -> PhaseResult:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒé˜¶æ®µ
        
        Args:
            phase_config: é˜¶æ®µé…ç½®
            
        Returns:
            PhaseResult: é˜¶æ®µæ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        self._current_phase = phase_config.name
        
        try:
            self.logger.debug(f"æ‰§è¡Œé˜¶æ®µ '{phase_config.name}' (learner: '{phase_config.learner_id}')")
            
            # æ£€æŸ¥æ˜¯å¦åœæ­¢æˆ–æš‚åœ
            if self.training_stopped:
                raise TrainingEngineError("Training stopped during phase execution")
            
            # è·å–learnerã€dataloaderå’Œscheduler
            learner = self.learners.get(phase_config.learner_id)
            if not learner:
                raise TrainingEngineError(f"Learner '{phase_config.learner_id}' not found")
            
            # è·å–dataloaderï¼Œæ”¯æŒå¤šç§æŸ¥æ‰¾æ–¹å¼
            dataloader = self._get_dataloader_for_phase(phase_config)
            if not dataloader:
                raise TrainingEngineError(f"No dataloader found for phase '{phase_config.name}'")
            
            # è·å–schedulerï¼ˆå¯é€‰ï¼‰
            scheduler = None
            if phase_config.scheduler_id and phase_config.scheduler_id != "None":
                scheduler = self.scheduler_manager.get_scheduler(phase_config.scheduler_id)
                if not scheduler:
                    self.logger.warning(f"Scheduler '{phase_config.scheduler_id}' not found, using default scheduler")
                    # ä½¿ç”¨é»˜è®¤scheduleræˆ–åˆ›å»ºä¸€ä¸ªç®€å•çš„è®­ç»ƒscheduler
                    scheduler = self._create_default_scheduler()
            else:
                self.logger.debug("æ²¡æœ‰æŒ‡å®šschedulerï¼Œä½¿ç”¨é»˜è®¤scheduler")
                scheduler = self._create_default_scheduler()
            
            # å‡†å¤‡ç»§æ‰¿çŠ¶æ€
            inherited_state = self._prepare_inherited_state(phase_config)
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šPHASE_TRANSITION -> EPOCH_EXECUTING
            self.state_manager.transition_to(
                TrainingPhaseState.EPOCH_EXECUTING,
                {
                    "action": "epoch_execution_started",
                    "phase_name": phase_config.name,
                    "epoch_range": phase_config.epochs,
                    "learner_id": phase_config.learner_id,
                    "timestamp": time.time()
                }
            )
            
            # æ‰§è¡Œepochè°ƒåº¦
            self.logger.info(f"ğŸš€ [è®­ç»ƒè°ƒåº¦] å¼€å§‹æ‰§è¡Œepochè°ƒåº¦: epoch_range={phase_config.epochs}, scheduler={type(scheduler).__name__}")
            
            # æ ¹æ®è°ƒåº¦å™¨ç±»å‹ä½¿ç”¨ä¸åŒçš„å‚æ•°
            if hasattr(scheduler.execute_epochs, '__code__') and 'training_engine' in scheduler.execute_epochs.__code__.co_varnames:
                # å¦‚æœæ”¯æŒtraining_engineå‚æ•°ï¼ˆå¦‚BasicTrainingSchedulerï¼‰
                self.logger.info(f"ğŸš€ [è®­ç»ƒè°ƒåº¦] ä½¿ç”¨æ”¯æŒtraining_engineå‚æ•°çš„è°ƒåº¦å™¨")
                execution_result = scheduler.execute_epochs(
                    learner=learner,
                    dataloader=dataloader,
                    epoch_range=phase_config.epochs,
                    inherited_state=inherited_state,
                    context=self.context,
                    training_engine=self  # ä¼ é€’training_engineå‚æ•°
                )
            else:
                # æ ‡å‡†è°ƒåº¦å™¨ä¸æ”¯æŒtraining_engineå‚æ•°ï¼ˆå¦‚StandardEpochSchedulerï¼‰
                self.logger.info(f"ğŸš€ [è®­ç»ƒè°ƒåº¦] ä½¿ç”¨æ ‡å‡†è°ƒåº¦å™¨ï¼ˆä¸æ”¯æŒtraining_engineå‚æ•°ï¼‰")
                execution_result = scheduler.execute_epochs(
                    learner=learner,
                    dataloader=dataloader,
                    epoch_range=phase_config.epochs,
                    inherited_state=inherited_state,
                    context=self.context
                )
            self.logger.info(f"âœ… [è®­ç»ƒè°ƒåº¦] Epochè°ƒåº¦å®Œæˆ: executed_epochs={execution_result.executed_epochs}, final_state_keys={list(execution_result.final_state.keys())}")
            
            # æ§åˆ¶å±‚çŠ¶æ€è½¬æ¢ï¼šEPOCH_EXECUTING -> EVALUATING
            self.state_manager.transition_to(
                TrainingPhaseState.EVALUATING,
                {
                    "action": "phase_evaluation_started",
                    "phase_name": phase_config.name,
                    "timestamp": time.time()
                }
            )
            
            # æ‰§è¡Œé˜¶æ®µè¯„ä¼°ï¼ˆè®­ç»ƒå®Œæˆåï¼‰
            evaluation_results = {}
            try:
                if self.evaluation_engine:
                    self.logger.info(f"ğŸ“Š [é˜¶æ®µè¯„ä¼°] å¼€å§‹è¯„ä¼°é˜¶æ®µ '{phase_config.name}' çš„è®­ç»ƒæ•ˆæœ")
                    evaluation_results = self.evaluate_learner(phase_config.learner_id, learner)
                    self.logger.debug(f"ğŸ“Š [é˜¶æ®µè¯„ä¼°] é˜¶æ®µ '{phase_config.name}' è¯„ä¼°å®Œæˆ: {list(evaluation_results.keys())}")
            except Exception as e:
                self.logger.warning(f"âŒ [é˜¶æ®µè¯„ä¼°] é˜¶æ®µ '{phase_config.name}' è¯„ä¼°å¤±è´¥: {e}")
            
            # æ„å»ºé˜¶æ®µç»“æœ
            phase_result = PhaseResult(
                phase_name=phase_config.name,
                executed_epochs=execution_result.executed_epochs,
                metrics=execution_result.metrics,
                final_state=execution_result.final_state,
                exported_knowledge=execution_result.exported_knowledge,
                execution_time=time.time() - start_time,
                memory_usage=execution_result.memory_usage,
                error_info=None,
                success=True
            )
            
            # å°†è¯„ä¼°ç»“æœæ·»åŠ åˆ°é˜¶æ®µç»“æœä¸­
            if evaluation_results:
                phase_result.metrics.update({"evaluation": evaluation_results})
            
            self.logger.debug(f"é˜¶æ®µ '{phase_config.name}' æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ {phase_result.execution_time:.2f}s")
            return phase_result
            
        except Exception as e:
            self.logger.error(f"é˜¶æ®µ '{phase_config.name}' æ‰§è¡Œå¤±è´¥: {e}")
            
            # é˜¶æ®µå¤±è´¥æ—¶ï¼ŒçŠ¶æ€å›åˆ°RUNNINGçŠ¶æ€ä»¥ä¾¿ç»§ç»­ä¸‹ä¸€é˜¶æ®µ
            try:
                self.state_manager.transition_to(
                    TrainingPhaseState.RUNNING,
                    {
                        "action": "phase_failed_continue",
                        "failed_phase": phase_config.name,
                        "error": str(e),
                        "timestamp": time.time()
                    }
                )
            except Exception as state_error:
                self.logger.warning(f"çŠ¶æ€è½¬æ¢å¤±è´¥: {state_error}")
            
            return PhaseResult(
                phase_name=phase_config.name,
                executed_epochs=[],
                metrics={},
                final_state={},
                exported_knowledge=None,
                execution_time=time.time() - start_time,
                memory_usage={},
                error_info=e,
                success=False
            )
        finally:
            self._current_phase = None
    
    def _create_default_scheduler(self):
        """åˆ›å»ºé»˜è®¤çš„è®­ç»ƒscheduler"""
        try:
            from .epoch_scheduler import StandardEpochScheduler, SchedulerPriority
            return StandardEpochScheduler("default_scheduler", {}, SchedulerPriority.NORMAL)
        except ImportError:
            # å¦‚æœæ²¡æœ‰StandardEpochSchedulerï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„scheduler
            return self._create_basic_scheduler()
    
    def _create_basic_scheduler(self):
        """åˆ›å»ºåŸºæœ¬çš„å†…ç½®scheduler"""
        class BasicTrainingScheduler:
            def execute_epochs(self, learner, dataloader, epoch_range, inherited_state=None, context=None, training_engine=None):
                """åŸºæœ¬çš„epochæ‰§è¡Œé€»è¾‘"""
                from dataclasses import dataclass
                from typing import List, Dict, Any
                
                @dataclass
                class ExecutionResult:
                    executed_epochs: List[int]
                    metrics: Dict[str, List[float]]
                    final_state: Dict[str, Any]
                    exported_knowledge: Optional[Dict[str, Any]]
                    memory_usage: Dict[str, float]
                
                result = ExecutionResult(
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    exported_knowledge=None,
                    memory_usage={}
                )
                
                if training_engine:
                    training_engine.logger.debug(f"ä½¿ç”¨åŸºæœ¬scheduleræ‰§è¡Œepochs: {epoch_range}")
                else:
                    print(f"DEBUG: åŸºæœ¬scheduleræ‰§è¡Œepochs: {epoch_range} (æ— training_engine)")
                
                for epoch in epoch_range:
                    if training_engine:
                        training_engine.logger.debug(f"å¼€å§‹æ‰§è¡Œepoch {epoch}")
                    
                    # æ£€æŸ¥æ˜¯å¦åœæ­¢
                    if training_engine and training_engine.training_stopped:
                        if training_engine:
                            training_engine.logger.debug(f"è®­ç»ƒå·²åœæ­¢ï¼Œè·³å‡ºepoch {epoch}")
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦æš‚åœ
                    if training_engine:
                        while training_engine.training_paused and not training_engine.training_stopped:
                            time.sleep(0.1)
                        if training_engine.training_stopped:
                            break
                    
                    # æ‰§è¡Œä¸€ä¸ªepochçš„è®­ç»ƒ
                    try:
                        if training_engine:
                            training_engine.logger.debug(f"å¼€å§‹è®­ç»ƒepoch {epoch}")
                        
                        if hasattr(learner, 'train_epoch'):
                            if training_engine:
                                training_engine.logger.debug(f"ä½¿ç”¨learner.train_epochæ–¹æ³•")
                            metrics = learner.train_epoch(dataloader, epoch)
                        else:
                            # å¦‚æœlearneræ²¡æœ‰train_epochæ–¹æ³•ï¼Œä½¿ç”¨åŸºæœ¬è®­ç»ƒé€»è¾‘
                            if training_engine:
                                training_engine.logger.debug(f"ä½¿ç”¨åŸºæœ¬è®­ç»ƒé€»è¾‘")
                                metrics = training_engine._basic_training_epoch(learner, dataloader, epoch)
                            else:
                                # å¦‚æœæ²¡æœ‰training_engineï¼Œæ‰§è¡Œç®€å•çš„è®­ç»ƒé€»è¾‘
                                metrics = self._fallback_training_epoch(learner, dataloader, epoch)
                        
                        result.executed_epochs.append(epoch)
                        
                        # æ”¶é›†æŒ‡æ ‡
                        for metric_name, value in metrics.items():
                            if metric_name not in result.metrics:
                                result.metrics[metric_name] = []
                            result.metrics[metric_name].append(value)
                            
                        if training_engine:
                            training_engine.logger.info(f"Epoch {epoch} å®Œæˆï¼ŒæŸå¤±: {metrics.get('loss', 'N/A')}")
                            
                            # ä¿å­˜å®¢æˆ·ç«¯æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆåœ¨æ¯ä¸ªepochåï¼‰
                            if hasattr(learner, 'learner_id'):
                                training_engine.save_client_model_checkpoint(
                                    learner.learner_id, epoch, metrics)
                        else:
                            print(f"DEBUG: Epoch {epoch} å®Œæˆï¼ŒæŸå¤±: {metrics.get('loss', 'N/A')}")
                            
                    except Exception as e:
                        if training_engine:
                            training_engine.logger.error(f"Epoch {epoch} è®­ç»ƒå¤±è´¥: {e}")
                        else:
                            print(f"DEBUG: Epoch {epoch} è®­ç»ƒå¤±è´¥: {e}")
                        break
                
                # æ›´æ–°æœ€ç»ˆçŠ¶æ€
                if result.executed_epochs:
                    # è·å–è®­ç»ƒåçš„æ¨¡å‹å‚æ•°
                    model_update = {}
                    if hasattr(learner, 'model') and hasattr(learner.model, 'state_dict'):
                        try:
                            # è·å–æ¨¡å‹å‚æ•°çš„å·®å€¼æˆ–å®Œæ•´å‚æ•°
                            state_dict = learner.model.state_dict()
                            model_update = {k: v.clone().detach() for k, v in state_dict.items()}
                            if training_engine:
                                training_engine.logger.debug(f"æˆåŠŸæå–æ¨¡å‹å‚æ•°ï¼Œå…± {len(model_update)} ä¸ªå‚æ•°")
                                training_engine.logger.debug(f"æ¨¡å‹å‚æ•°é”®: {list(model_update.keys())[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªé”®
                            else:
                                print(f"DEBUG: æˆåŠŸæå–æ¨¡å‹å‚æ•°ï¼Œå…± {len(model_update)} ä¸ªå‚æ•°")
                                print(f"DEBUG: æ¨¡å‹å‚æ•°é”®: {list(model_update.keys())[:5]}...")
                        except Exception as e:
                            if training_engine:
                                training_engine.logger.warning(f"æå–æ¨¡å‹å‚æ•°å¤±è´¥: {e}")
                            else:
                                print(f"DEBUG: æå–æ¨¡å‹å‚æ•°å¤±è´¥: {e}")
                            model_update = {}
                    else:
                        if training_engine:
                            training_engine.logger.warning(f"learneræ²¡æœ‰modelæˆ–modelæ²¡æœ‰state_dictæ–¹æ³•")
                        else:
                            print(f"DEBUG: learneræ²¡æœ‰modelæˆ–modelæ²¡æœ‰state_dictæ–¹æ³•")
                    
                    try:
                        result.final_state.update({
                            "last_epoch": result.executed_epochs[-1],
                            "final_metrics": {k: v[-1] if v else 0 for k, v in result.metrics.items()},
                            "model_update": model_update
                        })
                        if training_engine:
                            training_engine.logger.debug(f"final_stateæ›´æ–°æˆåŠŸï¼ŒåŒ…å«: {list(result.final_state.keys())}")
                        else:
                            print(f"DEBUG: final_stateæ›´æ–°æˆåŠŸï¼ŒåŒ…å«: {list(result.final_state.keys())}")
                    except Exception as e:
                        if training_engine:
                            training_engine.logger.error(f"æ›´æ–°final_stateå¤±è´¥: {e}")
                        else:
                            print(f"DEBUG: æ›´æ–°final_stateå¤±è´¥: {e}")
                else:
                    if training_engine:
                        training_engine.logger.warning(f"æ²¡æœ‰æ‰§è¡Œä»»ä½•epochï¼Œæ— æ³•æå–æ¨¡å‹å‚æ•°")
                    else:
                        print(f"DEBUG: æ²¡æœ‰æ‰§è¡Œä»»ä½•epochï¼Œæ— æ³•æå–æ¨¡å‹å‚æ•°")
                
                return result
            
            def _basic_training_epoch(self, learner, dataloader, epoch):
                """åŸºæœ¬çš„è®­ç»ƒepochå®ç°"""
                total_loss = 0.0
                num_batches = 0
                
                if hasattr(learner, 'model') and hasattr(learner, 'optimizer'):
                    learner.model.train()
                    for batch_data in dataloader:
                        learner.optimizer.zero_grad()
                        
                        if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 2:
                            inputs, targets = batch_data[0], batch_data[1]
                        else:
                            # å‡è®¾batch_dataæ˜¯è¾“å…¥æ•°æ®ï¼Œæ²¡æœ‰æ ‡ç­¾
                            inputs = batch_data
                            targets = None
                        
                        if hasattr(learner.model, '__call__'):
                            outputs = learner.model(inputs)
                            
                            if targets is not None and hasattr(learner, 'criterion'):
                                loss = learner.criterion(outputs, targets)
                                loss.backward()
                                learner.optimizer.step()
                                total_loss += loss.item()
                            
                        num_batches += 1
                        
                        # é¿å…æ— é™è®­ç»ƒï¼Œé™åˆ¶æœ€å¤§æ‰¹æ¬¡æ•°
                        if num_batches >= 100:  # å¢åŠ åˆ°100ä¸ªæ‰¹æ¬¡è¿›è¡Œæ›´å……åˆ†çš„è®­ç»ƒ
                            break
                
                avg_loss = total_loss / max(num_batches, 1)
                return {"loss": avg_loss, "epochs": epoch}
                
            def _fallback_training_epoch(self, learner, dataloader, epoch):
                """å¤‡é€‰çš„åŸºæœ¬è®­ç»ƒepochå®ç°ï¼ˆå½“æ²¡æœ‰training_engineæ—¶ä½¿ç”¨ï¼‰"""
                print(f"DEBUG: æ‰§è¡Œå¤‡é€‰è®­ç»ƒepoch {epoch}")
                total_loss = 0.0
                num_batches = 0
                
                if hasattr(learner, 'model') and hasattr(learner, 'optimizer'):
                    learner.model.train()
                    for batch_data in dataloader:
                        learner.optimizer.zero_grad()
                        
                        if isinstance(batch_data, (list, tuple)) and len(batch_data) >= 2:
                            inputs, targets = batch_data[0], batch_data[1]
                        else:
                            # å‡è®¾batch_dataæ˜¯è¾“å…¥æ•°æ®ï¼Œæ²¡æœ‰æ ‡ç­¾
                            inputs = batch_data
                            targets = None
                        
                        if hasattr(learner.model, '__call__'):
                            outputs = learner.model(inputs)
                            
                            if targets is not None and hasattr(learner, 'criterion'):
                                loss = learner.criterion(outputs, targets)
                                loss.backward()
                                learner.optimizer.step()
                                total_loss += loss.item()
                            
                        num_batches += 1
                        
                        # é¿å…æ— é™è®­ç»ƒï¼Œé™åˆ¶æœ€å¤§æ‰¹æ¬¡æ•°
                        if num_batches >= 100:  # å¢åŠ åˆ°100ä¸ªæ‰¹æ¬¡è¿›è¡Œæ›´å……åˆ†çš„è®­ç»ƒ
                            break
                
                avg_loss = total_loss / max(num_batches, 1)
                print(f"DEBUG: å¤‡é€‰è®­ç»ƒepoch {epoch} å®Œæˆï¼ŒæŸå¤±: {avg_loss}")
                return {"loss": avg_loss, "epochs": epoch}
        
        return BasicTrainingScheduler()
    
    def _get_dataloader_for_phase(self, phase_config: PhaseConfig) -> Optional[Any]:
        """è·å–é˜¶æ®µå¯¹åº”çš„dataloader"""
        # 1. å°è¯•ä½¿ç”¨learner_idå¯¹åº”çš„dataloader
        if phase_config.learner_id in self.dataloaders:
            return self.dataloaders[phase_config.learner_id]
        
        # 2. å°è¯•ä½¿ç”¨phaseåç§°å¯¹åº”çš„dataloader
        if phase_config.name in self.dataloaders:
            return self.dataloaders[phase_config.name]
        
        # 3. ä½¿ç”¨é»˜è®¤dataloader
        if "default" in self.dataloaders:
            return self.dataloaders["default"]
        
        # 4. è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„dataloader
        if self.dataloaders:
            return list(self.dataloaders.values())[0]
        
        return None
    
    def _register_control_state_callbacks(self):
        """æ³¨å†Œæ§åˆ¶å±‚çŠ¶æ€å›è°ƒ"""
        try:
            self.state_manager.register_callback(
                self._on_control_state_change,
                callback_id="training_engine_control_callback"
            )
            
            self.logger.debug("æ§åˆ¶å±‚çŠ¶æ€å›è°ƒæ³¨å†Œå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ³¨å†Œæ§åˆ¶å±‚çŠ¶æ€å›è°ƒå¤±è´¥: {e}")
    
    def _on_control_state_change(self, old_state: TrainingPhaseState,
                               new_state: TrainingPhaseState,
                               metadata: Dict[str, Any]):
        """æ§åˆ¶å±‚çŠ¶æ€å˜åŒ–å›è°ƒ"""
        self.logger.debug(
            f"è®­ç»ƒå¼•æ“æ§åˆ¶å±‚çŠ¶æ€å˜åŒ–: {old_state.name} -> {new_state.name}"
        )
        
        # æ ¹æ®çŠ¶æ€å˜åŒ–æ‰§è¡Œç›¸åº”æ“ä½œ
        if new_state == TrainingPhaseState.RUNNING:
            self.logger.debug("è®­ç»ƒå¼•æ“å‡†å¤‡å°±ç»ªï¼Œç­‰å¾…è®­ç»ƒä»»åŠ¡")
        elif new_state == TrainingPhaseState.EPOCH_EXECUTING:
            phase_name = metadata.get("phase_name", "unknown")
            learner_id = metadata.get("learner_id", "unknown")
            self.logger.info(f"å¼€å§‹æ‰§è¡Œè®­ç»ƒé˜¶æ®µ: {phase_name} (learner: {learner_id})")
        elif new_state == TrainingPhaseState.FINISHED:
            total_time = metadata.get("total_time", 0)
            successful_phases = metadata.get("successful_phases", 0)
            total_phases = metadata.get("total_phases", 0)
            self.logger.debug(f"è®­ç»ƒè®¡åˆ’æ‰§è¡Œå®Œæˆ: {successful_phases}/{total_phases} é˜¶æ®µæˆåŠŸ, è€—æ—¶ {total_time:.2f}s")
        elif new_state == TrainingPhaseState.FAILED:
            error = metadata.get("error", "unknown error")
            self.logger.error(f"è®­ç»ƒæ‰§è¡Œå¤±è´¥: {error}")
        elif new_state == TrainingPhaseState.PAUSED:
            self.logger.debug("è®­ç»ƒå·²æš‚åœ")
        
        # å‘å¸ƒæ§åˆ¶å±‚çŠ¶æ€å˜åŒ–äº‹ä»¶
        self.context.publish_event("training_engine_state_changed", {
            "old_state": old_state.name,
            "new_state": new_state.name,
            "metadata": metadata,
            "timestamp": metadata.get("timestamp", time.time())
        })
    
    # ===== è®­ç»ƒç®¡ç†æ–¹æ³• =====
    
    def register_training_hooks(self, hooks: List[Any]) -> None:
        """æ³¨å†Œè®­ç»ƒé’©å­"""
        try:
            if not self.hook_executor:
                self.hook_executor = self._create_hook_executor()
                
            self.hook_executor.register_hooks(hooks)
            self.registered_hooks.extend(hooks)
            self.logger.debug(f"æ³¨å†Œ {len(hooks)} ä¸ªè®­ç»ƒé’©å­")
        except Exception as e:
            self.logger.error(f"æ³¨å†Œè®­ç»ƒé’©å­å¤±è´¥: {e}")
    
    def save_client_model_checkpoint(self, learner_id: str, epoch: int, metrics: Optional[Dict[str, Any]] = None) -> None:
        """ä¿å­˜å®¢æˆ·ç«¯æ¨¡å‹æ£€æŸ¥ç‚¹"""
        try:
            if learner_id not in self.learners:
                self.logger.warning(f"Learner {learner_id} not found for checkpoint saving")
                return
            
            learner = self.learners[learner_id]
            
            # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®
            checkpoint_data = {
                'learner_id': learner_id,
                'epoch': epoch,
                'metrics': metrics or {},
                'timestamp': time.time()
            }
            
            # å¦‚æœlearneræœ‰æ¨¡å‹ï¼Œä¿å­˜æ¨¡å‹çŠ¶æ€
            if hasattr(learner, 'get_model'):
                model = learner.get_model()
                if model is not None:
                    checkpoint_data['model_state_dict'] = model.state_dict()
            
            # å¦‚æœlearneræœ‰çŠ¶æ€ï¼Œä¿å­˜learnerçŠ¶æ€
            if hasattr(learner, 'get_state'):
                checkpoint_data['learner_state'] = learner.get_state()
            
            # æ‰§è¡Œå®¢æˆ·ç«¯æ¨¡å‹ä¿å­˜é’©å­
            self._execute_hooks("after_epoch", 
                              learner_id=learner_id,
                              model=getattr(learner, 'get_model', lambda: None)(),
                              epoch=epoch,
                              metrics=metrics,
                              checkpoint_data=checkpoint_data)
            
            self.logger.debug(f"å®¢æˆ·ç«¯æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜: learner={learner_id}, epoch={epoch}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å®¢æˆ·ç«¯æ¨¡å‹æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def save_training_checkpoint(self, path: str) -> None:
        """ä¿å­˜è®­ç»ƒå¼•æ“çŠ¶æ€æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = {
                "state": str(self.training_state),
                "phase_results": self.phase_results,
                "training_plan": self.training_plan,
                "current_phase": self.current_phase,
                "stats": self.stats,
                "training_start_time": self._training_start_time,
                "timestamp": time.time(),
                "learners_state": {}
            }
            
            # ä¿å­˜æ‰€æœ‰learnerçš„çŠ¶æ€
            for learner_id, learner in self.learners.items():
                if hasattr(learner, 'get_state'):
                    checkpoint_data["learners_state"][learner_id] = learner.get_state()
            
            # æ‰§è¡Œè®­ç»ƒå¼•æ“æ£€æŸ¥ç‚¹ä¿å­˜é’©å­
            self._execute_hooks("training_checkpoint_save",
                              checkpoint_data=checkpoint_data,
                              path=path)
            
            self.context.save_checkpoint(path, checkpoint_data)
            self.logger.debug(f"è®­ç»ƒæ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def load_training_checkpoint(self, path: str) -> None:
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = self.context.load_checkpoint(path)
            
            # æ¢å¤çŠ¶æ€
            saved_state = checkpoint_data.get("state")
            if saved_state:
                for state in TrainingPhaseState:
                    if state.name == saved_state:
                        self.state_manager.transition_to(state, {"action": "checkpoint_restored"})
                        break
            
            self.phase_results = checkpoint_data.get("phase_results", {})
            self.training_plan = checkpoint_data.get("training_plan")
            self._current_phase = checkpoint_data.get("current_phase")
            
            if "stats" in checkpoint_data:
                self.stats.update(checkpoint_data["stats"])
            
            self._training_start_time = checkpoint_data.get("training_start_time")
            
            self.logger.debug(f"è®­ç»ƒæ£€æŸ¥ç‚¹å·²ä» {path} åŠ è½½")
        except Exception as e:
            self.logger.error(f"åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def cleanup_training_environment(self) -> None:
        """æ¸…ç†è®­ç»ƒç¯å¢ƒ"""
        try:
            self.logger.debug("æ¸…ç†è®­ç»ƒç¯å¢ƒ")
            
            # åœæ­¢è®­ç»ƒ
            self.training_stopped = True
            
            # å…³é—­å¹¶å‘æ‰§è¡Œå™¨
            if self.executor:
                try:
                    self.executor.shutdown(wait=True)
                    self.executor = None
                    self.logger.debug("å¹¶å‘æ‰§è¡Œå™¨å·²å…³é—­")
                except Exception as e:
                    self.logger.warning(f"å…³é—­å¹¶å‘æ‰§è¡Œå™¨å¤±è´¥: {e}")
            
            # æ¸…ç†ç»„ä»¶
            self.dataloaders.clear()
            self.learners.clear()
            self.phase_results.clear()
            
            # æ¸…ç†è¯„ä¼°ç»„ä»¶
            self.test_dataloaders.clear()
            self.evaluators.clear()
            if self.evaluation_engine:
                self.evaluation_engine = None
            
            if hasattr(self, 'scheduler_manager') and self.scheduler_manager:
                try:
                    self.scheduler_manager.cleanup()
                except Exception as e:
                    self.logger.warning(f"æ¸…ç†è°ƒåº¦å™¨ç®¡ç†å™¨å¤±è´¥: {e}")
            
            # æ¸…ç†Hook
            self.registered_hooks.clear()
            if self.hook_executor:
                self.hook_executor = None
            
            # é‡ç½®çŠ¶æ€
            self._current_phase = None
            self._training_start_time = None
            self.training_paused = False
            
            # æ³¨æ„ï¼šä¸æ¸…ç†state_managerï¼Œå› ä¸ºå®ƒæ˜¯å¤–éƒ¨ä¼ å…¥çš„
            
            self.logger.debug("è®­ç»ƒç¯å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†è®­ç»ƒç¯å¢ƒå¤±è´¥: {e}")
    
    # ===== å†…éƒ¨å®ç°æ–¹æ³• =====
    
    def _create_config_manager(self, config_dict):
        """åˆ›å»ºé…ç½®ç®¡ç†å™¨"""
        class ConfigManager:
            def __init__(self, config_dict):
                self.config = config_dict or {}
                
            def get_training_config(self):
                return self.config.get("training", {})
                
            def get_training_plan(self):
                return self.config.get("training_plan", {})
                
            def get_dataloader_configs(self):
                return self.config.get("dataloaders", {})
                
            def get_learner_configs(self):
                return self.config.get("learners", {})
                
            def get_scheduler_configs(self):
                return self.config.get("schedulers", {})
                
            def get_evaluation_config(self):
                return self.config.get("evaluation", {})
                
            def get_test_data_configs(self):
                return self.config.get("test_datas", {})
                
            def get_evaluator_configs(self):
                return self.config.get("evaluators", {})
                
            def get_system_config(self):
                return self.config.get("system", {})
        
        if config_dict:
            return ConfigManager(config_dict)
        return None
    
    def _execute_hooks(self, phase: str, **kwargs) -> None:
        """æ‰§è¡Œé’©å­é˜¶æ®µ"""
        try:
            if self.hook_executor:
                self.hook_executor.execute_hooks(phase, **kwargs)
            
            # è®°å½•é’©å­æ‰§è¡Œåˆ°ä¸Šä¸‹æ–‡
            self.context.emit_event(f"hook_{phase}", kwargs)
        except Exception as e:
            self.logger.warning(f"Hookæ‰§è¡Œå¤±è´¥ in phase {phase}: {e}")
    
    def _prepare_inherited_state(self, phase_config: PhaseConfig) -> Optional[Dict[str, Any]]:
        """å‡†å¤‡ç»§æ‰¿çŠ¶æ€"""
        if not phase_config.inherit_from:
            return None
        
        inherited_state = {}
        for source_phase in phase_config.inherit_from:
            if source_phase in self.phase_results:
                source_result = self.phase_results[source_phase]
                
                # åˆå¹¶çŠ¶æ€
                if source_result.final_state:
                    inherited_state.update(source_result.final_state)
                
                # åˆå¹¶å¯¼å‡ºçš„çŸ¥è¯†
                if source_result.exported_knowledge:
                    inherited_state.setdefault("knowledge", {}).update(source_result.exported_knowledge)
        
        return inherited_state if inherited_state else None
    
    def _handle_state_transfer(self, phase_config: PhaseConfig, phase_result: PhaseResult) -> None:
        """å¤„ç†çŠ¶æ€ä¼ é€’"""
        try:
            # æ‰§è¡ŒçŠ¶æ€ä¼ é€’Hook
            self._execute_hooks("state_transfer",
                              source_phase=phase_config.name,
                              phase_result=phase_result)
            
            # è¿™é‡Œå¯ä»¥ä¿å­˜çŠ¶æ€åˆ°æŸä¸ªçŠ¶æ€ç®¡ç†å™¨
            # ä½†ç”±äºæˆ‘ä»¬ä½¿ç”¨å¤–éƒ¨çŠ¶æ€ç®¡ç†å™¨ï¼Œæ‰€ä»¥ç®€åŒ–å¤„ç†
            
        except Exception as e:
            self.logger.error(f"çŠ¶æ€ä¼ é€’å¤±è´¥ for phase {phase_config.name}: {e}")
    
    def _group_phases_by_priority(self) -> Dict[int, List[PhaseConfig]]:
        """æŒ‰ä¼˜å…ˆçº§åˆ†ç»„é˜¶æ®µ"""
        priority_groups = {}
        for phase in self.training_plan.phases:
            priority = phase.priority
            if priority not in priority_groups:
                priority_groups[priority] = []
            priority_groups[priority].append(phase)
        
        return dict(sorted(priority_groups.items()))
    
    def _parse_training_plan(self) -> None:
        """è§£æè®­ç»ƒè®¡åˆ’é…ç½®"""
        try:
            if not self.config_manager:
                raise TrainingEngineError("Configuration manager not initialized")
                
            plan_config = self.config_manager.get_training_plan()
            self.logger.debug(f"è·å–åˆ°è®­ç»ƒè®¡åˆ’é…ç½®: {plan_config}")
            
            if not plan_config:
                self.logger.error("æœªæ‰¾åˆ°è®­ç»ƒè®¡åˆ’é…ç½®")
                raise TrainingEngineError("No training plan configuration found")
            
            phases = []
            phase_configs = plan_config.get("phases", [])
            self.logger.debug(f"è®­ç»ƒè®¡åˆ’åŒ…å« {len(phase_configs)} ä¸ªé˜¶æ®µé…ç½®")
            
            for i, phase_dict in enumerate(phase_configs):
                self.logger.debug(f"è§£æé˜¶æ®µ {i+1}: {phase_dict}")
                phase = PhaseConfig(
                    name=phase_dict["name"],
                    description=phase_dict.get("description", ""),
                    epochs=phase_dict["epochs"],
                    learner_id=phase_dict["learner"],
                    scheduler_id=phase_dict.get("scheduler"),  # ä½¿scheduleræˆä¸ºå¯é€‰
                    inherit_from=phase_dict.get("inherit_from"),
                    priority=phase_dict.get("priority", 0),
                    execution_mode=phase_dict.get("execution_mode", "sequential")
                )
                phases.append(phase)
            
            self.training_plan = TrainingPlan(
                total_epochs=plan_config["total_epochs"],
                phases=phases,
                execution_strategy=plan_config.get("execution_strategy", "sequential")
            )
            
            # éªŒè¯è®­ç»ƒè®¡åˆ’
            self.training_plan.validate()
            self.logger.debug(f"è®­ç»ƒè®¡åˆ’è§£æå®Œæˆ: {len(phases)} ä¸ªé˜¶æ®µ, {self.training_plan.total_epochs} ä¸ªepoch")
            
        except Exception as e:
            self.logger.error(f"è§£æè®­ç»ƒè®¡åˆ’å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    
    def _initialize_dataloaders(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰DataLoader"""
        try:
            if not self.config_manager:
                raise TrainingEngineError("Configuration manager not initialized")
                
            self.logger.debug("åˆå§‹åŒ–dataloaders...")
            
            dataloader_configs = self.config_manager.get_dataloader_configs()
            if not dataloader_configs:
                self.logger.warning("æ²¡æœ‰dataloaderé…ç½®")
                return
            
            for dataloader_id, config in dataloader_configs.items():
                try:
                    dataloader = self._create_dataloader(dataloader_id, config)
                    self.dataloaders[dataloader_id] = dataloader
                    self.logger.debug(f"åˆ›å»ºdataloader: {dataloader_id}")
                except Exception as e:
                    self.logger.error(f"åˆ›å»ºdataloaderå¤±è´¥ {dataloader_id}: {e}")
            
            self.logger.debug(f"åˆå§‹åŒ– {len(self.dataloaders)} ä¸ªdataloaders")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–dataloaderså¤±è´¥: {e}")
            raise
    
    def _initialize_learners(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰Learner"""
        try:
            if not self.config_manager:
                raise TrainingEngineError("Configuration manager not initialized")
                
            self.logger.debug("åˆå§‹åŒ–learners...")
            
            learner_configs = self.config_manager.get_learner_configs()
            if not learner_configs:
                self.logger.warning("æ²¡æœ‰learneré…ç½®")
                return
            
            for learner_id, config in learner_configs.items():
                try:
                    learner = self._create_learner(learner_id, config)
                    self.learners[learner_id] = learner
                    self.logger.debug(f"åˆ›å»ºlearner: {learner_id}")
                except Exception as e:
                    self.logger.error(f"åˆ›å»ºlearnerå¤±è´¥ {learner_id}: {e}")
            
            self.logger.debug(f"åˆå§‹åŒ– {len(self.learners)} ä¸ªlearners")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–learnerså¤±è´¥: {e}")
            raise
    
    def _initialize_schedulers(self) -> None:
        """åˆå§‹åŒ–è°ƒåº¦å™¨ç®¡ç†å™¨ï¼ˆç”¨äºepochçº§åˆ«çš„è®­ç»ƒè°ƒåº¦ï¼Œä¸æ˜¯PyTorch LR schedulerï¼‰"""
        try:
            self.logger.debug("åˆå§‹åŒ–epochè°ƒåº¦å™¨ç®¡ç†å™¨...")
            
            # å¯¼å…¥EpochSchedulerç³»ç»Ÿ
            from .epoch_scheduler import SchedulerManager, StandardEpochScheduler, SchedulerPriority
            
            # åˆ›å»ºSchedulerManagerï¼Œä½†ä¸åŠ è½½PyTorch LR scheduleré…ç½®
            # PyTorch LR schedulerç”±LocalTrainerå•ç‹¬ç®¡ç†
            self.scheduler_manager = SchedulerManager({})
            
            # åˆ›å»ºé»˜è®¤çš„epochè°ƒåº¦å™¨
            default_scheduler = StandardEpochScheduler("default_scheduler", {}, SchedulerPriority.NORMAL)
            self.scheduler_manager.register_scheduler(default_scheduler)
            
            self.logger.debug("åˆå§‹åŒ–epochè°ƒåº¦å™¨ç®¡ç†å™¨å®Œæˆï¼Œä½¿ç”¨é»˜è®¤epochè°ƒåº¦å™¨")
            self.logger.debug("æ³¨æ„ï¼šPyTorchå­¦ä¹ ç‡è°ƒåº¦å™¨ç”±LocalTrainerå•ç‹¬ç®¡ç†ï¼Œä¸åœ¨æ­¤å¤„åˆå§‹åŒ–")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–epochè°ƒåº¦å™¨ç®¡ç†å™¨å¤±è´¥: {e}")
            raise
    
    def _initialize_evaluation_components(self) -> None:
        """åˆå§‹åŒ–è¯„ä¼°ç»„ä»¶ï¼ˆæµ‹è¯•æ•°æ®é›†å’Œè¯„ä¼°å™¨ï¼‰"""
        try:
            if not self.config_manager:
                raise TrainingEngineError("Configuration manager not initialized")
                
            self.logger.debug("åˆå§‹åŒ–è¯„ä¼°ç»„ä»¶...")
            
            # åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†
            self._initialize_test_dataloaders()
            
            # åˆå§‹åŒ–è¯„ä¼°å™¨
            self._initialize_evaluators()
            
            # åˆå§‹åŒ–è¯„ä¼°å¼•æ“
            self._initialize_evaluation_engine()
            
            test_data_count = len(self.test_dataloaders)
            evaluator_count = len(self.evaluators)
            self.logger.debug(f"åˆå§‹åŒ–è¯„ä¼°ç»„ä»¶å®Œæˆ: {test_data_count} ä¸ªæµ‹è¯•æ•°æ®é›†, {evaluator_count} ä¸ªè¯„ä¼°å™¨")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–è¯„ä¼°ç»„ä»¶å¤±è´¥: {e}")
            # è¯„ä¼°ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ä¸åº”è¯¥ä¸­æ–­è®­ç»ƒ
            self.logger.warning("è¯„ä¼°ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç»§ç»­è®­ç»ƒä½†ä¸è¿›è¡Œè¯„ä¼°")
    
    def _initialize_test_dataloaders(self) -> None:
        """åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†"""
        try:
            self.logger.debug("å¼€å§‹åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†...")
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—æ£€æŸ¥config_manager
            if not hasattr(self, 'config_manager') or self.config_manager is None:
                self.logger.warning("config_manager æœªåˆå§‹åŒ–æˆ–ä¸ºç©º")
                return
                
            self.logger.debug(f"config_manager ç±»å‹: {type(self.config_manager)}")
            
            test_data_configs = self.config_manager.get_test_data_configs()
            self.logger.debug(f"è·å–åˆ°çš„æµ‹è¯•æ•°æ®é…ç½®: {test_data_configs}")
            
            if not test_data_configs:
                self.logger.debug("æ²¡æœ‰æµ‹è¯•æ•°æ®é›†é…ç½®")
                return
            
            self.logger.info(f"å‘ç° {len(test_data_configs)} ä¸ªæµ‹è¯•æ•°æ®é›†é…ç½®")
            
            for test_data_id, config in test_data_configs.items():
                try:
                    self.logger.debug(f"æ­£åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®é›†: {test_data_id}, é…ç½®: {config}")
                    # å¤ç”¨ç°æœ‰çš„dataloaderåˆ›å»ºé€»è¾‘
                    test_dataloader = self._create_dataloader(test_data_id, config)
                    self.test_dataloaders[test_data_id] = test_dataloader
                    self.logger.info(f"âœ… æˆåŠŸåˆ›å»ºæµ‹è¯•æ•°æ®é›†: {test_data_id}")
                except Exception as e:
                    self.logger.error(f"âŒ åˆ›å»ºæµ‹è¯•æ•°æ®é›†å¤±è´¥ {test_data_id}: {e}")
            
            self.logger.info(f"æµ‹è¯•æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…±åˆ›å»º {len(self.test_dataloaders)} ä¸ª")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–æµ‹è¯•æ•°æ®é›†å¤±è´¥: {e}")
            raise
    
    def _initialize_evaluators(self) -> None:
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        try:
            self.logger.debug("å¼€å§‹åˆå§‹åŒ–è¯„ä¼°å™¨...")
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—æ£€æŸ¥config_manager
            if not hasattr(self, 'config_manager') or self.config_manager is None:
                self.logger.warning("config_manager æœªåˆå§‹åŒ–æˆ–ä¸ºç©º")
                return
                
            evaluator_configs = self.config_manager.get_evaluator_configs()
            self.logger.debug(f"è·å–åˆ°çš„è¯„ä¼°å™¨é…ç½®: {evaluator_configs}")
            
            if not evaluator_configs:
                self.logger.debug("æ²¡æœ‰è¯„ä¼°å™¨é…ç½®")
                return
            
            self.logger.info(f"å‘ç° {len(evaluator_configs)} ä¸ªè¯„ä¼°å™¨é…ç½®")
            
            for evaluator_id, config in evaluator_configs.items():
                try:
                    self.logger.debug(f"æ­£åœ¨åˆ›å»ºè¯„ä¼°å™¨: {evaluator_id}, é…ç½®: {config}")
                    evaluator = self._create_evaluator(evaluator_id, config)
                    self.evaluators[evaluator_id] = evaluator
                    self.logger.info(f"âœ… æˆåŠŸåˆ›å»ºè¯„ä¼°å™¨: {evaluator_id}")
                except Exception as e:
                    self.logger.error(f"âŒ åˆ›å»ºè¯„ä¼°å™¨å¤±è´¥ {evaluator_id}: {e}")
            
            self.logger.info(f"è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…±åˆ›å»º {len(self.evaluators)} ä¸ª")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–è¯„ä¼°å™¨å¤±è´¥: {e}")
            raise
    
    def _initialize_evaluation_engine(self) -> None:
        """åˆå§‹åŒ–è¯„ä¼°å¼•æ“"""
        try:
            self.logger.debug("å¼€å§‹åˆå§‹åŒ–è¯„ä¼°å¼•æ“...")
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—æ£€æŸ¥config_manager
            if not hasattr(self, 'config_manager') or self.config_manager is None:
                self.logger.warning("config_manager æœªåˆå§‹åŒ–æˆ–ä¸ºç©º")
                return
            
            # ä¿®å¤æ–¹æ³•åç§° - åº”è¯¥æ˜¯get_evaluation_configè€Œä¸æ˜¯get_evaluation_configs
            evaluation_config = self.config_manager.get_evaluation_config()
            self.logger.debug(f"è·å–åˆ°çš„è¯„ä¼°é…ç½®: {evaluation_config}")
            
            if not evaluation_config:
                self.logger.debug("æ²¡æœ‰è¯„ä¼°ä»»åŠ¡é…ç½®")
                return
            
            # å¯¼å…¥è¯„ä¼°å¼•æ“
            from .evaluation_engine import EvaluationEngine
            
            self.evaluation_engine = EvaluationEngine(
                context=self.context,
                config=evaluation_config
            )
            
            self.logger.info("âœ… è¯„ä¼°å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–è¯„ä¼°å¼•æ“å¤±è´¥: {e}")
            self.evaluation_engine = None
    
    def _create_evaluator(self, evaluator_id: str, config: Dict[str, Any]) -> Any:
        """åˆ›å»ºè¯„ä¼°å™¨"""
        try:
            # ä»registryå¯¼å…¥ç»„ä»¶æ³¨å†Œç³»ç»Ÿ
            from ..registry.component_registry import registry
            
            # æ”¯æŒåµŒå¥—é…ç½®ç»“æ„ï¼ševaluator.class æˆ–ç›´æ¥çš„ type å­—æ®µ
            if 'evaluator' in config and isinstance(config['evaluator'], dict):
                evaluator_config = config['evaluator']
                evaluator_class_name = evaluator_config.get('class', 'accuracy')
            else:
                evaluator_class_name = config.get('type', 'accuracy')
            
            self.logger.debug(f"Creating evaluator '{evaluator_id}' with type '{evaluator_class_name}'")
            
            # å°è¯•ä»æ³¨å†Œè¡¨è·å–evaluatorç±»
            try:
                evaluator_class = registry.get_component('evaluator', evaluator_class_name)
            except Exception as e:
                self.logger.warning(f"Failed to get evaluator class '{evaluator_class_name}': {e}")
                # åˆ›å»ºé»˜è®¤è¯„ä¼°å™¨
                return self._create_fallback_evaluator(evaluator_id, config)
            
            # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹ - ä¼ é€’æ­£ç¡®çš„å‚æ•°ç±»å‹
            from omegaconf import DictConfig
            from ..core.execution_context import ExecutionContext
            
            # ä¸ºè¯„ä¼°å™¨åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            evaluator_context = ExecutionContext(
                config=DictConfig({}),  # ç©ºé…ç½®
                experiment_id=getattr(self, 'experiment_id', 'default_experiment')
            )
            
            # å°†configè½¬æ¢ä¸ºDictConfig
            if isinstance(config, dict):
                evaluator_config = DictConfig(config)
            else:
                evaluator_config = config
                
            evaluator = evaluator_class(evaluator_context, evaluator_config)
            
            self.logger.debug(f"Successfully created evaluator '{evaluator_id}' of type '{evaluator_class.__name__}'")
            return evaluator
            
        except Exception as e:
            self.logger.error(f"Failed to create evaluator '{evaluator_id}': {e}")
            return self._create_fallback_evaluator(evaluator_id, config)
    
    def _create_fallback_evaluator(self, evaluator_id: str, config: Dict[str, Any]) -> Any:
        """åˆ›å»ºå›é€€è¯„ä¼°å™¨"""
        self.logger.warning(f"Creating fallback evaluator for '{evaluator_id}'")
        
        class FallbackEvaluator:
            def __init__(self, evaluator_id: str, config: Dict[str, Any]):
                self.evaluator_id = evaluator_id
                self.config = config
                self.metrics = ["accuracy", "loss"]
            
            def evaluate(self, model, dataloader, **kwargs) -> Dict[str, float]:
                """ç®€å•çš„è¯„ä¼°å®ç°"""
                import random
                return {
                    "accuracy": random.uniform(0.7, 0.95),
                    "loss": random.uniform(0.1, 0.5),
                    "samples": len(dataloader) * 32 if hasattr(dataloader, '__len__') else 1000
                }
            
            def get_metrics(self) -> List[str]:
                return self.metrics
        
        return FallbackEvaluator(evaluator_id, config)
    
    def evaluate_learner(self, learner_id: str, learner: Any) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªå­¦ä¹ å™¨
        
        Args:
            learner_id: å­¦ä¹ å™¨ID
            learner: å­¦ä¹ å™¨å®ä¾‹
            
        Returns:
            Dict[str, Any]: è¯„ä¼°ç»“æœ
        """
        try:
            if not self.evaluation_engine:
                self.logger.warning("è¯„ä¼°å¼•æ“æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯„ä¼°")
                return {}
            
            self.logger.info(f"ğŸ” [è®­ç»ƒåè¯„ä¼°] å¼€å§‹è¯„ä¼° learner {learner_id} (è®­ç»ƒé˜¶æ®µå®Œæˆå)")
            
            evaluation_config = self.config_manager.get_evaluation_config()
            self.logger.debug(f"è¯„ä¼°é…ç½®ç»“æ„: {evaluation_config}")
            
            # æ”¯æŒæ–°æ ¼å¼: evaluation.tasks åˆ—è¡¨
            evaluation_tasks = []
            
            if "tasks" in evaluation_config:
                # è¿‡æ»¤å‡ºå½“å‰learnerçš„ä»»åŠ¡
                all_tasks = evaluation_config["tasks"]
                learner_tasks = [task for task in all_tasks if task.get("learner") == learner_id]
                
                if learner_tasks:
                    self.logger.debug(f"æ‰¾åˆ° {len(learner_tasks)} ä¸ªé’ˆå¯¹learner {learner_id} çš„è¯„ä¼°ä»»åŠ¡")
                    for task in learner_tasks:
                        evaluation_tasks.append({
                            "evaluator": task.get("evaluator"),
                            "test_dataset": task.get("test_data"),  # æ³¨æ„å­—æ®µåæ˜ å°„
                            "name": f"{task.get('evaluator')}_{task.get('test_data')}"
                        })
                else:
                    self.logger.debug(f"åœ¨tasksåˆ—è¡¨ä¸­æœªæ‰¾åˆ°é’ˆå¯¹learner {learner_id} çš„è¯„ä¼°ä»»åŠ¡")
            
            if not evaluation_tasks:
                self.logger.debug(f"æ²¡æœ‰ä¸ºlearner {learner_id} æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ä»»åŠ¡")
                return {}
                
            results = {}
            
            # æ‰§è¡Œè®­ç»ƒåè¯„ä¼°ä»»åŠ¡
            for task in evaluation_tasks:
                evaluator_id = task["evaluator"]
                test_dataset_id = task["test_dataset"]
                task_name = task.get("name", f"{evaluator_id}_{test_dataset_id}")
                
                self.logger.debug(f"ğŸ” [è®­ç»ƒåè¯„ä¼°] æ‰§è¡Œè¯„ä¼°ä»»åŠ¡: {task_name}")
                
                if evaluator_id not in self.evaluators:
                    self.logger.warning(f"è¯„ä¼°å™¨ {evaluator_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä»»åŠ¡ {task_name}")
                    continue
                    
                if test_dataset_id not in self.test_dataloaders:
                    self.logger.warning(f"æµ‹è¯•æ•°æ®é›† {test_dataset_id} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä»»åŠ¡ {task_name}")
                    continue
                
                evaluator = self.evaluators[evaluator_id]
                test_dataloader = self.test_dataloaders[test_dataset_id]
                
                try:
                    # æ‰§è¡Œè®­ç»ƒåè¯„ä¼°
                    eval_result = evaluator.evaluate(
                        model=learner.get_model() if hasattr(learner, 'get_model') else learner,
                        data_loader=test_dataloader,
                        task_id=hash(task_name) % 1000  # ç”Ÿæˆä¸€ä¸ªç®€å•çš„task_id
                    )
                    
                    # ä¿å­˜ç»“æœ
                    results[task_name] = eval_result
                    
                    self.logger.info(f"âœ… [è®­ç»ƒåè¯„ä¼°] è¯„ä¼°ä»»åŠ¡å®Œæˆ: {task_name} - {eval_result}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ [è®­ç»ƒåè¯„ä¼°] è¯„ä¼°ä»»åŠ¡ {task_name} æ‰§è¡Œå¤±è´¥: {e}")
                    results[task_name] = {"error": str(e)}
            
            if results:
                self.logger.info(f"ğŸ¯ [è®­ç»ƒåè¯„ä¼°] learner {learner_id} è¯„ä¼°å®Œæˆï¼Œå…±æ‰§è¡Œ {len(results)} ä¸ªä»»åŠ¡")
            return results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°learner {learner_id} å¤±è´¥: {e}")
            return {}
    def evaluate_all_learners(self) -> Dict[str, Dict[str, Any]]:
        """è¯„ä¼°æ‰€æœ‰learner"""
        try:
            all_results = {}
            
            for learner_id, learner in self.learners.items():
                learner_results = self.evaluate_learner(learner_id, learner)
                if learner_results:
                    all_results[learner_id] = learner_results
            
            self.logger.debug(f"æ‰€æœ‰learnerè¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(all_results)} ä¸ªlearner")
            return all_results
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°æ‰€æœ‰learnerå¤±è´¥: {e}")
            return {}
    
    def _create_learner(self, learner_id: str, config: Dict[str, Any]) -> Any:
        """æ ¹æ®é…ç½®åˆ›å»ºçœŸå®çš„learnerå®ä¾‹"""
        try:
            # ä»registryå¯¼å…¥ç»„ä»¶æ³¨å†Œç³»ç»Ÿ
            from ..registry.component_registry import registry
            from ..registry.component_composer import ComponentComposer
            from omegaconf import DictConfig
            
            # è·å–learnerç±»åï¼Œé»˜è®¤ä½¿ç”¨default learner
            learner_class_name = config.get('class', 'default')
            
            self.logger.debug(f"Creating learner '{learner_id}' with class '{learner_class_name}'")
            
            # å°è¯•ä»æ³¨å†Œè¡¨è·å–learnerç±»
            try:
                learner_class = registry.get_component('learner', learner_class_name)
            except Exception as e:
                self.logger.warning(f"Failed to get learner class '{learner_class_name}': {e}")
                self.logger.info(f"Using default learner for '{learner_id}'")
                learner_class = registry.get_component('learner', 'default')
            
            # å‡†å¤‡learneré…ç½®
            learner_config = DictConfig({
                'learner_id': learner_id,
                'class': learner_class_name,
                'learning_rate': config.get('learning_rate', 0.001),
                'batch_size': config.get('batch_size', 32),
                'local_epochs': config.get('local_epochs', 1),
                'optimizer': config.get('optimizer', {'type': 'Adam', 'lr': 0.001}),
                'default_model_config': {
                    'input_size': config.get('input_size', 784),
                    'num_classes': config.get('num_classes', 10),
                    'hidden_sizes': config.get('hidden_sizes', [256, 128]),
                    'dropout_rate': config.get('dropout_rate', 0.2)
                },
                **config  # åŒ…å«æ‰€æœ‰å…¶ä»–é…ç½®å‚æ•°
            })
            
            # åˆ›å»ºlearnerå®ä¾‹
            learner = learner_class(self.context, learner_config)
            
            self.logger.debug(f"Successfully created learner '{learner_id}' of type '{learner_class.__name__}'")
            return learner
            
        except Exception as e:
            self.logger.error(f"Failed to create learner '{learner_id}': {e}")
            self.logger.error(f"Error details: {traceback.format_exc()}")
            
            # åˆ›å»ºfallback learner
            return self._create_fallback_learner(learner_id, config)
    
    def _create_fallback_learner(self, learner_id: str, config: Dict[str, Any]) -> Any:
        """åˆ›å»ºå›é€€learner"""
        self.logger.warning(f"Creating fallback learner for '{learner_id}'")
        
        class FallbackLearner:
            def __init__(self, learner_id: str, config: Dict[str, Any]):
                self.learner_id = learner_id
                self.config = config
                self.metrics = {"loss": 1.0, "accuracy": 0.0}
                self.epoch_count = 0
                import torch
                self.device = torch.device("cpu")
            
            def train_epoch(self, dataloader, epoch: int, **kwargs) -> Dict[str, float]:
                # ç®€å•çš„è®­ç»ƒæ¨¡æ‹Ÿ
                time.sleep(0.05)  # å‡å°‘æ¨¡æ‹Ÿæ—¶é—´
                self.epoch_count += 1
                
                # æ¨¡æ‹Ÿæ”¶æ•›è¿‡ç¨‹
                self.metrics["loss"] = max(0.1, self.metrics["loss"] * 0.95)
                self.metrics["accuracy"] = min(0.95, self.metrics["accuracy"] + 0.02)
                
                return self.metrics.copy()
            
            def get_model(self):
                import torch.nn as nn
                return nn.Sequential(
                    nn.Linear(784, 128),
                    nn.ReLU(),
                    nn.Linear(128, 10)
                )
            
            def get_state(self) -> Dict[str, Any]:
                return {"learner_id": self.learner_id, "metrics": self.metrics, "epoch_count": self.epoch_count}
            
            def set_state(self, state: Dict[str, Any]) -> None:
                if "metrics" in state:
                    self.metrics.update(state["metrics"])
                if "epoch_count" in state:
                    self.epoch_count = state["epoch_count"]
            
            def to(self, device):
                self.device = device
                return self
            
            def get_device(self):
                return self.device
        
        return FallbackLearner(learner_id, config)
    
    def _create_dataloader(self, dataloader_id: str, config: Dict[str, Any]) -> Any:
        """æ ¹æ®é…ç½®åˆ›å»ºçœŸå®çš„dataloaderå®ä¾‹"""
        try:
            # ä»config_managerå¯¼å…¥DataLoaderFactory
            from ..config.config_manager import DataLoaderFactory, StandardDataLoaderFactory
            
            self.logger.debug(f"Creating dataloader '{dataloader_id}' with config: {config}")
            
            # åˆ›å»ºDataLoaderFactoryå®ä¾‹
            if not hasattr(self, '_dataloader_factory'):
                self._dataloader_factory = DataLoaderFactory({})
                # æ³¨å†Œæ ‡å‡†å·¥å‚
                self._dataloader_factory.register_factory('StandardDataLoader', StandardDataLoaderFactory())
                self._dataloader_factory.register_factory('default', StandardDataLoaderFactory())
            
            # è·å–dataloaderç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨StandardDataLoader
            loader_type = config.get('type', 'StandardDataLoader')
            
            # ä½¿ç”¨å·¥å‚åˆ›å»ºdataloader
            dataloader = self._dataloader_factory.create_dataloader(dataloader_id, config)
            
            self.logger.debug(f"Successfully created dataloader '{dataloader_id}' of type '{loader_type}'")
            return dataloader
            
        except Exception as e:
            self.logger.error(f"Failed to create dataloader '{dataloader_id}': {e}")
            self.logger.error(f"Error details: {traceback.format_exc()}")
            
            # åˆ›å»ºfallback dataloader
            return self._create_fallback_dataloader(dataloader_id, config)
    
    def _create_fallback_dataloader(self, dataloader_id: str, config: Dict[str, Any]) -> Any:
        """åˆ›å»ºå›é€€dataloader"""
        self.logger.warning(f"Creating fallback dataloader for '{dataloader_id}'")
        
        try:
            import torch
            from torch.utils.data import DataLoader, TensorDataset
            
            # åˆ›å»ºç®€å•çš„æ¨¡æ‹Ÿæ•°æ®
            batch_size = config.get('batch_size', 32)
            num_samples = config.get('num_samples', 1000)
            input_size = config.get('input_size', [3, 224, 224])
            num_classes = config.get('num_classes', 10)
            
            # ç¡®ä¿input_sizeæ˜¯åˆ—è¡¨
            if isinstance(input_size, int):
                input_size = [input_size]
            
            data = torch.randn(num_samples, *input_size)
            labels = torch.randint(0, num_classes, (num_samples,))
            dataset = TensorDataset(data, labels)
            
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=config.get('shuffle', True),
                num_workers=config.get('num_workers', 0),
                pin_memory=config.get('pin_memory', False)
            )
            
            self.logger.debug(f"Created fallback dataloader with {num_samples} samples, batch_size={batch_size}")
            return dataloader
            
        except Exception as e:
            self.logger.error(f"Failed to create fallback dataloader: {e}")
            return f"FallbackDataloader-{dataloader_id}"
    
    def _create_hook_executor(self) -> Any:
        """åˆ›å»ºé’©å­æ‰§è¡Œå™¨"""
        class SimpleHookExecutor:
            def __init__(self):
                self.hooks = []
                
            def register_hooks(self, hooks: List[Any]) -> None:
                self.hooks.extend(hooks)
                
            def execute_hooks(self, phase: str, **kwargs) -> None:
                for hook in self.hooks:
                    if hasattr(hook, phase):
                        try:
                            getattr(hook, phase)(**kwargs)
                        except Exception as e:
                            logging.error(f"Hook {hook.__class__.__name__} å¤±è´¥: {e}")
        
        return SimpleHookExecutor()


# å‘åå…¼å®¹çš„åˆ«å
TrainingEngine = RefactoredEnhancedTrainingEngine