# fedcl/engine/epoch_scheduler.py
"""
EpochSchedulerç³»ç»Ÿ - è´Ÿè´£epochçº§åˆ«çš„è®­ç»ƒè°ƒåº¦

åŒ…å«ï¼š
- EpochScheduleråŸºç±»
- å…·ä½“çš„schedulerå®ç°
- ExecutionResultæ•°æ®ç»“æ„
- SchedulerManagerç®¡ç†å™¨
"""

import logging
import time
import threading
from abc import ABC, abstractmethod
from enum import Enum
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, Future

from ..core.execution_context import ExecutionContext
from ..core.hook import HookPhase
from .exceptions import SchedulerError, ExecutionError


class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼æšä¸¾"""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    HYBRID = "hybrid"


class SchedulerPriority(Enum):
    """è°ƒåº¦å™¨ä¼˜å…ˆçº§"""
    CRITICAL = 0
    HIGH = 1
    NORMAL = 2
    LOW = 3
    BACKGROUND = 4


@dataclass
class ExecutionResult:
    """Epochè°ƒåº¦æ‰§è¡Œç»“æœ"""
    scheduler_id: str
    learner_id: str
    executed_epochs: List[int]
    metrics: Dict[str, List[float]]  # æ¯ä¸ªepochçš„æŒ‡æ ‡åˆ—è¡¨
    final_state: Dict[str, Any]
    exported_knowledge: Optional[Dict[str, Any]] = None
    execution_time: float = 0.0
    memory_usage: Dict[str, float] = field(default_factory=dict)
    success: bool = True
    error_message: Optional[str] = None
    
    def get_final_metrics(self) -> Dict[str, float]:
        """è·å–æœ€ç»ˆæŒ‡æ ‡ï¼ˆæœ€åä¸€ä¸ªepochçš„æŒ‡æ ‡ï¼‰"""
        if not self.metrics:
            return {}
        
        final_metrics = {}
        for key, values in self.metrics.items():
            if values:
                final_metrics[key] = values[-1]
        return final_metrics
    
    def get_average_metrics(self) -> Dict[str, float]:
        """è·å–å¹³å‡æŒ‡æ ‡"""
        if not self.metrics:
            return {}
        
        avg_metrics = {}
        for key, values in self.metrics.items():
            if values:
                avg_metrics[f"avg_{key}"] = sum(values) / len(values)
        return avg_metrics


class BaseEpochScheduler(ABC):
    """
    EpochScheduleråŸºç±»
    
    è´Ÿè´£ç®¡ç†ç‰¹å®šlearnerçš„epochçº§è®­ç»ƒè°ƒåº¦é€»è¾‘
    """
    
    def __init__(self, 
                 scheduler_id: str,
                 config: Optional[Dict[str, Any]] = None,
                 priority: SchedulerPriority = SchedulerPriority.NORMAL):
        """
        åˆå§‹åŒ–epochè°ƒåº¦å™¨
        
        Args:
            scheduler_id: è°ƒåº¦å™¨å”¯ä¸€æ ‡è¯†
            config: è°ƒåº¦å™¨é…ç½®
            priority: è°ƒåº¦å™¨ä¼˜å…ˆçº§
        """
        self.scheduler_id = scheduler_id
        self.config = config or {}
        self.priority = priority
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # æ‰§è¡ŒçŠ¶æ€
        self._is_running = False
        self._current_epoch = 0
        self._execution_lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._execution_count = 0
        self._total_execution_time = 0.0
        self._last_execution_time = 0.0
        
        self.logger.debug(f"EpochScheduler '{scheduler_id}' initialized")
    
    @abstractmethod
    def execute_epochs(self,
                      learner: Any,
                      dataloader: Any,
                      epoch_range: List[int],
                      inherited_state: Optional[Dict[str, Any]] = None,
                      context: Optional[ExecutionContext] = None) -> ExecutionResult:
        """
        æ‰§è¡ŒepochèŒƒå›´å†…çš„è®­ç»ƒ
        
        Args:
            learner: å­¦ä¹ å™¨å®ä¾‹
            dataloader: æ•°æ®åŠ è½½å™¨
            epoch_range: è¦æ‰§è¡Œçš„epochåˆ—è¡¨
            inherited_state: ç»§æ‰¿çš„çŠ¶æ€
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ExecutionResult: æ‰§è¡Œç»“æœ
        """
        pass
    
    @abstractmethod
    def get_scheduler_type(self) -> str:
        """è·å–è°ƒåº¦å™¨ç±»å‹"""
        pass
    
    def validate_config(self, config: Dict[str, Any]) -> bool:
        """
        éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§
        
        Args:
            config: é…ç½®å­—å…¸
            
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        # é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™
        return True
    
    def prepare_execution(self, 
                         learner: Any, 
                         dataloader: Any, 
                         inherited_state: Optional[Dict[str, Any]] = None) -> None:
        """
        å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
        
        Args:
            learner: å­¦ä¹ å™¨å®ä¾‹
            dataloader: æ•°æ®åŠ è½½å™¨
            inherited_state: ç»§æ‰¿çŠ¶æ€
        """
        # åº”ç”¨ç»§æ‰¿çŠ¶æ€
        if inherited_state and hasattr(learner, 'set_state'):
            try:
                learner.set_state(inherited_state)
                self.logger.debug(f"Applied inherited state to learner")
            except Exception as e:
                self.logger.warning(f"Failed to apply inherited state: {e}")
    
    def finalize_execution(self, learner: Any) -> Dict[str, Any]:
        """
        å®Œæˆæ‰§è¡Œåçš„æ¸…ç†å·¥ä½œ
        
        Args:
            learner: å­¦ä¹ å™¨å®ä¾‹
            
        Returns:
            Dict[str, Any]: å¯¼å‡ºçš„çŠ¶æ€ï¼ŒåŒ…å«æ¨¡å‹å‚æ•°
        """
        exported_state = {}
        
        # å¯¼å‡ºlearnerçŠ¶æ€
        if hasattr(learner, 'get_state'):
            try:
                exported_state = learner.get_state()
                self.logger.debug(f"Exported learner state")
            except Exception as e:
                self.logger.warning(f"Failed to export learner state: {e}")
        
        # æå–æ¨¡å‹å‚æ•° - è¿™æ˜¯å…³é”®ä¿®å¤
        model_update = {}
        if hasattr(learner, 'model') and hasattr(learner.model, 'state_dict'):
            try:
                # è·å–æ¨¡å‹å‚æ•°
                state_dict = learner.model.state_dict()
                model_update = {k: v.clone().detach() for k, v in state_dict.items()}
                self.logger.debug(f"Successfully extracted model parameters, {len(model_update)} parameters")
                self.logger.debug(f"Model parameter keys: {list(model_update.keys())[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªé”®
            except Exception as e:
                self.logger.warning(f"Failed to extract model parameters: {e}")
                model_update = {}
        elif hasattr(learner, 'get_model'):
            try:
                # å°è¯•é€šè¿‡get_modelæ–¹æ³•è·å–æ¨¡å‹
                model = learner.get_model()
                if model is not None and hasattr(model, 'state_dict'):
                    state_dict = model.state_dict()
                    model_update = {k: v.clone().detach() for k, v in state_dict.items()}
                    self.logger.debug(f"Successfully extracted model parameters via get_model(), {len(model_update)} parameters")
                else:
                    self.logger.warning("get_model() returned None or model has no state_dict")
            except Exception as e:
                self.logger.warning(f"Failed to extract model parameters via get_model(): {e}")
                model_update = {}
        else:
            self.logger.warning("Learner has no model or get_model method, cannot extract model parameters")
        
        # å°†æ¨¡å‹å‚æ•°æ·»åŠ åˆ°å¯¼å‡ºçŠ¶æ€ä¸­
        exported_state['model_update'] = model_update
        
        return exported_state
    
    def handle_epoch_error(self, epoch: int, error: Exception) -> bool:
        """
        å¤„ç†epochæ‰§è¡Œé”™è¯¯
        
        Args:
            epoch: å‡ºé”™çš„epoch
            error: é”™è¯¯ä¿¡æ¯
            
        Returns:
            bool: æ˜¯å¦ç»§ç»­æ‰§è¡Œåç»­epoch
        """
        self.logger.error(f"Epoch {epoch} failed: {error}")
        
        # é»˜è®¤ç­–ç•¥ï¼šè®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œ
        error_tolerance = self.config.get("error_tolerance", "continue")
        
        if error_tolerance == "stop":
            return False
        elif error_tolerance == "retry":
            max_retries = self.config.get("max_retries", 3)
            # è¿™é‡Œå¯ä»¥å®ç°é‡è¯•é€»è¾‘
            return True
        else:  # continue
            return True
    
    def export_state(self) -> Dict[str, Any]:
        """å¯¼å‡ºè°ƒåº¦å™¨çŠ¶æ€"""
        return {
            "scheduler_id": self.scheduler_id,
            "scheduler_type": self.get_scheduler_type(),
            "priority": self.priority.value,
            "execution_count": self._execution_count,
            "total_execution_time": self._total_execution_time,
            "last_execution_time": self._last_execution_time,
            "is_running": self._is_running,
            "current_epoch": self._current_epoch
        }
    
    def import_state(self, state: Dict[str, Any]) -> None:
        """å¯¼å…¥è°ƒåº¦å™¨çŠ¶æ€"""
        self._execution_count = state.get("execution_count", 0)
        self._total_execution_time = state.get("total_execution_time", 0.0)
        self._last_execution_time = state.get("last_execution_time", 0.0)
    
    @property
    def is_running(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return self._is_running
    
    @property
    def current_epoch(self) -> int:
        """è·å–å½“å‰epoch"""
        return self._current_epoch
    
    def get_execution_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        return {
            "execution_count": self._execution_count,
            "total_execution_time": self._total_execution_time,
            "average_execution_time": self._total_execution_time / max(1, self._execution_count),
            "last_execution_time": self._last_execution_time
        }


class StandardEpochScheduler(BaseEpochScheduler):
    """
    æ ‡å‡†Epochè°ƒåº¦å™¨
    
    é€‚ç”¨äºå¸¸è§„çš„è¿ç»­å­¦ä¹ åœºæ™¯
    """
    
    def get_scheduler_type(self) -> str:
        return "StandardEpochScheduler"
    
    def execute_epochs(self,
                      learner: Any,
                      dataloader: Any,
                      epoch_range: List[int],
                      inherited_state: Optional[Dict[str, Any]] = None,
                      context: Optional[ExecutionContext] = None) -> ExecutionResult:
        """æ‰§è¡Œæ ‡å‡†epochè®­ç»ƒ"""
        self.logger.info(f"ğŸš€ [Epochè°ƒåº¦] å¼€å§‹æ‰§è¡Œæ ‡å‡†epochè®­ç»ƒ: epochs={epoch_range}, learner={type(learner).__name__}")
        
        with self._execution_lock:
            if self._is_running:
                raise SchedulerError(f"Scheduler {self.scheduler_id} is already running")
            
            self._is_running = True
            start_time = time.time()
            
            try:
                self.logger.debug(f"Starting standard epoch execution for epochs {epoch_range}")
                
                # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
                self.prepare_execution(learner, dataloader, inherited_state)
                
                # æ‰§è¡Œæ‰€æœ‰epoch
                executed_epochs = []
                metrics_history = {}
                
                for epoch in epoch_range:
                    self.logger.info(f"ğŸ“Š [Epochè®­ç»ƒ] æ‰§è¡Œç¬¬ {epoch} ä¸ªepoch")
                    self._current_epoch = epoch
                    
                    try:
                        # æ‰§è¡Œepochå‰Hook
                        if context:
                            self._execute_hooks(context, HookPhase.BEFORE_EPOCH.value,
                                              epoch=epoch, learner=learner, scheduler_id=self.scheduler_id)
                        
                        # æ‰§è¡Œå•ä¸ªepochè®­ç»ƒ
                        epoch_metrics = self._execute_single_epoch(learner, dataloader, epoch, context)
                        self.logger.info(f"âœ… [Epochè®­ç»ƒ] ç¬¬ {epoch} ä¸ªepochå®Œæˆï¼ŒæŒ‡æ ‡: {epoch_metrics}")
                        
                        # è®°å½•æŒ‡æ ‡
                        for key, value in epoch_metrics.items():
                            if key not in metrics_history:
                                metrics_history[key] = []
                            metrics_history[key].append(value)
                        
                        executed_epochs.append(epoch)
                        
                                                # æ‰§è¡ŒepochåHook
                        if context:
                            # è·å–learnerçš„æ¨¡å‹ç”¨äºcheckpoint
                            model = getattr(learner, 'model', None) if learner else None
                            self.logger.debug(f"About to execute after_epoch hooks with model={type(model).__name__ if model else None}")
                            self._execute_hooks(context, HookPhase.AFTER_EPOCH.value,
                                              epoch=epoch, metrics=epoch_metrics, learner=learner,
                                              scheduler_id=self.scheduler_id, model=model)
                        else:
                            self.logger.debug(f"No context available for after_epoch hooks")
                        
                        self.logger.debug(f"Completed epoch {epoch}: {epoch_metrics}")
                        
                    except Exception as e:
                        should_continue = self.handle_epoch_error(epoch, e)
                        if not should_continue:
                            break
                
                # å®Œæˆæ‰§è¡Œ
                final_state = self.finalize_execution(learner)
                execution_time = time.time() - start_time
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._execution_count += 1
                self._total_execution_time += execution_time
                self._last_execution_time = execution_time
                
                result = ExecutionResult(
                    scheduler_id=self.scheduler_id,
                    learner_id=getattr(learner, 'learner_id', 'unknown'),
                    executed_epochs=executed_epochs,
                    metrics=metrics_history,
                    final_state=final_state,
                    execution_time=execution_time,
                    memory_usage=self._get_memory_usage(),
                    success=True
                )
                
                self.logger.debug(f"Standard epoch execution completed: {len(executed_epochs)}/{len(epoch_range)} epochs")
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                self.logger.error(f"Standard epoch execution failed: {e}")
                
                return ExecutionResult(
                    scheduler_id=self.scheduler_id,
                    learner_id=getattr(learner, 'learner_id', 'unknown'),
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    execution_time=execution_time,
                    success=False,
                    error_message=str(e)
                )
            
            finally:
                self._is_running = False
                self._current_epoch = 0
    
    def _execute_single_epoch(self, 
                             learner: Any, 
                             dataloader: Any, 
                             epoch: int,
                             context: Optional[ExecutionContext] = None) -> Dict[str, float]:
        """æ‰§è¡Œå•ä¸ªepochè®­ç»ƒ"""
        self.logger.info(f"ğŸ”¥ [å•epochè®­ç»ƒ] å¼€å§‹æ‰§è¡Œç¬¬{epoch}ä¸ªepochï¼Œlearnerç±»å‹: {type(learner).__name__}")
        
        if hasattr(learner, 'train_epoch'):
            self.logger.info(f"ğŸ”¥ [å•epochè®­ç»ƒ] ä½¿ç”¨learner.train_epochæ–¹æ³•è®­ç»ƒ")
            result = learner.train_epoch(dataloader, epoch)
            self.logger.info(f"ğŸ”¥ [å•epochè®­ç»ƒ] train_epochå®Œæˆï¼Œç»“æœ: {result}")
            return result
        elif hasattr(learner, 'train_on_batch'):
            self.logger.info(f"ğŸ”¥ [å•epochè®­ç»ƒ] ä½¿ç”¨learner.train_on_batchæ–¹æ³•è®­ç»ƒ")
            return self._execute_batch_training(learner, dataloader, epoch, context)
        else:
            # æ¨¡æ‹Ÿè®­ç»ƒ
            self.logger.warning(f"âš ï¸ [å•epochè®­ç»ƒ] Learner doesn't have train_epoch or train_on_batch methodï¼Œä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ")
            return {
                "loss": max(0.1, 1.0 / (epoch + 1)),
                "accuracy": min(0.9, 0.1 + epoch * 0.1)
            }
    
    def _execute_batch_training(self, 
                               learner: Any, 
                               dataloader: Any, 
                               epoch: int,
                               context: Optional[ExecutionContext] = None) -> Dict[str, float]:
        """æ‰§è¡Œæ‰¹æ¬¡çº§è®­ç»ƒ"""
        epoch_metrics = {"loss": 0.0, "accuracy": 0.0}
        batch_count = 0
        
        for batch_idx, batch_data in enumerate(dataloader):
            # æ‰§è¡Œæ‰¹æ¬¡å‰Hook
            if context:
                self._execute_hooks(context, HookPhase.BEFORE_BATCH.value,
                                  epoch=epoch, batch_idx=batch_idx, batch_data=batch_data,
                                  learner=learner, scheduler_id=self.scheduler_id)
            
            # è®­ç»ƒæ‰¹æ¬¡
            batch_metrics = learner.train_on_batch(batch_data)
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key, value in batch_metrics.items():
                if key in epoch_metrics:
                    epoch_metrics[key] += value
                else:
                    epoch_metrics[key] = value
            
            batch_count += 1
            
            # æ‰§è¡Œæ‰¹æ¬¡åHook
            if context:
                self._execute_hooks(context, HookPhase.AFTER_BATCH.value,
                                  epoch=epoch, batch_idx=batch_idx, batch_metrics=batch_metrics,
                                  learner=learner, scheduler_id=self.scheduler_id)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if batch_count > 0:
            epoch_metrics = {k: v / batch_count for k, v in epoch_metrics.items()}
        
        return epoch_metrics
    
    def _execute_hooks(self, context: ExecutionContext, phase: str, **kwargs):
        """æ‰§è¡ŒHookï¼ˆå¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼‰"""
        self.logger.debug(f"_execute_hooks called with phase={phase}")
        if context and hasattr(context, 'hook_manager'):
            try:
                context.hook_manager.execute_hooks(phase, context, **kwargs)
            except Exception as e:
                self.logger.error(f"Hook execution failed for phase {phase}: {e}")
        
        # ç›´æ¥å¤„ç†CheckpointHookï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        self.logger.debug(f"About to call _execute_checkpoint_hooks")
        self._execute_checkpoint_hooks(context, phase, **kwargs)
    
    def _execute_checkpoint_hooks(self, context: ExecutionContext, phase: str, **kwargs):
        """ç›´æ¥æ‰§è¡ŒCheckpointHookï¼ˆç»•è¿‡hook_managerï¼‰"""
        try:
            self.logger.debug(f"_execute_checkpoint_hooks called with phase={phase}, kwargs keys={list(kwargs.keys())}")
            
            # ä»contextçš„configä¸­è·å–hooksé…ç½®
            if not hasattr(context, 'config') or not context.config:
                self.logger.debug("No config in context, skipping checkpoint hooks")
                return
                
            hooks_config = context.config.get('hooks', {})
            checkpoint_hook_config = hooks_config.get('checkpoint_hook', {})
            
            self.logger.debug(f"checkpoint_hook_config: {checkpoint_hook_config}")
            
            # æ£€æŸ¥hookæ˜¯å¦å¯ç”¨ä»¥åŠphaseæ˜¯å¦åŒ¹é…
            if not checkpoint_hook_config.get('enabled', False):
                self.logger.debug("checkpoint_hook not enabled, skipping")
                return
                
            hook_phase = checkpoint_hook_config.get('phase', '')
            if hook_phase != phase:
                self.logger.debug(f"Phase mismatch: hook_phase={hook_phase}, current_phase={phase}")
                return
            
            # æ„å»ºcheckpointé…ç½®
            from omegaconf import DictConfig
            from fedcl.core.checkpoint_hook import CheckpointHook
            
            checkpoint_config = hooks_config.get('checkpoint', {})
            if not checkpoint_config:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                checkpoint_config = {
                    'save_frequency': checkpoint_hook_config.get('save_frequency', 1),
                    'checkpoint_dir': './checkpoints',
                    'max_checkpoints': 3,
                    'save_model': True,
                    'save_optimizer': False,
                    'save_experiment_state': True,
                    'compress': False
                }
            
            # åˆ›å»ºå¹¶æ‰§è¡ŒCheckpointHook
            self.logger.debug(f"Creating CheckpointHook with config: {checkpoint_config}")
            hook = CheckpointHook(
                phase=phase,
                checkpoint_config=DictConfig(checkpoint_config),
                priority=checkpoint_hook_config.get('priority', 0)
            )
            
            self.logger.debug(f"Checking if CheckpointHook should execute...")
            if hook.should_execute(context, **kwargs):
                self.logger.debug(f"Executing CheckpointHook for phase {phase}")
                hook.execute(context, **kwargs)
                self.logger.debug(f"CheckpointHook execution completed")
            else:
                self.logger.debug(f"CheckpointHook should not execute for phase {phase}")
                
        except Exception as e:
            self.logger.error(f"Failed to execute CheckpointHook for phase {phase}: {e}")
            import traceback
            self.logger.debug(f"CheckpointHook execution traceback: {traceback.format_exc()}")
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            return {
                "rss": memory_info.rss / 1024 / 1024,  # MB
                "vms": memory_info.vms / 1024 / 1024   # MB
            }
        except ImportError:
            return {}
        except Exception:
            return {}


class AdaptiveEpochScheduler(BaseEpochScheduler):
    """
    è‡ªé€‚åº”Epochè°ƒåº¦å™¨
    
    æ ¹æ®è®­ç»ƒè¡¨ç°åŠ¨æ€è°ƒæ•´è®­ç»ƒç­–ç•¥
    """
    
    def get_scheduler_type(self) -> str:
        return "AdaptiveEpochScheduler"
    
    def execute_epochs(self,
                      learner: Any,
                      dataloader: Any,
                      epoch_range: List[int],
                      inherited_state: Optional[Dict[str, Any]] = None,
                      context: Optional[ExecutionContext] = None) -> ExecutionResult:
        """æ‰§è¡Œè‡ªé€‚åº”epochè®­ç»ƒ"""
        with self._execution_lock:
            if self._is_running:
                raise SchedulerError(f"Scheduler {self.scheduler_id} is already running")
            
            self._is_running = True
            start_time = time.time()
            
            try:
                self.logger.info(f"Starting adaptive epoch execution for epochs {epoch_range}")
                
                # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
                self.prepare_execution(learner, dataloader, inherited_state)
                
                # è‡ªé€‚åº”è®­ç»ƒé…ç½®
                patience = self.config.get("early_stopping_patience", 5)
                min_delta = self.config.get("min_improvement_delta", 0.001)
                best_metric_value = float('-inf')
                patience_counter = 0
                
                executed_epochs = []
                metrics_history = {}
                
                for epoch in epoch_range:
                    self._current_epoch = epoch
                    
                    try:
                        # æ‰§è¡Œå•ä¸ªepoch
                        epoch_metrics = self._execute_single_epoch(learner, dataloader, epoch, context)
                        
                        # è®°å½•æŒ‡æ ‡
                        for key, value in epoch_metrics.items():
                            if key not in metrics_history:
                                metrics_history[key] = []
                            metrics_history[key].append(value)
                        
                        executed_epochs.append(epoch)
                        
                        # è‡ªé€‚åº”é€»è¾‘ï¼šæ—©åœæ£€æŸ¥
                        monitor_metric = self.config.get("monitor_metric", "loss")
                        if monitor_metric in epoch_metrics:
                            current_value = epoch_metrics[monitor_metric]
                            
                            # å¯¹äºlossï¼Œå€¼è¶Šå°è¶Šå¥½ï¼›å¯¹äºaccuracyï¼Œå€¼è¶Šå¤§è¶Šå¥½
                            if monitor_metric.lower() in ["loss", "error"]:
                                improvement = best_metric_value - current_value
                                if current_value < best_metric_value:
                                    best_metric_value = current_value
                            else:
                                improvement = current_value - best_metric_value
                                if current_value > best_metric_value:
                                    best_metric_value = current_value
                            
                            if improvement > min_delta:
                                patience_counter = 0
                                self.logger.debug(f"Improvement detected: {improvement:.6f}")
                            else:
                                patience_counter += 1
                                self.logger.debug(f"No improvement for {patience_counter} epochs")
                            
                            # æ—©åœæ£€æŸ¥
                            if patience_counter >= patience:
                                self.logger.info(f"Early stopping triggered after {patience} epochs without improvement")
                                break
                        
                        # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚æœlearneræ”¯æŒï¼‰
                        if hasattr(learner, 'adjust_learning_rate'):
                            learner.adjust_learning_rate(epoch_metrics)
                        
                    except Exception as e:
                        should_continue = self.handle_epoch_error(epoch, e)
                        if not should_continue:
                            break
                
                # å®Œæˆæ‰§è¡Œ
                final_state = self.finalize_execution(learner)
                execution_time = time.time() - start_time
                
                result = ExecutionResult(
                    scheduler_id=self.scheduler_id,
                    learner_id=getattr(learner, 'learner_id', 'unknown'),
                    executed_epochs=executed_epochs,
                    metrics=metrics_history,
                    final_state=final_state,
                    exported_knowledge={"best_metric_value": best_metric_value},
                    execution_time=execution_time,
                    memory_usage=self._get_memory_usage(),
                    success=True
                )
                
                self.logger.debug(f"Adaptive epoch execution completed: {len(executed_epochs)}/{len(epoch_range)} epochs")
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                self.logger.error(f"Adaptive epoch execution failed: {e}")
                
                return ExecutionResult(
                    scheduler_id=self.scheduler_id,
                    learner_id=getattr(learner, 'learner_id', 'unknown'),
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    execution_time=execution_time,
                    success=False,
                    error_message=str(e)
                )
            
            finally:
                self._is_running = False
                self._current_epoch = 0
    
    def _execute_single_epoch(self, 
                             learner: Any, 
                             dataloader: Any, 
                             epoch: int,
                             context: Optional[ExecutionContext] = None) -> Dict[str, float]:
        """æ‰§è¡Œå•ä¸ªepochè®­ç»ƒï¼ˆå¤ç”¨StandardEpochSchedulerçš„å®ç°ï¼‰"""
        if hasattr(learner, 'train_epoch'):
            return learner.train_epoch(dataloader, epoch)
        elif hasattr(learner, 'train_on_batch'):
            return self._execute_batch_training(learner, dataloader, epoch, context)
        else:
            # æ¨¡æ‹Ÿè®­ç»ƒ
            return {
                "loss": max(0.1, 1.0 / (epoch + 1)),
                "accuracy": min(0.9, 0.1 + epoch * 0.1)
            }
    
    def _execute_batch_training(self, learner: Any, dataloader: Any, epoch: int, context: Optional[ExecutionContext] = None) -> Dict[str, float]:
        """æ‰§è¡Œæ‰¹æ¬¡çº§è®­ç»ƒï¼ˆå¤ç”¨StandardEpochSchedulerçš„å®ç°ï¼‰"""
        epoch_metrics = {"loss": 0.0, "accuracy": 0.0}
        batch_count = 0
        
        for batch_idx, batch_data in enumerate(dataloader):
            batch_metrics = learner.train_on_batch(batch_data)
            
            for key, value in batch_metrics.items():
                if key in epoch_metrics:
                    epoch_metrics[key] += value
                else:
                    epoch_metrics[key] = value
            
            batch_count += 1
        
        if batch_count > 0:
            epoch_metrics = {k: v / batch_count for k, v in epoch_metrics.items()}
        
        return epoch_metrics
    
    def _get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            return {
                "rss": memory_info.rss / 1024 / 1024,
                "vms": memory_info.vms / 1024 / 1024
            }
        except ImportError:
            return {}
        except Exception:
            return {}


class SchedulerManager:
    """
    è°ƒåº¦å™¨ç®¡ç†å™¨
    
    è´Ÿè´£ç®¡ç†å’Œåè°ƒå¤šä¸ªEpochSchedulerçš„æ‰§è¡Œ
    """
    
    def __init__(self, scheduler_configs: Dict[str, Dict[str, Any]]):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨ç®¡ç†å™¨
        
        Args:
            scheduler_configs: è°ƒåº¦å™¨é…ç½®å­—å…¸
        """
        self.scheduler_configs = scheduler_configs
        self.schedulers: Dict[str, BaseEpochScheduler] = {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # å¹¶å‘æ§åˆ¶
        self.executor: Optional[ThreadPoolExecutor] = None
        self.running_futures: Dict[str, Future] = {}
        
        self.logger.debug("SchedulerManager initialized")
    
    def create_scheduler(self, scheduler_id: str, config: Dict[str, Any]) -> BaseEpochScheduler:
        """
        åˆ›å»ºè°ƒåº¦å™¨å®ä¾‹
        
        Args:
            scheduler_id: è°ƒåº¦å™¨ID
            config: è°ƒåº¦å™¨é…ç½®
            
        Returns:
            BaseEpochScheduler: è°ƒåº¦å™¨å®ä¾‹
        """
        scheduler_type = config.get("type", "StandardEpochScheduler")
        priority_str = config.get("priority", "NORMAL")
        priority = SchedulerPriority[priority_str.upper()]
        
        # æ˜ å°„PyTorchè°ƒåº¦å™¨ç±»å‹åˆ°å†…éƒ¨ç±»å‹
        if scheduler_type in ["StandardEpochScheduler", "StepLR", "step"]:
            scheduler = StandardEpochScheduler(scheduler_id, config.get("config", {}), priority)
        elif scheduler_type in ["AdaptiveEpochScheduler", "adaptive"]:
            scheduler = AdaptiveEpochScheduler(scheduler_id, config.get("config", {}), priority)
        else:
            # å¯¹äºä¸æ”¯æŒçš„ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤çš„StandardEpochScheduler
            self.logger.warning(f"Unknown scheduler type: {scheduler_type}, using StandardEpochScheduler")
            scheduler = StandardEpochScheduler(scheduler_id, config.get("config", {}), priority)
        
        self.schedulers[scheduler_id] = scheduler
        self.logger.debug(f"Created scheduler '{scheduler_id}' of type '{scheduler_type}'")
        
        return scheduler
    
    def register_scheduler(self, scheduler: BaseEpochScheduler) -> None:
        """
        æ³¨å†Œè°ƒåº¦å™¨å®ä¾‹
        
        Args:
            scheduler: è°ƒåº¦å™¨å®ä¾‹
        """
        self.schedulers[scheduler.scheduler_id] = scheduler
        self.logger.debug(f"Registered scheduler '{scheduler.scheduler_id}' of type '{scheduler.get_scheduler_type()}'")
    
    def get_scheduler(self, scheduler_id: str) -> BaseEpochScheduler:
        """
        è·å–è°ƒåº¦å™¨å®ä¾‹
        
        Args:
            scheduler_id: è°ƒåº¦å™¨ID
            
        Returns:
            BaseEpochScheduler: è°ƒåº¦å™¨å®ä¾‹
        """
        if scheduler_id not in self.schedulers:
            raise SchedulerError(f"Scheduler '{scheduler_id}' not found")
        
        return self.schedulers[scheduler_id]
    
    def execute_parallel(self, 
                        execution_tasks: List[Tuple[str, Any, Any, List[int], Optional[Dict[str, Any]], Optional[ExecutionContext]]],
                        max_workers: int = 2) -> Dict[str, ExecutionResult]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªè°ƒåº¦å™¨
        
        Args:
            execution_tasks: æ‰§è¡Œä»»åŠ¡åˆ—è¡¨ [(scheduler_id, learner, dataloader, epoch_range, inherited_state, context)]
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            Dict[str, ExecutionResult]: æ‰§è¡Œç»“æœå­—å…¸
        """
        if not execution_tasks:
            return {}
        
        self.logger.info(f"Starting parallel execution of {len(execution_tasks)} schedulers")
        
        if not self.executor:
            self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
        sorted_tasks = sorted(execution_tasks, 
                            key=lambda x: self.get_scheduler(x[0]).priority.value)
        
        # æäº¤ä»»åŠ¡
        futures = {}
        for task in sorted_tasks:
            scheduler_id, learner, dataloader, epoch_range, inherited_state, context = task
            scheduler = self.get_scheduler(scheduler_id)
            
            future = self.executor.submit(
                scheduler.execute_epochs,
                learner, dataloader, epoch_range, inherited_state, context
            )
            futures[scheduler_id] = future
            self.running_futures[scheduler_id] = future
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = {}
        for scheduler_id, future in futures.items():
            try:
                result = future.result()
                results[scheduler_id] = result
                self.logger.info(f"Scheduler '{scheduler_id}' æˆåŠŸå®Œæˆ")
            except Exception as e:
                self.logger.error(f"Scheduler '{scheduler_id}' failed: {e}")
                results[scheduler_id] = ExecutionResult(
                    scheduler_id=scheduler_id,
                    learner_id="unknown",
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    success=False,
                    error_message=str(e)
                )
            finally:
                if scheduler_id in self.running_futures:
                    del self.running_futures[scheduler_id]
        
        self.logger.info(f"Parallel execution completed: {len(results)} results")
        return results
    
    def execute_sequential(self,
                          execution_tasks: List[Tuple[str, Any, Any, List[int], Optional[Dict[str, Any]], Optional[ExecutionContext]]]) -> Dict[str, ExecutionResult]:
        """
        é¡ºåºæ‰§è¡Œå¤šä¸ªè°ƒåº¦å™¨
        
        Args:
            execution_tasks: æ‰§è¡Œä»»åŠ¡åˆ—è¡¨
            
        Returns:
            Dict[str, ExecutionResult]: æ‰§è¡Œç»“æœå­—å…¸
        """
        if not execution_tasks:
            return {}
        
        self.logger.info(f"Starting sequential execution of {len(execution_tasks)} schedulers")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
        sorted_tasks = sorted(execution_tasks, 
                            key=lambda x: self.get_scheduler(x[0]).priority.value)
        
        results = {}
        for task in sorted_tasks:
            scheduler_id, learner, dataloader, epoch_range, inherited_state, context = task
            scheduler = self.get_scheduler(scheduler_id)
            
            try:
                self.logger.info(f"Executing scheduler '{scheduler_id}'")
                result = scheduler.execute_epochs(learner, dataloader, epoch_range, inherited_state, context)
                results[scheduler_id] = result
                self.logger.info(f"Scheduler '{scheduler_id}' æˆåŠŸå®Œæˆ")
            except Exception as e:
                self.logger.error(f"Scheduler '{scheduler_id}' failed: {e}")
                results[scheduler_id] = ExecutionResult(
                    scheduler_id=scheduler_id,
                    learner_id="unknown",
                    executed_epochs=[],
                    metrics={},
                    final_state={},
                    success=False,
                    error_message=str(e)
                )
        
        self.logger.info(f"Sequential execution completed: {len(results)} results")
        return results
    
    def get_running_schedulers(self) -> List[str]:
        """è·å–æ­£åœ¨è¿è¡Œçš„è°ƒåº¦å™¨åˆ—è¡¨"""
        running = []
        for scheduler_id, scheduler in self.schedulers.items():
            if scheduler.is_running:
                running.append(scheduler_id)
        return running
    
    def get_scheduler_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰è°ƒåº¦å™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for scheduler_id, scheduler in self.schedulers.items():
            stats[scheduler_id] = {
                "type": scheduler.get_scheduler_type(),
                "priority": scheduler.priority.name,
                "is_running": scheduler.is_running,
                "current_epoch": scheduler.current_epoch,
                "execution_stats": scheduler.get_execution_stats()
            }
        return stats
    
    def cleanup(self) -> None:
        """æ¸…ç†è°ƒåº¦å™¨ç®¡ç†å™¨"""
        try:
            self.logger.info("Cleaning up SchedulerManager")
            
            # å…³é—­å¹¶å‘æ‰§è¡Œå™¨
            if self.executor:
                self.executor.shutdown(wait=True)
                self.executor = None
            
            # æ¸…ç†è°ƒåº¦å™¨
            self.schedulers.clear()
            self.running_futures.clear()
            
            self.logger.debug("SchedulerManager cleanup å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"Error during SchedulerManager cleanup: {e}")