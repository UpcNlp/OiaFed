# fedcl/learners/default_learner.py
"""
é»˜è®¤é€šç”¨å­¦ä¹ å™¨

æä¾›ä¸€ä¸ªå®Œå…¨é€šç”¨çš„å­¦ä¹ å™¨å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•ç‰¹å®šæ¨¡å‹ã€‚
æ¨¡å‹å®Œå…¨é€šè¿‡å¤–éƒ¨é…ç½®ä¼ é€’ï¼ˆauxiliary_modelsæˆ–model_factoryï¼‰ã€‚
"""

import time
from typing import Dict, Any, Optional, List
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from omegaconf import DictConfig
from loguru import logger
from tqdm import tqdm

from ...core.base_learner import BaseLearner
from ...core.execution_context import ExecutionContext
from ...data.results import TaskResults
from ...exceptions import LearnerError
from ...registry.component_registry import registry


@registry.learner("default", 
                  version="1.0.0",
                  author="FedCL Team", 
                  description="Default generic learner that works with any externally provided model",
                  supported_features=["classification", "federated_learning", "continual_learning", "model_agnostic"])
class DefaultLearner(BaseLearner):
    """
    é»˜è®¤é€šç”¨å­¦ä¹ å™¨
    
    å®Œå…¨é€šç”¨çš„å­¦ä¹ å™¨å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•ç‰¹å®šæ¨¡å‹ã€‚
    æ¨¡å‹å®Œå…¨é€šè¿‡å¤–éƒ¨é…ç½®ä¼ é€’ï¼š
    1. é€šè¿‡auxiliary_modelså‚æ•°ä¼ å…¥é¢„åˆ›å»ºçš„æ¨¡å‹
    2. é€šè¿‡model_factoryé…ç½®ä¼ å…¥æ¨¡å‹åˆ›å»ºå‡½æ•°
    3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤æ¨¡å‹
    """
    
    def __init__(self, context: ExecutionContext, config: DictConfig, **kwargs):
        """
        åˆå§‹åŒ–é»˜è®¤å­¦ä¹ å™¨
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            config: å­¦ä¹ å™¨é…ç½®
            **kwargs: é¢å¤–å‚æ•°ï¼Œæ”¯æŒauxiliary_modelsä¼ å…¥é¢„åˆ›å»ºçš„æ¨¡å‹
        """
        # åˆ›å»ºcontext-aware logger
        super().__init__(context, config, **kwargs)
        
        # åŸºç¡€å­¦ä¹ å‚æ•°
        self.learning_rate = config.get('learning_rate', 0.001)
        self.weight_decay = config.get('weight_decay', 1e-4)
        
        # è®­ç»ƒå‚æ•° - æ”¯æŒå¤šç§é…ç½®è·¯å¾„
        # ä¼˜å…ˆè¯»å– training.local_epochsï¼Œç„¶åæ˜¯ epochs_per_task
        training_config = config.get('training', {})
        self.epochs_per_task = training_config.get('local_epochs') or config.get('epochs_per_task', 5)
        
        self.early_stopping_patience = config.get('early_stopping_patience', 10)
        self.min_improvement = config.get('min_improvement', 0.001)
        self.loss_function = config.get('loss_function', 'cross_entropy')
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        if self.model is not None:
            self._initialize_optimizer()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_metric = 0.0
        self.training_history = []
        
        # è¿›åº¦æ¡é…ç½®
        self._progress_position = 0  # è¿›åº¦æ¡æ˜¾ç¤ºä½ç½®ï¼Œç”¨äºå¤šè¿›åº¦æ¡åœºæ™¯
        self._enable_progress_bar = config.get('enable_progress_bar', True)  # æ˜¯å¦å¯ç”¨è¿›åº¦æ¡
        
        # è®°å½•æ¨¡å‹æ¥æº
        self.model_source = self._determine_model_source()
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.logger.debug(f"DefaultLearner initialized (model source: {self.model_source})")
        self.logger.debug(f"Training config: epochs_per_task={self.epochs_per_task}, learning_rate={self.learning_rate}")
        self.logger.debug(f"Raw training config: {training_config}")
        self.logger.debug(f"Raw config: {dict(config) if hasattr(config, 'items') else config}")
    
    
    
    def _determine_model_source(self) -> str:
        """ç¡®å®šæ¨¡å‹æ¥æº"""
        if hasattr(self, '_model_from_kwargs') and self._model_from_kwargs:
            return "direct_model"
        elif hasattr(self, '_model_from_auxiliary') and self._model_from_auxiliary:
            return "auxiliary_models"
        else:
            return "default_fallback"
    
    def set_progress_bar_position(self, position: int):
        """
        è®¾ç½®è¿›åº¦æ¡æ˜¾ç¤ºä½ç½®
        
        åœ¨å¤šå®¢æˆ·ç«¯æˆ–å¤šä»»åŠ¡å¹¶è¡Œè®­ç»ƒåœºæ™¯ä¸­ï¼Œå¯ä»¥è®¾ç½®ä¸åŒçš„ä½ç½®æ¥é¿å…è¿›åº¦æ¡é‡å 
        
        Args:
            position: è¿›åº¦æ¡ä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰
        """
        self._progress_position = position
        self.logger.debug(f"Progress bar position set to {position}")
    
    def enable_progress_bar(self, enable: bool = True):
        """
        å¯ç”¨æˆ–ç¦ç”¨è¿›åº¦æ¡æ˜¾ç¤º
        
        Args:
            enable: æ˜¯å¦å¯ç”¨è¿›åº¦æ¡
        """
        self._enable_progress_bar = enable
        self.logger.debug(f"Progress bar {'enabled' if enable else 'disabled'}")
    
    def _create_default_model(self) -> nn.Module:
        """
        åˆ›å»ºé»˜è®¤å›é€€æ¨¡å‹
        
        å½“æ²¡æœ‰å¤–éƒ¨æä¾›æ¨¡å‹æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„é€šç”¨æ¨¡å‹ä½œä¸ºå›é€€ã€‚
        è¿™ä¸ªæ¨¡å‹ä¼šå°è¯•ä»é…ç½®ä¸­æ¨æ–­åˆé€‚çš„æ¶æ„ã€‚
        
        Returns:
            é»˜è®¤æ¨¡å‹å®ä¾‹
        """
        try:
            self.logger.debug("Creating default fallback model")
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹ç±»å‹
            model_config = self.config.get('model', {})
            if model_config and 'type' in model_config:
                model_type = model_config.get('type')
                self.logger.debug(f"Using configured model type: {model_type}")
                
                # å°è¯•ä½¿ç”¨ModelFactoryï¼ˆæ”¯æŒæ³¨å†Œçš„æ¨¡å‹åç§°ï¼‰
                try:
                    from ..factory import ModelFactory
                    if model_type == "mnist_cnn":
                        # ä½¿ç”¨ModelFactoryåˆ›å»ºæ³¨å†Œçš„CNNæ¨¡å‹
                        model = ModelFactory.create_model(model_config)
                        self.logger.debug(f"Created {model_type} model via ModelFactory")
                        return model
                except Exception as e:
                    self.logger.warning(f"Failed to create model via ModelFactory: {e}, trying direct import")
                
                # å°è¯•å¯¼å…¥å¹¶åˆ›å»ºæŒ‡å®šçš„æ¨¡å‹ç±»å‹ï¼ˆå‘åå…¼å®¹ï¼‰
                try:
                    from ..models.mnist import SimpleMLP, SimpleCNN
                    
                    if model_type in ["SimpleMLP", "mnist_mlp"]:
                        input_size = model_config.get('input_size', 784)
                        hidden_sizes = model_config.get('hidden_sizes', [256, 128])
                        num_classes = model_config.get('num_classes', 10)
                        dropout_rate = model_config.get('dropout_rate', 0.2)
                        activation = model_config.get('activation', 'relu')
                        use_batch_norm = model_config.get('use_batch_norm', False)
                        
                        model = SimpleMLP(
                            input_size=input_size,
                            hidden_sizes=hidden_sizes,
                            num_classes=num_classes,
                            dropout_rate=dropout_rate,
                            activation=activation,
                            use_batch_norm=use_batch_norm
                        )
                        self.logger.debug(f"Created {model_type} model with config: {model_config}")
                        return model
                        
                    elif model_type in ["SimpleCNN", "mnist_cnn"]:
                        # CNN æ¨¡å‹é…ç½®
                        model = SimpleCNN(**{k: v for k, v in model_config.items() if k != 'type'})
                        self.logger.debug(f"Created {model_type} model with config: {model_config}")
                        return model
                        
                except Exception as e:
                    self.logger.warning(f"Failed to create configured model {model_type}: {e}, falling back to Sequential")
            
            # ä»é…ç½®ä¸­è·å–æ¨¡å‹å‚æ•°æç¤º
            default_config = self.config.get('default_model_config', {})
            
            # å°è¯•æ¨æ–­æ¨¡å‹ç±»å‹
            input_size = default_config.get('input_size', 784)
            num_classes = default_config.get('num_classes', 10)
            hidden_sizes = default_config.get('hidden_sizes', [256, 128])
            dropout_rate = default_config.get('dropout_rate', 0.2)
            
            # åˆ›å»ºç®€å•çš„MLPä½œä¸ºé»˜è®¤æ¨¡å‹
            layers = []
            prev_size = input_size
            
            for hidden_size in hidden_sizes:
                layers.append(nn.Linear(prev_size, hidden_size))
                layers.append(nn.ReLU())
                if dropout_rate > 0:
                    layers.append(nn.Dropout(dropout_rate))
                prev_size = hidden_size
            
            layers.append(nn.Linear(prev_size, num_classes))
            
            model = nn.Sequential(*layers)
            
            self.logger.debug(f"Created default MLP model: input={input_size}, hidden={hidden_sizes}, output={num_classes}")
            return model
            
        except Exception as e:
            self.logger.error(f"Failed to create default model: {e}")
            
            # æœ€ç®€å•çš„å›é€€æ¨¡å‹
            return nn.Sequential(
                nn.Flatten(),
                nn.Linear(784, 128),
                nn.ReLU(),
                nn.Linear(128, 10)
            )
    
    def _initialize_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        try:
            optimizer_config = self.config.get('optimizer', {})
            optimizer_type = optimizer_config.get('type', 'Adam').lower()
            
            if optimizer_type == 'adam':
                self.optimizer = optim.Adam(
                    self.model.parameters(),
                    lr=self.learning_rate,
                    weight_decay=self.weight_decay,
                    betas=optimizer_config.get('betas', (0.9, 0.999))
                )
            elif optimizer_type == 'sgd':
                self.optimizer = optim.SGD(
                    self.model.parameters(),
                    lr=self.learning_rate,
                    momentum=optimizer_config.get('momentum', 0.9),
                    weight_decay=self.weight_decay
                )
            elif optimizer_type == 'adamw':
                self.optimizer = optim.AdamW(
                    self.model.parameters(),
                    lr=self.learning_rate,
                    weight_decay=self.weight_decay,
                    betas=optimizer_config.get('betas', (0.9, 0.999))
                )
            else:
                self.logger.warning(f"Unknown optimizer {optimizer_type}, using Adam")
                self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
            
            self.logger.debug(f"Initialized {optimizer_type} optimizer")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize optimizer: {e}")
            raise LearnerError(f"Optimizer initialization failed: {e}")
    
    def _get_loss_function(self):
        """è·å–æŸå¤±å‡½æ•°"""
        loss_functions = {
            'cross_entropy': F.cross_entropy,
            'nll_loss': F.nll_loss,
            'mse': F.mse_loss,
            'l1_loss': F.l1_loss,
            'binary_cross_entropy': F.binary_cross_entropy,
            'binary_cross_entropy_with_logits': F.binary_cross_entropy_with_logits
        }
        
        loss_fn = loss_functions.get(self.loss_function)
        if loss_fn is None:
            self.logger.warning(f"Unknown loss function {self.loss_function}, using cross_entropy")
            return F.cross_entropy
        
        return loss_fn
    
    def train_task(self, task_data: DataLoader) -> TaskResults:
        """
        è®­ç»ƒä»»åŠ¡
        
        Args:
            task_data: ä»»åŠ¡è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            TaskResults: è®­ç»ƒç»“æœ
        """
        try:
            self.logger.info(f"Starting training for task {self.current_task_id} (model source: {self.model_source})")
            start_time = time.time()
            
            if self.model is None:
                raise LearnerError("Model not initialized")
            
            if self.optimizer is None:
                self._initialize_optimizer()
            
            self.model.train()
            loss_fn = self._get_loss_function()
            
            # è®­ç»ƒæŒ‡æ ‡
            epoch_losses = []
            epoch_metrics = []
            best_metric = 0.0
            patience_counter = 0
            
            # åˆ›å»ºepochçº§åˆ«çš„è¿›åº¦æ¡
            if self._enable_progress_bar:
                epoch_progress = tqdm(
                    range(self.epochs_per_task),
                    desc=f"Task {self.current_task_id} Training",
                    unit="epoch",
                    ncols=100,
                    position=max(0, self._progress_position - 1) if self._progress_position > 0 else 0,
                    leave=True,
                    colour='blue'
                )
                epoch_iterator = epoch_progress
            else:
                epoch_iterator = range(self.epochs_per_task)
            print("epoch_iterator",epoch_iterator)
            try:
                # è®­ç»ƒå¾ªç¯
                for epoch in epoch_iterator:
                    self.current_epoch = epoch
                    
                    # æ‰§è¡Œå‰é’©å­
                    self.before_epoch_hook(epoch)
                    
                    epoch_loss, epoch_acc = self._train_epoch(task_data, loss_fn, epoch)
                    
                    epoch_losses.append(epoch_loss)
                    epoch_metrics.append(epoch_acc)
                    
                    # æ—©åœæ£€æŸ¥
                    if epoch_acc > best_metric + self.min_improvement:
                        best_metric = epoch_acc
                        patience_counter = 0
                        self.best_metric = best_metric
                    else:
                        patience_counter += 1
                    
                    # æ‰§è¡Œåé’©å­
                    metrics = {
                        'loss': epoch_loss,
                        'accuracy': epoch_acc,
                        'epoch': epoch
                    }
                    self.after_epoch_hook(epoch, metrics)
                    
                    # æ›´æ–°epochè¿›åº¦æ¡ä¿¡æ¯
                    if self._enable_progress_bar and hasattr(epoch_iterator, 'set_postfix'):
                        epoch_iterator.set_postfix({
                            'Loss': f'{epoch_loss:.4f}',
                            'Acc': f'{epoch_acc:.4f}',
                            'Best': f'{best_metric:.4f}',
                            'Patience': f'{patience_counter}/{self.early_stopping_patience}'
                        })
                    
                    # æ—©åœ
                    if patience_counter >= self.early_stopping_patience:
                        self.logger.info(f"Early stopping at epoch {epoch}")
                        if self._enable_progress_bar and hasattr(epoch_iterator, 'set_description'):
                            epoch_iterator.set_description(f"Task {self.current_task_id} Early Stopped")
                        break
                    
                    self.logger.info(f"Epoch {epoch}: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.4f}")
            
            finally:
                # å…³é—­epochè¿›åº¦æ¡
                if self._enable_progress_bar and hasattr(epoch_iterator, 'close'):
                    epoch_iterator.close()
            
            training_time = time.time() - start_time
            
            # æ„å»ºè®­ç»ƒç»“æœ
            final_metrics = {
                'final_loss': epoch_losses[-1] if epoch_losses else 0.0,
                'final_accuracy': epoch_metrics[-1] if epoch_metrics else 0.0,
                'best_accuracy': self.best_metric,
                'training_time': training_time,
                'epochs_trained': len(epoch_losses)
            }
            
            # æ›´æ–°è®­ç»ƒå†å²
            self.training_history.append({
                'task_id': self.current_task_id,
                'metrics': final_metrics,
                'epoch_losses': epoch_losses,
                'epoch_accuracies': epoch_metrics
            })
            
            # åˆ›å»ºä»»åŠ¡ç»“æœ
            task_results = TaskResults(
                task_id=self.current_task_id,
                metrics=final_metrics,
                training_time=training_time,
                metadata={
                    'learner_type': 'default',
                    'model_source': self.model_source,
                    'epochs_trained': len(epoch_losses),
                    'early_stopped': patience_counter >= self.early_stopping_patience,
                    'model_state': self.get_model_state()  # å°†model_stateæ”¾åˆ°metadataä¸­
                }
            )
            
            self.logger.info(f"Training completed for task {self.current_task_id}")
            self.logger.info(f"Final metrics: {final_metrics}")
            
            return task_results
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}")
            raise LearnerError(f"Training failed: {e}")
    
    def _train_epoch(self, dataloader: DataLoader, loss_fn, epoch: int) -> tuple:
        """
        è®­ç»ƒå•ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            loss_fn: æŸå¤±å‡½æ•°
            epoch: å½“å‰epoch
            
        Returns:
            tuple: (å¹³å‡æŸå¤±, å¹³å‡å‡†ç¡®ç‡)
        """
        print(f"\n=== è®­ç»ƒ Epoch {epoch} ===")
        print(f"DataLoader batch_size: {dataloader.batch_size}")
        print(f"DataLoader dataset size (æ€»æ ·æœ¬æ•°): {len(dataloader.dataset)}")
        print(f"DataLoader total batches (æ€»æ‰¹æ¬¡æ•°): {len(dataloader)}")
        print(f"éªŒè¯: {len(dataloader.dataset)} æ ·æœ¬ Ã· {dataloader.batch_size} batch_size = {len(dataloader.dataset) / dataloader.batch_size:.1f} æ‰¹æ¬¡")
        
        self.model.train()
        
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨è¿›åº¦æ¡
        if self._enable_progress_bar:
            # åˆ›å»ºè¿›åº¦æ¡ï¼Œæ”¯æŒå¤šè¿›åº¦æ¡æ˜¾ç¤º
            progress_bar = tqdm(
                enumerate(dataloader), 
                total=len(dataloader),
                desc=f"Epoch {epoch:3d} [Task {self.current_task_id}]",
                unit="batch",
                ncols=140,  # å¢åŠ è¿›åº¦æ¡å®½åº¦ä»¥æ˜¾ç¤ºæ›´å¤šä¿¡æ¯
                position=self._progress_position,  # æ”¯æŒå¤šè¿›åº¦æ¡ä½ç½®
                leave=True,  # ä¿æŒè¿›åº¦æ¡åœ¨å®Œæˆåæ˜¾ç¤º
                ascii=False,  # ä½¿ç”¨Unicodeå­—ç¬¦
                colour='green'  # è®¾ç½®è¿›åº¦æ¡é¢œè‰²
            )
            data_iterator = progress_bar
        else:
            # ä¸ä½¿ç”¨è¿›åº¦æ¡æ—¶çš„æ™®é€šè¿­ä»£å™¨
            data_iterator = enumerate(dataloader)
        try:
            for batch_idx, (data, target) in data_iterator:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                data = data.to(self.device)
                target = target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = loss_fn(output, target)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct_predictions += pred.eq(target.view_as(pred)).sum().item()
                total_samples += data.size(0)
                
                # è®¡ç®—å½“å‰å‡†ç¡®ç‡å’Œå¹³å‡æŸå¤±
                current_acc = correct_predictions / total_samples
                current_avg_loss = total_loss / (batch_idx + 1)
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°ï¼ˆä»…åœ¨ä½¿ç”¨è¿›åº¦æ¡æ—¶ï¼‰
                if self._enable_progress_bar and hasattr(data_iterator, 'set_postfix'):
                    data_iterator.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Avg Loss': f'{current_avg_loss:.4f}',
                        'Acc': f'{current_acc:.4f}'
                    })
                
                # å®šæœŸæ—¥å¿—
                log_interval = 500 if self._enable_progress_bar else 100
                if batch_idx % log_interval == 0 and batch_idx > 0:
                    self.logger.debug(f"Epoch {epoch}, Batch {batch_idx}: Loss={loss.item():.6f}, Acc={current_acc:.4f}")
        
        finally:
            # ç¡®ä¿è¿›åº¦æ¡æ­£ç¡®å…³é—­ï¼ˆä»…åœ¨ä½¿ç”¨æ—¶ï¼‰
            if self._enable_progress_bar and hasattr(data_iterator, 'close'):
                data_iterator.close()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = correct_predictions / total_samples
        
        return avg_loss, accuracy
    
    def evaluate_task(self, task_data: DataLoader) -> Dict[str, float]:
        """
        è¯„ä¼°ä»»åŠ¡
        
        Args:
            task_data: ä»»åŠ¡è¯„ä¼°æ•°æ®åŠ è½½å™¨
            
        Returns:
            Dict[str, float]: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        try:
            logger.info(f"Starting evaluation (model source: {self.model_source})")
            
            if self.model is None:
                raise LearnerError("Model not initialized")
            
            self.model.eval()
            loss_fn = self._get_loss_function()
            
            total_loss = 0.0
            correct_predictions = 0
            total_samples = 0
            
            with torch.no_grad():
                for data, target in task_data:
                    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                    data = data.to(self.device)
                    target = target.to(self.device)
                    
                    # è‡ªåŠ¨å¤„ç†æ•°æ®å½¢çŠ¶ - SimpleMLPç­‰æ¨¡å‹è‡ªå·±ä¼šå¤„ç†å±•å¹³ï¼Œè·³è¿‡æ‰‹åŠ¨å±•å¹³
                    # if len(data.shape) > 2 and self.model_source == "default_fallback":
                    #     data = data.view(data.size(0), -1)
                    
                    # å‰å‘ä¼ æ’­
                    output = self.model(data)
                    loss = loss_fn(output, target, reduction='sum')
                    
                    # ç»Ÿè®¡
                    total_loss += loss.item()
                    pred = output.argmax(dim=1, keepdim=True)
                    correct_predictions += pred.eq(target.view_as(pred)).sum().item()
                    total_samples += data.size(0)
            
            # è®¡ç®—æŒ‡æ ‡
            avg_loss = total_loss / total_samples
            accuracy = correct_predictions / total_samples
            
            evaluation_metrics = {
                'loss': avg_loss,
                'accuracy': accuracy,
                'correct_predictions': correct_predictions,
                'total_samples': total_samples
            }
            
            logger.info(f"Evaluation completed: Accuracy={accuracy:.4f}, Loss={avg_loss:.4f}")
            
            return evaluation_metrics
            
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            raise LearnerError(f"Evaluation failed: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ¨¡å‹ç›¸å…³ä¿¡æ¯
        """
        base_info = super().get_model_info()
        
        # æ·»åŠ é»˜è®¤å­¦ä¹ å™¨ç‰¹å®šä¿¡æ¯
        default_info = {
            'model_source': self.model_source,
            'loss_function': self.loss_function,
            'current_epoch': self.current_epoch,
            'best_metric': self.best_metric,
            'training_history_length': len(self.training_history),
            'learning_rate': self.learning_rate,
            'weight_decay': self.weight_decay,
            'early_stopping_patience': self.early_stopping_patience
        }
        
        # åˆå¹¶ä¿¡æ¯
        base_info.update(default_info)
        return base_info
    
    def update_model_from_server(self, global_parameters: Dict[str, torch.Tensor]):
        """
        ä»æœåŠ¡ç«¯æ›´æ–°æ¨¡å‹å‚æ•°
        
        Args:
            global_parameters: å…¨å±€æ¨¡å‹å‚æ•°
        """
        try:
            if self.model is None:
                raise LearnerError("Model not initialized")
            
            # åŠ è½½å‚æ•°
            self.model.load_state_dict(global_parameters, strict=False)
            
            logger.info(f"Model updated from server parameters (source: {self.model_source})")
            
        except Exception as e:
            logger.error(f"Failed to update model from server: {e}")
            raise LearnerError(f"Model update failed: {e}")
    
    def reset_for_new_task(self, task_id: int) -> None:
        """
        ä¸ºæ–°ä»»åŠ¡é‡ç½®å­¦ä¹ å™¨
        
        Args:
            task_id: æ–°ä»»åŠ¡çš„ID
        """
        super().reset_for_new_task(task_id)
        
        # é‡ç½®è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é‡ç½®æœ€ä½³æŒ‡æ ‡
        reset_best_metric = self.config.get('reset_best_metric_per_task', False)
        if reset_best_metric:
            self.best_metric = 0.0
        
        logger.info(f"Default learner reset for new task: {task_id} (model source: {self.model_source})")
    
    def save_checkpoint(self, checkpoint_path: str):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        """
        try:
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
                'current_epoch': self.current_epoch,
                'best_metric': self.best_metric,
                'training_history': self.training_history,
                'config': self.config,
                'current_task_id': self.current_task_id,
                'model_source': self.model_source
            }
            
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"Checkpoint saved to {checkpoint_path}")
            
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")
            raise LearnerError(f"Checkpoint save failed: {e}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """
        åŠ è½½æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        """
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            if self.model:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if self.optimizer and 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # åŠ è½½è®­ç»ƒçŠ¶æ€
            self.current_epoch = checkpoint.get('current_epoch', 0)
            self.best_metric = checkpoint.get('best_metric', 0.0)
            self.training_history = checkpoint.get('training_history', [])
            self.current_task_id = checkpoint.get('current_task_id')
            self.model_source = checkpoint.get('model_source', 'unknown')
            
            logger.info(f"Checkpoint loaded from {checkpoint_path}")
            
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            raise LearnerError(f"Checkpoint load failed: {e}")
    
    def get_custom_parameter_selection(self) -> Dict[str, Any]:
        """
        è‡ªå®šä¹‰å‚æ•°é€‰æ‹©ç­–ç•¥ï¼ˆé‡å†™çˆ¶ç±»æ–¹æ³•ï¼‰
        
        æ ¹æ®æ¨¡å‹æ¥æºæä¾›ä¸åŒçš„å‚æ•°é€‰æ‹©ç­–ç•¥
        
        Returns:
            Dict[str, Any]: è‡ªå®šä¹‰é€‰æ‹©çš„å‚æ•°
        """
        if self.model_source == "auxiliary_models":
            # å¦‚æœæ¨¡å‹æ¥è‡ªauxiliary_modelsï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            logger.debug("Using auxiliary model parameter selection")
            return self.model.state_dict()
        elif self.model_source == "direct_model":
            # å¦‚æœæ¨¡å‹ç›´æ¥ä¼ å…¥ï¼Œä½¿ç”¨å…¨éƒ¨å‚æ•°
            logger.debug("Using direct model parameter selection")
            return self.model.state_dict()
        else:
            # é»˜è®¤å›é€€æ¨¡å‹ï¼Œä½¿ç”¨å…¨éƒ¨å‚æ•°
            logger.debug("Using default parameter selection")
            return self.model.state_dict()

    def train_epoch(self, dataloader, epoch: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epochç¼–å·
            
        Returns:
            Dict[str, float]: è®­ç»ƒæŒ‡æ ‡ï¼ˆloss, accuracyç­‰ï¼‰
        """
        self.logger.info(f"ğŸ”¥ [DefaultLearnerè®­ç»ƒ] å¼€å§‹train_epoch - epoch {epoch}")
        
        if self.model is None:
            self.logger.warning("ğŸ”¥ [DefaultLearnerè®­ç»ƒ] No model available for training")
            return {"loss": 0.0, "accuracy": 0.0}
        
        if self.optimizer is None:
            self.logger.warning("ğŸ”¥ [DefaultLearnerè®­ç»ƒ] No optimizer available for training")
            return {"loss": 0.0, "accuracy": 0.0}
        
        try:
            # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
            self.model.train()
            # è·å–æŸå¤±å‡½æ•°
            if hasattr(self, 'criterion') and self.criterion is not None:
                loss_fn = self.criterion
            else:
                # åˆ›å»ºé»˜è®¤æŸå¤±å‡½æ•°
                loss_fn = nn.CrossEntropyLoss()
            
            self.logger.info(f"ğŸ”¥ [DefaultLearnerè®­ç»ƒ] å¼€å§‹è®­ç»ƒepoch {epoch}ï¼Œæ•°æ®é›†å¤§å°: {len(dataloader) if hasattr(dataloader, '__len__') else 'unknown'}")
            # è°ƒç”¨å†…éƒ¨çš„_train_epochæ–¹æ³•
            epoch_loss, epoch_acc = self._train_epoch(dataloader, loss_fn, epoch)
            # æ›´æ–°å½“å‰epoch
            self.current_epoch = epoch
            
            # è®°å½•è®­ç»ƒå†å²
            epoch_metrics = {
                "loss": float(epoch_loss),
                "accuracy": float(epoch_acc),
                "epoch": epoch
            }
            self.training_history.append(epoch_metrics)
            self.logger.info(f"âœ… [DefaultLearnerè®­ç»ƒ] Epoch {epoch} å®Œæˆ - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}")
            return epoch_metrics
            
        except Exception as e:
            self.logger.error(f"âŒ [DefaultLearnerè®­ç»ƒ] è®­ç»ƒepoch {epoch} å¤±è´¥: {e}")
            return {"loss": float('inf'), "accuracy": 0.0, "epoch": epoch}


# ===== ä¾¿åˆ©å‡½æ•° =====

def create_default_learner(context: ExecutionContext, config: DictConfig, 
                          model: nn.Module = None, **kwargs) -> DefaultLearner:
    """
    åˆ›å»ºé»˜è®¤å­¦ä¹ å™¨å®ä¾‹
    
    Args:
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        config: é…ç½®
        model: é¢„åˆ›å»ºçš„æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        **kwargs: é¢å¤–å‚æ•°
        
    Returns:
        é»˜è®¤å­¦ä¹ å™¨å®ä¾‹
    """
    if model is not None:
        kwargs['model'] = model
    
    return DefaultLearner(context, config, **kwargs)


def create_learner_with_auxiliary_model(context: ExecutionContext, config: DictConfig,
                                       model_name: str, model_instance: nn.Module) -> DefaultLearner:
    """
    ä½¿ç”¨è¾…åŠ©æ¨¡å‹åˆ›å»ºå­¦ä¹ å™¨
    
    Args:
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        config: é…ç½®
        model_name: æ¨¡å‹åç§°
        model_instance: æ¨¡å‹å®ä¾‹
        
    Returns:
        å­¦ä¹ å™¨å®ä¾‹
    """
    auxiliary_models = {model_name: model_instance}
    config['model_name'] = model_name
    
    return DefaultLearner(context, config, auxiliary_models=auxiliary_models)


def create_learner_with_progress_config(context: ExecutionContext, config: DictConfig,
                                       progress_position: int = 0, 
                                       enable_progress: bool = True,
                                       **kwargs) -> DefaultLearner:
    """
    åˆ›å»ºå¸¦æœ‰è¿›åº¦æ¡é…ç½®çš„å­¦ä¹ å™¨
    
    åœ¨å¤šå®¢æˆ·ç«¯æˆ–å¤šä»»åŠ¡å¹¶è¡Œè®­ç»ƒåœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨
    
    Args:
        context: æ‰§è¡Œä¸Šä¸‹æ–‡
        config: é…ç½®
        progress_position: è¿›åº¦æ¡æ˜¾ç¤ºä½ç½®ï¼ˆç”¨äºå¤šè¿›åº¦æ¡ï¼‰
        enable_progress: æ˜¯å¦å¯ç”¨è¿›åº¦æ¡
        **kwargs: é¢å¤–å‚æ•°
        
    Returns:
        é…ç½®å¥½è¿›åº¦æ¡çš„å­¦ä¹ å™¨å®ä¾‹
    """
    # åœ¨é…ç½®ä¸­è®¾ç½®è¿›åº¦æ¡é€‰é¡¹
    config = config.copy() if hasattr(config, 'copy') else dict(config)
    config['enable_progress_bar'] = enable_progress
    
    learner = DefaultLearner(context, config, **kwargs)
    learner.set_progress_bar_position(progress_position)
    
    return learner


# ===== ç¤ºä¾‹ä½¿ç”¨ =====

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šéªŒè¯learnerå¯ä»¥æ­£å¸¸åˆ›å»ºå’Œä½¿ç”¨
    from omegaconf import OmegaConf
    from ...core.execution_context import ExecutionContext
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = OmegaConf.create({
        'learning_rate': 0.001,
        'epochs_per_task': 2,
        'loss_function': 'cross_entropy',
        'optimizer': {'type': 'Adam'},
        'default_model_config': {
            'input_size': 784,
            'num_classes': 10,
            'hidden_sizes': [128, 64]
        }
    })
    
    # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
    context = ExecutionContext(
        config=OmegaConf.create({}),
        experiment_id="default_learner_test"
    )
    
    # åˆ›å»ºå­¦ä¹ å™¨
    learner = DefaultLearner(context, config)
    
    print(f"Created learner: {learner}")
    print(f"Model info: {learner.get_model_info()}")
    print("Default learner test completed successfully!")