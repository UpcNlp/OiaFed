"""
SCAFFOLD èšåˆå™¨

å®žçŽ° SCAFFOLD (Stochastic Controlled Averaging for Federated Learning) èšåˆç®—æ³•ã€‚
ä½¿ç”¨æŽ§åˆ¶å˜é‡å‡å°‘å®¢æˆ·ç«¯æ¼‚ç§»ï¼Œæé«˜è”é‚¦å­¦ä¹ çš„æ”¶æ•›é€Ÿåº¦ã€‚

è®ºæ–‡ï¼šSCAFFOLD: Stochastic Controlled Averaging for Federated Learning
ä½œè€…ï¼šSai Praneeth Karimireddy et al.
å‘è¡¨ï¼šICML 2020

ç®—æ³•ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨æŽ§åˆ¶å˜é‡çº æ­£å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„æ›´æ–°åå·®
2. æ›´å¥½çš„æ”¶æ•›ä¿è¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å¼‚æž„æƒ…å†µä¸‹
3. éœ€è¦é¢å¤–å­˜å‚¨å’Œä¼ è¾“æŽ§åˆ¶å˜é‡
4. é€‚åˆæ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§çš„è”é‚¦å­¦ä¹ åœºæ™¯
"""

import torch
from typing import Dict, List, Any, Optional
from loguru import logger

from ...api.decorators import aggregator


@aggregator("scaffold", description="SCAFFOLDæŽ§åˆ¶å˜é‡è”é‚¦èšåˆå™¨")
class SCAFFOLDAggregator:
    """
    SCAFFOLD èšåˆå™¨å®žçŽ°
    
    ç®—æ³•æ ¸å¿ƒï¼š
    1. ç»´æŠ¤å…¨å±€æŽ§åˆ¶å˜é‡ c
    2. æ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤æœ¬åœ°æŽ§åˆ¶å˜é‡ c_i
    3. å®¢æˆ·ç«¯æ›´æ–°è€ƒè™‘æŽ§åˆ¶å˜é‡çš„æ¢¯åº¦ä¿®æ­£
    4. æœåŠ¡å™¨èšåˆæ—¶åŒæ—¶æ›´æ–°æ¨¡åž‹å’ŒæŽ§åˆ¶å˜é‡
    
    å‚æ•°ï¼š
    - learning_rate: å…¨å±€å­¦ä¹ çŽ‡ï¼Œé»˜è®¤1.0
    - control_lr: æŽ§åˆ¶å˜é‡å­¦ä¹ çŽ‡ï¼Œé»˜è®¤Noneï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
    - weighted: æ˜¯å¦æŒ‰æ ·æœ¬æ•°é‡åŠ æƒï¼Œé»˜è®¤True
    - momentum: åŠ¨é‡ç³»æ•°ï¼Œé»˜è®¤0.0
    """
    
    def __init__(self, config: Dict[str, Any] = None, **kwargs):
        """åˆå§‹åŒ–SCAFFOLDèšåˆå™¨"""
        self.config = config or {}
        
        # SCAFFOLDç‰¹å®šå‚æ•°
        self.learning_rate = self.config.get("learning_rate", 1.0)
        self.control_lr = self.config.get("control_lr", None)  # è‡ªåŠ¨è®¡ç®—
        self.weighted = self.config.get("weighted", True)
        self.momentum = self.config.get("momentum", 0.0)
        
        # è®¾å¤‡é…ç½®
        self.device = self.config.get("device", "auto")
        if self.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æŽ§åˆ¶å˜é‡çŠ¶æ€
        self.global_control_variate: Optional[Dict[str, torch.Tensor]] = None
        self.client_control_variates: Dict[str, Dict[str, torch.Tensor]] = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.round_count = 0
        self.control_variate_norm_history = []
        
        # å…¼å®¹å‚æ•°
        self.global_model = kwargs.get("global_model")
        
        logger.info(f"âœ… SCAFFOLDèšåˆå™¨åˆå§‹åŒ–å®Œæˆ - LR: {self.learning_rate}, åŠ¨é‡: {self.momentum}")
    
    def aggregate(self, client_updates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒSCAFFOLDèšåˆ
        
        Args:
            client_updates: å®¢æˆ·ç«¯æ›´æ–°åˆ—è¡¨ï¼Œéœ€åŒ…å«ï¼š
                - model_weights: æ¨¡åž‹æƒé‡
                - control_variate: å®¢æˆ·ç«¯æŽ§åˆ¶å˜é‡ï¼ˆå¯é€‰ï¼‰
                - control_variate_delta: æŽ§åˆ¶å˜é‡æ›´æ–°ï¼ˆå¯é€‰ï¼‰
                - num_samples: æ ·æœ¬æ•°é‡
                - local_epochs: æœ¬åœ°è®­ç»ƒè½®æ•°ï¼ˆç”¨äºŽè®¡ç®—æŽ§åˆ¶å˜é‡å­¦ä¹ çŽ‡ï¼‰
        
        Returns:
            èšåˆç»“æžœï¼ŒåŒ…å«æ›´æ–°çš„æ¨¡åž‹å’ŒæŽ§åˆ¶å˜é‡
        """
        if not client_updates:
            raise ValueError("æ²¡æœ‰å®¢æˆ·ç«¯æ›´æ–°å¯èšåˆ")
        
        self.round_count += 1
        logger.debug(f"ðŸ”„ SCAFFOLDèšåˆè½®æ¬¡ {self.round_count} - {len(client_updates)} ä¸ªå®¢æˆ·ç«¯")
        
        # 1. åˆå§‹åŒ–å…¨å±€æŽ§åˆ¶å˜é‡ï¼ˆå¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡ï¼‰
        if self.global_control_variate is None:
            self._initialize_global_control_variate(client_updates[0])
        
        # 2. è®¡ç®—èšåˆæƒé‡
        weights = self._compute_aggregation_weights(client_updates)
        
        # 3. èšåˆæ¨¡åž‹å‚æ•°
        aggregated_weights = self._aggregate_model_weights(client_updates, weights)
        
        # 4. æ›´æ–°æŽ§åˆ¶å˜é‡
        control_stats = self._update_control_variates(client_updates, weights)
        
        # 5. æž„å»ºç»“æžœ
        total_samples = sum(update.get("num_samples", 0) for update in client_updates)
        
        result = {
            "aggregated_weights": aggregated_weights,
            "global_control_variate": self.global_control_variate.copy(),
            "total_samples": total_samples,
            "num_participants": len(client_updates),
            "aggregation_weights": {
                update.get("client_id", f"client_{i}"): weights[i] 
                for i, update in enumerate(client_updates)
            },
            "algorithm": "SCAFFOLD",
            "round": self.round_count,
            "control_stats": control_stats
        }
        
        logger.debug(f"âœ… SCAFFOLDèšåˆå®Œæˆ - æŽ§åˆ¶å˜é‡èŒƒæ•°: {control_stats.get('global_cv_norm', 0):.6f}")
        return result
    
    def _initialize_global_control_variate(self, sample_update: Dict[str, Any]):
        """åˆå§‹åŒ–å…¨å±€æŽ§åˆ¶å˜é‡"""
        model_weights = sample_update["model_weights"]
        self.global_control_variate = {}
        
        for param_name, param_value in model_weights.items():
            if isinstance(param_value, torch.Tensor):
                self.global_control_variate[param_name] = torch.zeros_like(
                    param_value, device=self.device
                )
            else:
                self.global_control_variate[param_name] = 0.0
        
        logger.debug("ðŸ”§ å…¨å±€æŽ§åˆ¶å˜é‡å·²åˆå§‹åŒ–")
    
    def _compute_aggregation_weights(self, client_updates: List[Dict[str, Any]]) -> List[float]:
        """è®¡ç®—èšåˆæƒé‡"""
        if not self.weighted:
            num_clients = len(client_updates)
            return [1.0 / num_clients] * num_clients
        
        sample_counts = [update.get("num_samples", 1) for update in client_updates]
        total_samples = sum(sample_counts)
        
        if total_samples == 0:
            num_clients = len(client_updates)
            return [1.0 / num_clients] * num_clients
        
        return [count / total_samples for count in sample_counts]
    
    def _aggregate_model_weights(self, client_updates: List[Dict[str, Any]], 
                                weights: List[float]) -> Dict[str, torch.Tensor]:
        """èšåˆæ¨¡åž‹æƒé‡"""
        aggregated_weights = {}
        
        # èŽ·å–å‚æ•°ç»“æž„
        first_weights = client_updates[0]["model_weights"]
        param_names = list(first_weights.keys())
        
        # åˆå§‹åŒ–èšåˆç»“æžœ
        for param_name in param_names:
            param_tensor = first_weights[param_name]
            if isinstance(param_tensor, torch.Tensor):
                aggregated_weights[param_name] = torch.zeros_like(param_tensor, device=self.device)
            else:
                aggregated_weights[param_name] = 0.0
        
        # åŠ æƒèšåˆ
        for i, update in enumerate(client_updates):
            client_weights = update["model_weights"]
            weight = weights[i]
            
            for param_name in param_names:
                if param_name in client_weights:
                    param_value = client_weights[param_name]
                    
                    if isinstance(param_value, torch.Tensor):
                        param_value = param_value.to(self.device)
                        aggregated_weights[param_name] += weight * param_value
                    else:
                        aggregated_weights[param_name] += weight * param_value
        
        return aggregated_weights
    
    def _update_control_variates(self, client_updates: List[Dict[str, Any]], 
                               weights: List[float]) -> Dict[str, float]:
        """æ›´æ–°æŽ§åˆ¶å˜é‡"""
        control_stats = {}
        
        # è®¡ç®—æŽ§åˆ¶å˜é‡å­¦ä¹ çŽ‡
        if self.control_lr is None:
            # è‡ªåŠ¨è®¡ç®—ï¼šåŸºäºŽå¹³å‡æœ¬åœ°epochæ•°
            avg_local_epochs = sum(update.get("local_epochs", 1) for update in client_updates) / len(client_updates)
            effective_lr = self.learning_rate / avg_local_epochs
        else:
            effective_lr = self.control_lr
        
        # æ›´æ–°å…¨å±€æŽ§åˆ¶å˜é‡
        control_variate_deltas = {}
        
        # èšåˆæŽ§åˆ¶å˜é‡å¢žé‡
        for param_name in self.global_control_variate.keys():
            control_variate_deltas[param_name] = torch.zeros_like(
                self.global_control_variate[param_name], device=self.device
            )
        
        for i, update in enumerate(client_updates):
            client_id = update.get("client_id", f"client_{i}")
            weight = weights[i]
            
            # èŽ·å–å®¢æˆ·ç«¯æŽ§åˆ¶å˜é‡å¢žé‡
            if "control_variate_delta" in update:
                cv_delta = update["control_variate_delta"]
                
                for param_name in control_variate_deltas.keys():
                    if param_name in cv_delta:
                        delta_value = cv_delta[param_name]
                        if isinstance(delta_value, torch.Tensor):
                            delta_value = delta_value.to(self.device)
                            control_variate_deltas[param_name] += weight * delta_value
            
            # æ›´æ–°å®¢æˆ·ç«¯æŽ§åˆ¶å˜é‡ç¼“å­˜
            if "control_variate" in update:
                self.client_control_variates[client_id] = update["control_variate"]
        
        # åº”ç”¨æŽ§åˆ¶å˜é‡æ›´æ–°
        global_cv_norm = 0.0
        for param_name, delta in control_variate_deltas.items():
            if isinstance(delta, torch.Tensor):
                # ä½¿ç”¨åŠ¨é‡æ›´æ–°
                if self.momentum > 0:
                    self.global_control_variate[param_name] = (
                        self.momentum * self.global_control_variate[param_name] + 
                        (1 - self.momentum) * effective_lr * delta
                    )
                else:
                    self.global_control_variate[param_name] += effective_lr * delta
                
                # è®¡ç®—èŒƒæ•°
                global_cv_norm += torch.norm(self.global_control_variate[param_name]).item() ** 2
        
        global_cv_norm = global_cv_norm ** 0.5
        self.control_variate_norm_history.append(global_cv_norm)
        
        control_stats = {
            "global_cv_norm": global_cv_norm,
            "effective_control_lr": effective_lr,
            "num_client_cv_updates": len([u for u in client_updates if "control_variate_delta" in u])
        }
        
        return control_stats
    
    def get_client_control_variate(self, client_id: str) -> Optional[Dict[str, torch.Tensor]]:
        """èŽ·å–æŒ‡å®šå®¢æˆ·ç«¯çš„æŽ§åˆ¶å˜é‡"""
        return self.client_control_variates.get(client_id)
    
    def get_control_variate_trend(self) -> List[float]:
        """èŽ·å–æŽ§åˆ¶å˜é‡èŒƒæ•°çš„åŽ†å²è¶‹åŠ¿"""
        return self.control_variate_norm_history.copy()
    
    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–èšåˆå™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "algorithm": "SCAFFOLD",
            "learning_rate": self.learning_rate,
            "control_lr": self.control_lr,
            "momentum": self.momentum,
            "total_rounds": self.round_count,
            "num_registered_clients": len(self.client_control_variates),
            "device": str(self.device)
        }
        
        # æ·»åŠ æŽ§åˆ¶å˜é‡ç»Ÿè®¡
        if self.control_variate_norm_history:
            stats["latest_cv_norm"] = self.control_variate_norm_history[-1]
            stats["avg_cv_norm"] = sum(self.control_variate_norm_history) / len(self.control_variate_norm_history)
            
            if len(self.control_variate_norm_history) > 1:
                trend = "increasing" if (self.control_variate_norm_history[-1] > 
                                       self.control_variate_norm_history[0]) else "decreasing"
                stats["cv_norm_trend"] = trend
        
        return stats
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.round_count = 0
        self.control_variate_norm_history.clear()
        self.client_control_variates.clear()
        self.global_control_variate = None
        logger.info("ðŸ”„ SCAFFOLDèšåˆå™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def __repr__(self) -> str:
        return (f"SCAFFOLDAggregator(lr={self.learning_rate}, momentum={self.momentum}, "
                f"rounds={self.round_count})")