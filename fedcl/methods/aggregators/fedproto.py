"""
FedProto èšåˆå™¨

å®žçŽ° FedProto (Federated Prototypical Learning) èšåˆç®—æ³•ã€‚
é™¤äº†èšåˆæ¨¡åž‹æƒé‡ï¼Œè¿˜éœ€è¦èšåˆå„ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«åŽŸåž‹ï¼ˆprototypesï¼‰ã€‚

è®ºæ–‡ï¼šFedProto: Federated Prototype Learning across Heterogeneous Clients
ä½œè€…ï¼šYue Tan et al.
å‘è¡¨ï¼šAAAI 2022

ç®—æ³•ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨FedAvgèšåˆæ¨¡åž‹æƒé‡
2. èšåˆå®¢æˆ·ç«¯çš„ç±»åˆ«åŽŸåž‹ï¼ˆæŒ‰æ ·æœ¬æ•°åŠ æƒï¼‰
3. åŽŸåž‹ç”¨äºŽå®¢æˆ·ç«¯çš„çŸ¥è¯†è’¸é¦
"""

import torch
from typing import Dict, List, Any
from loguru import logger

from ...api.decorators import aggregator


@aggregator("fedproto", description="FedProtoåŽŸåž‹èšåˆå™¨")
class FedProtoAggregator:
    """
    FedProto èšåˆå™¨å®žçŽ°

    æ‰§è¡Œä¸¤ä¸ªå±‚é¢çš„èšåˆï¼š
    1. æ¨¡åž‹æƒé‡èšåˆï¼šä½¿ç”¨FedAvgåŠ æƒå¹³å‡
    2. åŽŸåž‹èšåˆï¼šèšåˆå„å®¢æˆ·ç«¯çš„ç±»åˆ«åŽŸåž‹

    åŽŸåž‹èšåˆå…¬å¼ï¼š
    proto_global[c] = Î£(n_k * proto_k[c]) / Î£(n_k)

    å…¶ä¸­ï¼š
    - proto_k[c]: å®¢æˆ·ç«¯kå¯¹ç±»åˆ«cçš„åŽŸåž‹
    - n_k: å®¢æˆ·ç«¯kä¸­ç±»åˆ«cçš„æ ·æœ¬æ•°é‡
    - proto_global[c]: ç±»åˆ«cçš„å…¨å±€åŽŸåž‹

    å‚æ•°ï¼š
    - weighted: æ˜¯å¦æŒ‰æ ·æœ¬æ•°é‡åŠ æƒï¼Œé»˜è®¤True
    - device: è®¡ç®—è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹
    """

    def __init__(self, config: Dict[str, Any] = None, **kwargs):
        """åˆå§‹åŒ–FedProtoèšåˆå™¨"""
        self.config = config or {}

        # èšåˆé…ç½®
        self.weighted = self.config.get("weighted", True)
        self.device = self.config.get("device", "auto")

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        if self.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ç»Ÿè®¡ä¿¡æ¯
        self.round_count = 0
        self.total_aggregations = 0

        logger.info(f"âœ… FedProtoèšåˆå™¨åˆå§‹åŒ–å®Œæˆ - åŠ æƒ: {self.weighted}, è®¾å¤‡: {self.device}")

    def aggregate(self, client_updates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒFedProtoèšåˆ

        Args:
            client_updates: å®¢æˆ·ç«¯æ›´æ–°åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - model_weights: æ¨¡åž‹æƒé‡å­—å…¸
                - num_samples: æ ·æœ¬æ•°é‡
                - client_id: å®¢æˆ·ç«¯ID
                - prototypes: æœ¬åœ°åŽŸåž‹å­—å…¸ {class_id: prototype_tensor}

        Returns:
            èšåˆç»“æžœå­—å…¸ï¼ŒåŒ…å«ï¼š
                - aggregated_weights: èšåˆåŽçš„æ¨¡åž‹æƒé‡
                - global_prototypes: èšåˆåŽçš„å…¨å±€åŽŸåž‹
                - total_samples: æ€»æ ·æœ¬æ•°
                - num_participants: å‚ä¸Žå®¢æˆ·ç«¯æ•°é‡
                - aggregation_weights: å„å®¢æˆ·ç«¯æƒé‡
        """
        if not client_updates:
            raise ValueError("æ²¡æœ‰å®¢æˆ·ç«¯æ›´æ–°å¯èšåˆ")

        # éªŒè¯å¿…éœ€å­—æ®µ
        for i, update in enumerate(client_updates):
            if "model_weights" not in update:
                raise ValueError(f"å®¢æˆ·ç«¯æ›´æ–° {i} ç¼ºå°‘å¿…éœ€å­—æ®µ 'model_weights'")
            if "num_samples" not in update:
                raise ValueError(f"å®¢æˆ·ç«¯æ›´æ–° {i} ç¼ºå°‘å¿…éœ€å­—æ®µ 'num_samples'")

        self.round_count += 1
        self.total_aggregations += 1

        logger.debug(f"FedProtoèšåˆè½®æ¬¡ {self.round_count} - {len(client_updates)} ä¸ªå®¢æˆ·ç«¯")

        # 1. è®¡ç®—èšåˆæƒé‡
        weights = self._compute_aggregation_weights(client_updates)

        # 2. èšåˆæ¨¡åž‹æƒé‡ï¼ˆä½¿ç”¨FedAvgï¼‰
        aggregated_weights = self._aggregate_model_weights(client_updates, weights)

        # 3. èšåˆåŽŸåž‹
        global_prototypes = self._aggregate_prototypes(client_updates, weights)

        # 4. è®¡ç®—åŸºç¡€ç»Ÿè®¡
        total_samples = sum(update["num_samples"] for update in client_updates)

        # 5. æž„å»ºç»“æžœ
        result = {
            "aggregated_weights": aggregated_weights,
            "global_prototypes": global_prototypes,  # FedProtoç‰¹æœ‰
            "total_samples": total_samples,
            "num_participants": len(client_updates),
            "aggregation_weights": {
                update.get("client_id", f"client_{i}"): weights[i]
                for i, update in enumerate(client_updates)
            },
            "algorithm": "FedProto",
            "round": self.round_count
        }

        logger.debug(
            f"âœ… FedProtoèšåˆå®Œæˆ - æ€»æ ·æœ¬: {total_samples}, "
            f"å…¨å±€åŽŸåž‹æ•°: {len(global_prototypes)}"
        )
        return result

    def _compute_aggregation_weights(self, client_updates: List[Dict[str, Any]]) -> List[float]:
        """è®¡ç®—èšåˆæƒé‡"""
        if not self.weighted:
            # å‡ç­‰æƒé‡
            num_clients = len(client_updates)
            return [1.0 / num_clients] * num_clients

        # æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ
        sample_counts = [update["num_samples"] for update in client_updates]
        total_samples = sum(sample_counts)

        if total_samples == 0:
            raise ValueError("æ‰€æœ‰å®¢æˆ·ç«¯çš„æ ·æœ¬æ•°éƒ½ä¸º0ï¼Œæ— æ³•è¿›è¡ŒåŠ æƒèšåˆ")

        weights = [count / total_samples for count in sample_counts]
        return weights

    def _aggregate_model_weights(self, client_updates: List[Dict[str, Any]],
                                 weights: List[float]) -> Dict[str, torch.Tensor]:
        """èšåˆæ¨¡åž‹æƒé‡ï¼ˆä½¿ç”¨FedAvgï¼‰"""
        aggregated_weights = {}

        # èŽ·å–å‚æ•°ç»“æž„
        first_update = client_updates[0]
        model_weights = first_update["model_weights"]
        param_names = list(model_weights.keys())

        # åˆå§‹åŒ–èšåˆæƒé‡
        for param_name in param_names:
            param_shape = model_weights[param_name].shape
            aggregated_weights[param_name] = torch.zeros(param_shape, device=self.device)

        # åŠ æƒèšåˆ
        for i, update in enumerate(client_updates):
            client_weights = update["model_weights"]
            client_weight = weights[i]

            for param_name in param_names:
                if param_name not in client_weights:
                    logger.warning(f"å®¢æˆ·ç«¯ {update.get('client_id', i)} ç¼ºå°‘å‚æ•° {param_name}")
                    continue

                # å°†å‚æ•°ç§»åˆ°æ­£ç¡®è®¾å¤‡å¹¶åŠ æƒ
                param_value = client_weights[param_name]
                if isinstance(param_value, torch.Tensor):
                    param_value = param_value.to(self.device)
                    aggregated_weights[param_name] += client_weight * param_value
                else:
                    aggregated_weights[param_name] += client_weight * param_value

        return aggregated_weights

    def _aggregate_prototypes(self, client_updates: List[Dict[str, Any]],
                             weights: List[float]) -> Dict[int, torch.Tensor]:
        """
        èšåˆå®¢æˆ·ç«¯åŽŸåž‹

        åŽŸåž‹èšåˆç­–ç•¥ï¼š
        - å¯¹äºŽæ¯ä¸ªç±»åˆ«ï¼Œæ”¶é›†æ‰€æœ‰æ‹¥æœ‰è¯¥ç±»åˆ«çš„å®¢æˆ·ç«¯çš„åŽŸåž‹
        - æŒ‰ç…§å®¢æˆ·ç«¯æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡
        - å¦‚æžœæŸä¸ªå®¢æˆ·ç«¯æ²¡æœ‰æŸä¸ªç±»åˆ«çš„æ ·æœ¬ï¼Œåˆ™è·³è¿‡è¯¥å®¢æˆ·ç«¯å¯¹è¯¥ç±»åˆ«çš„è´¡çŒ®
        """
        global_prototypes = {}

        # æ”¶é›†æ‰€æœ‰å‡ºçŽ°çš„ç±»åˆ«
        all_classes = set()
        for update in client_updates:
            if "prototypes" in update and update["prototypes"]:
                all_classes.update(update["prototypes"].keys())

        if not all_classes:
            logger.warning("æ²¡æœ‰å®¢æˆ·ç«¯æä¾›åŽŸåž‹ï¼Œè¿”å›žç©ºåŽŸåž‹å­—å…¸")
            return {}

        logger.debug(f"  èšåˆ {len(all_classes)} ä¸ªç±»åˆ«çš„åŽŸåž‹")

        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œèšåˆ
        for class_id in all_classes:
            class_prototypes = []
            class_weights = []

            # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰å®¢æˆ·ç«¯åŽŸåž‹
            for i, update in enumerate(client_updates):
                if "prototypes" not in update or not update["prototypes"]:
                    continue

                prototypes = update["prototypes"]

                # å¦‚æžœè¯¥å®¢æˆ·ç«¯æœ‰è¿™ä¸ªç±»åˆ«çš„åŽŸåž‹
                if class_id in prototypes:
                    proto = prototypes[class_id]

                    # è·³è¿‡é›¶å‘é‡ï¼ˆè¡¨ç¤ºè¯¥å®¢æˆ·ç«¯æ²¡æœ‰è¯¥ç±»åˆ«çš„æ ·æœ¬ï¼‰
                    if isinstance(proto, torch.Tensor):
                        if proto.sum().item() != 0:
                            class_prototypes.append(proto)
                            class_weights.append(weights[i])
                    else:
                        # å¤„ç†numpyæ•°ç»„
                        proto_tensor = torch.tensor(proto)
                        if proto_tensor.sum().item() != 0:
                            class_prototypes.append(proto_tensor)
                            class_weights.append(weights[i])

            # è®¡ç®—è¯¥ç±»åˆ«çš„å…¨å±€åŽŸåž‹ï¼ˆåŠ æƒå¹³å‡ï¼‰
            if class_prototypes:
                # å½’ä¸€åŒ–æƒé‡
                total_weight = sum(class_weights)
                if total_weight > 0:
                    normalized_weights = [w / total_weight for w in class_weights]

                    # åŠ æƒå¹³å‡
                    global_proto = torch.zeros_like(class_prototypes[0])
                    for proto, weight in zip(class_prototypes, normalized_weights):
                        global_proto += weight * proto.to(global_proto.device)

                    global_prototypes[class_id] = global_proto
                    logger.debug(
                        f"    ç±»åˆ« {class_id}: {len(class_prototypes)} ä¸ªå®¢æˆ·ç«¯è´¡çŒ®åŽŸåž‹"
                    )
            else:
                logger.warning(f"    ç±»åˆ« {class_id}: æ²¡æœ‰æœ‰æ•ˆçš„å®¢æˆ·ç«¯åŽŸåž‹")

        return global_prototypes

    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–èšåˆå™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "algorithm": "FedProto",
            "total_rounds": self.round_count,
            "total_aggregations": self.total_aggregations,
            "weighted": self.weighted,
            "device": str(self.device)
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.round_count = 0
        self.total_aggregations = 0
        logger.info("ðŸ”„ FedProtoèšåˆå™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    def __repr__(self) -> str:
        return f"FedProtoAggregator(weighted={self.weighted}, device={self.device}, rounds={self.round_count})"
