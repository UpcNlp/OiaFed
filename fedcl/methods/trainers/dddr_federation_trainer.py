"""
DDDRè”é‚¦è®­ç»ƒå™¨

åŸºäºFedCLæ¡†æ¶å®ç°DDDRçš„å®Œæ•´è”é‚¦è®­ç»ƒæµç¨‹ï¼Œè´Ÿè´£å…¨å±€è”é‚¦é€»è¾‘ã€‚
"""

import os
import copy
import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm, trange
from einops import rearrange
from copy import deepcopy
from typing import Dict, Any, List, Optional
from omegaconf import OmegaConf

from ...fl.server import FLTrainerBase
from ...fl.results import EvaluationResult
from ...methods.learners.dddr import DDDRLearner
from ...methods.aggregators import FedAvgAggregator
from ...api.decorators import trainer
from ...models.ldm import LatentDiffusion


@trainer("dddr")
class DDDRFederationTrainer(FLTrainerBase):
    """DDDRè”é‚¦è®­ç»ƒå™¨ - åŸºäºDDDR-master OURs.pyè®¾è®¡ï¼Œé€‚é…FedCLæ¶æ„"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # åŸºæœ¬é…ç½®
        self.num_clients = config.get("num_clients", 10)
        self.num_tasks = config.get("num_tasks", 5)
        self.classes_per_task = config.get("classes_per_task", 10)
        self.total_classes = config.get("total_classes", 50)
        
        # è®­ç»ƒå‚æ•°ï¼ˆå®Œå…¨å¯¹é½ours.pyçš„argsè¶…å‚æ•°ï¼‰
        self.com_round = config.get("com_round", 10)  # å¯¹åº”ours.pyçš„com_round
        self.com_round_gen = config.get("com_round_gen", 5)  # å¯¹åº”ours.pyçš„com_round_gen
        self.local_ep = config.get("local_ep", 1)  # å¯¹åº”ours.pyçš„local_ep
        self.frac = config.get("frac", 1.0)  # å¯¹åº”ours.pyçš„frac
        self.num_users = config.get("num_users", self.num_clients)  # å¯¹åº”ours.pyçš„num_users
        self.batch_size = config.get("batch_size", 32)
        self.learning_rate = config.get("learning_rate", 0.001)
        
        # DDDRç‰¹å®šå‚æ•°ï¼ˆå®Œå…¨å¯¹é½ours.pyï¼‰
        self.pre_size = config.get("pre_size", 200)
        self.cur_size = config.get("cur_size", 50)
        self.n_iter = config.get("n_iter", 2)
        self.g_local_train_steps = config.get("g_local_train_steps", 5)  # å¯¹åº”ours.pyçš„g_local_train_steps
        self.w_kd = config.get("w_kd", 10.0)  # å¯¹åº”ours.pyçš„w_kd
        self.w_ce_pre = config.get("w_ce_pre", 0.5)  # å¯¹åº”ours.pyçš„w_ce_pre
        self.w_scl = config.get("w_scl", 1.0)  # å¯¹åº”ours.pyçš„w_scl
        self.g_sigma = config.get("g_sigma", 0.0)  # å¯¹åº”ours.pyçš„g_sigma
        self.classifer_dp = config.get("classifer_dp", 0.0)  # å¯¹åº”ours.pyçš„classifer_dpï¼ˆæ³¨æ„æ‹¼å†™ï¼‰
        self.save_cls_embeds = config.get("save_cls_embeds", False)  # å¯¹åº”ours.pyçš„save_cls_embeds
        
        # LDMé…ç½®
        self.ldm_config_path = config.get("ldm_config")
        self.ldm_ckpt_path = config.get("ldm_ckpt")
        
        # çŠ¶æ€å˜é‡
        self.current_task = 0
        self.known_classes = 0
        self.total_classes_seen = 0
        
        # æ¨¡å‹ç»„ä»¶
        self._generator = None
        self._classifier = None
        
        # Learnerä»£ç†ç³»ç»Ÿ
        self.learner_proxies = {}
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆä»£ç†åˆ›å»ºå»¶åè‡³æ³¨å†Œäº‹ä»¶è§¦å‘ï¼‰
        self._init_aggregator()
        self._init_diffusion_generator()
        self._init_classifier()
        
        # ğŸ†• ç¡®ä¿é€šä¿¡ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼ˆç”±æŠ½è±¡åŸºç±»ç®¡ç†ï¼‰
        self._ensure_communication_initialized()
        
        self.logger.info("âœ… DDDRFederationTrainer åˆå§‹åŒ–å®Œæˆ")
    
    def _init_aggregator(self):
        """åˆå§‹åŒ–èšåˆå™¨ - ä½¿ç”¨ç»Ÿä¸€çš„èšåˆå™¨"""
        # ä½¿ç”¨çˆ¶ç±»çš„èšåˆå™¨ï¼Œä¸éœ€è¦é‡å¤åˆå§‹åŒ–
        if not hasattr(self, 'aggregator') or self.aggregator is None:
            self.aggregator = FedAvgAggregator()
        self.logger.info("âœ… èšåˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_diffusion_generator(self):
        """åˆå§‹åŒ–æ‰©æ•£ç”Ÿæˆå™¨"""
        if not self.ldm_config_path or not self.ldm_ckpt_path:
            raise ValueError("LDMé…ç½®è·¯å¾„å’Œæ£€æŸ¥ç‚¹è·¯å¾„å¿…é¡»æä¾›")
        
        try:
            # åŠ è½½LDMé…ç½®
            ldm_config = OmegaConf.load(self.ldm_config_path)
            
            # æå–å¿…éœ€çš„é…ç½®
            first_stage_config = ldm_config.model.params.first_stage_config
            cond_stage_config = ldm_config.model.params.cond_stage_config
            personalization_config = ldm_config.model.params.personalization_config
            
            # åˆ›å»ºå‚æ•°å­—å…¸ï¼Œé¿å…é‡å¤
            model_params = dict(ldm_config.model.params)
            # ç§»é™¤å·²ç»å•ç‹¬ä¼ é€’çš„å‚æ•°
            model_params.pop('first_stage_config', None)
            model_params.pop('cond_stage_config', None)
            model_params.pop('personalization_config', None)
            
            # åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹
            self._generator = LatentDiffusion(
                first_stage_config=first_stage_config,
                cond_stage_config=cond_stage_config,
                personalization_config=personalization_config,
                **model_params
            )
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            if os.path.exists(self.ldm_ckpt_path):
                checkpoint = torch.load(self.ldm_ckpt_path, map_location="cpu")
                self._generator.load_state_dict(checkpoint, strict=False)
                self.logger.info(f"âœ… æ‰©æ•£æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {self.ldm_ckpt_path}")
            else:
                self.logger.warning(f"âš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {self.ldm_ckpt_path}")
            
        except Exception as e:
            self.logger.error(f" æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_classifier(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨ - åœ¨FedCLä¸­ï¼Œåˆ†ç±»å™¨ç”±learnerç®¡ç†ï¼Œtrainerä¸ç›´æ¥åˆå§‹åŒ–"""
        # åœ¨FedCLæ¶æ„ä¸­ï¼Œåˆ†ç±»å™¨ï¼ˆç½‘ç»œï¼‰ç”±learnerç«¯ç®¡ç†
        # traineråªè´Ÿè´£åè°ƒå’Œèšåˆï¼Œä¸ç›´æ¥æŒæœ‰åˆ†ç±»å™¨å®ä¾‹
        self._classifier = None
        self.logger.info(" åˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç”±learnerç®¡ç†ï¼‰")
    
    def _init_learner_proxies(self):
        """
        åˆå§‹åŒ–Learnerä»£ç†ç³»ç»Ÿ
        
        åœ¨DDDRä¸­ï¼Œlearnerä»£ç†æ˜¯åœ¨å®¢æˆ·ç«¯æ³¨å†Œæ—¶ç”±æœåŠ¡ç«¯è‡ªåŠ¨åˆ›å»ºçš„ï¼Œ
        è¿™é‡Œåªéœ€è¦ç¡®ä¿ç›¸å…³å˜é‡è¢«æ­£ç¡®åˆå§‹åŒ–ã€‚
        å®é™…çš„ä»£ç†åˆ›å»ºä¼šåœ¨start_server()ä¸­çš„_on_registerå›è°ƒä¸­è¿›è¡Œã€‚
        """
        # ç¡®ä¿learner_proxieså­—å…¸å­˜åœ¨
        if not hasattr(self, '_learner_proxies'):
            self._learner_proxies = {}
        
        # å¯¹äºDDDRï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¿é—®çˆ¶ç±»çš„_learner_proxies
        # è¿™æ ·get_learner_proxyç­‰æ–¹æ³•å¯ä»¥æ­£å¸¸å·¥ä½œ
        self.learner_proxies = self._learner_proxies
        
        self.logger.info("ğŸ”§ Learnerä»£ç†ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ³¨å†Œ")
    
    def _get_communication_backend(self):
        """è·å–é€šä¿¡åç«¯ - ä½¿ç”¨AbstractFederationTrainerå¯åŠ¨çš„serveré€šä¿¡"""
        if hasattr(self, '_server_comm') and self._server_comm is not None:
            return self._server_comm
        raise RuntimeError("Server communication not initialized; call start_server() first")
    
    def setup_training(self, data_manager=None):
        """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
        self.logger.info("ğŸ”§ è®¾ç½®DDDRè®­ç»ƒç¯å¢ƒ")
        
        # å‡†å¤‡ä»»åŠ¡æ•°æ®
        self._prepare_task_data(data_manager)
        
        # è®¾ç½®è®¾å¤‡
        self.device = self.config.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self._generator.to(self.device)
        self._classifier.to(self.device)
        
        self.logger.info(f"âœ… è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆï¼Œè®¾å¤‡: {self.device}")
    
    def _prepare_task_data(self, data_manager):
        """å‡†å¤‡ä»»åŠ¡æ•°æ®"""
        if data_manager is None:
            raise ValueError("æ•°æ®ç®¡ç†å™¨ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æä¾›çœŸå®çš„æ•°æ®ç®¡ç†å™¨")
        
        self.logger.info("ğŸ“Š ä½¿ç”¨çœŸå®æ•°æ®ç®¡ç†å™¨")
        self._load_real_task_data(data_manager)
    
    def _load_real_task_data(self, data_manager):
        """åŠ è½½çœŸå®ä»»åŠ¡æ•°æ®"""
        # è·å–å½“å‰ä»»åŠ¡çš„æ•°æ®
        task_data = data_manager.get_task_data(self.current_task)
        
        # æ›´æ–°ç±»åˆ«ä¿¡æ¯
        self.known_classes = self.total_classes_seen
        self.total_classes_seen += self.classes_per_task
        
        # å­˜å‚¨ä»»åŠ¡æ•°æ®
        self.current_task_data = task_data
        
        self.logger.info(f"âœ… ä»»åŠ¡ {self.current_task + 1} æ•°æ®åŠ è½½å®Œæˆï¼Œç±»åˆ«èŒƒå›´: {self.known_classes}-{self.total_classes_seen}")
    
    def execute_client_round(self, round_idx: int, client_ids: List[int]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå®¢æˆ·ç«¯è®­ç»ƒè½®æ¬¡"""
        self.logger.info(f" æ‰§è¡Œå®¢æˆ·ç«¯è½®æ¬¡ {round_idx + 1}")
        
        client_results = []
        
        # é€šè¿‡learnerä»£ç†æ‰§è¡Œå®¢æˆ·ç«¯è®­ç»ƒ
        for client_id in client_ids:
            client_id_str = f"client_{client_id}"
            
            if client_id_str in self.learner_proxies:
                learner_proxy = self.learner_proxies[client_id_str]
                
                try:
                    # é€šè¿‡ä»£ç†æ‰§è¡Œå®¢æˆ·ç«¯è®­ç»ƒ
                    result = learner_proxy.train_epoch(
                        epochs=self.local_epochs,
                        batch_size=self.batch_size,
                        learning_rate=self.learning_rate
                    )
                    
                    client_results.append({
                        'client_id': client_id,
                        'round': round_idx,
                        'status': 'completed',
                        'metrics': result.get('metrics', {}),
                        'weights': result.get('weights', {})
                    })
                    
                except Exception as e:
                    self.logger.error(f"å®¢æˆ·ç«¯ {client_id_str} è®­ç»ƒå¤±è´¥: {e}")
                    client_results.append({
                        'client_id': client_id,
                        'round': round_idx,
                        'status': 'failed',
                        'error': str(e)
                    })
            else:
                self.logger.warning(f"å®¢æˆ·ç«¯ {client_id_str} çš„learnerä»£ç†ä¸å­˜åœ¨")
        
        return client_results
    
    def execute_server_aggregation(self, client_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡ŒæœåŠ¡ç«¯èšåˆ"""
        self.logger.info("ğŸ”— æ‰§è¡ŒæœåŠ¡ç«¯èšåˆ")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„èšåˆå™¨è¿›è¡Œèšåˆ
        if client_results:
            # æå–æˆåŠŸçš„å®¢æˆ·ç«¯æƒé‡
            successful_results = [r for r in client_results if r.get('status') == 'completed']
            
            if successful_results:
                # æå–æƒé‡è¿›è¡Œèšåˆ
                weights_list = [r.get('weights', {}) for r in successful_results]
                
                # ä½¿ç”¨èšåˆå™¨èšåˆæƒé‡
                aggregated_weights = self.aggregator.aggregate(weights_list)
                
                # æ›´æ–°å…¨å±€æ¨¡å‹
                self._update_global_model(aggregated_weights)
                
                aggregation_result = {
                    'status': 'completed',
                    'num_clients': len(successful_results),
                    'aggregation_method': 'fedavg',
                    'aggregated_weights': aggregated_weights
                }
            else:
                aggregation_result = {
                    'status': 'no_successful_clients',
                    'num_clients': 0
                }
        else:
            aggregation_result = {
                'status': 'no_clients',
                'num_clients': 0
            }
        
        return aggregation_result
    
    def incremental_train(self, data_manager=None, task_id=None):
        """
        å¢é‡è®­ç»ƒ - å®Œå…¨åŸºäºDDDR-master ours.pyçš„incremental_trainè®¾è®¡
        
        æ¯ä¸ªä»»åŠ¡çš„æµç¨‹ï¼š
        1. ä»»åŠ¡åˆå§‹åŒ–ï¼šæ›´æ–°ä»»åŠ¡çŠ¶æ€å’Œç±»åˆ«ä¿¡æ¯
        2. æ•°æ®åˆå§‹åŒ–ï¼šç”±ç”¨æˆ·åœ¨å¤–éƒ¨å®Œæˆï¼ˆinit_dataloaderé€»è¾‘åœ¨exampleä¸­ï¼‰
        3. ç±»åæ¼”ï¼šå¦‚æœéœ€è¦åˆæˆå›¾åƒï¼Œè¿›è¡Œè”é‚¦ç±»åæ¼”
        4. å›¾åƒç”Ÿæˆï¼šå¦‚æœéœ€è¦åˆæˆå›¾åƒï¼Œç”Ÿæˆåˆæˆå›¾åƒ
        5. åˆæˆæ•°æ®è®¾ç½®ï¼šè®¾ç½®åˆæˆæ•°æ®åŠ è½½å™¨
        6. è”é‚¦åˆ†ç±»å™¨è®­ç»ƒï¼šè¿›è¡Œcom_roundè½®è”é‚¦è®­ç»ƒ
        """
        if task_id is None:
            self.current_task += 1
        else:
            self.current_task = task_id
            
        self.logger.info(f"ğŸš€ å¼€å§‹DDDRå¢é‡è®­ç»ƒ - ä»»åŠ¡ {self.current_task}")
        
        # 1. ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆå¯¹åº”ours.pyçš„setup_seedå’Œä»»åŠ¡è®¡æ•°å™¨ï¼‰
        self._update_task_state(data_manager)
        
        # 2. æ•°æ®åˆå§‹åŒ–ç”±ç”¨æˆ·åœ¨å¤–éƒ¨å®Œæˆï¼ˆä¸åœ¨æ¡†æ¶å†…å¤„ç†ï¼‰
        # å¯¹åº”ours.pyçš„init_dataloaderï¼Œä½†ç”¨æˆ·è‡ªå·±è´Ÿè´£æ•°æ®å‡†å¤‡
        
        # 3. ç±»åæ¼”é˜¶æ®µï¼ˆå¯¹åº”ours.pyçš„_class_inversionï¼‰
        inv_text_embeds = None
        if self.need_syn_imgs:
            self.logger.info("å¼€å§‹è”é‚¦ç±»åæ¼”")
            inv_text_embeds = self._class_inversion()
            
            # 4. å›¾åƒåˆæˆé˜¶æ®µï¼ˆå¯¹åº”ours.pyçš„_synthesis_imgsï¼‰
            self.logger.info("å¼€å§‹ç”Ÿæˆåˆæˆå›¾åƒ")
            self._synthesis_imgs(inv_text_embeds)
        
        # 5. åˆæˆæ•°æ®åˆå§‹åŒ–ï¼ˆå¯¹åº”ours.pyçš„_init_syn_dataloaderï¼‰
        self.logger.info("åˆå§‹åŒ–åˆæˆæ•°æ®åŠ è½½å™¨")
        self._init_syn_dataloader()
        
        # 6. è”é‚¦åˆ†ç±»å™¨è®­ç»ƒé˜¶æ®µï¼ˆå¯¹åº”ours.pyçš„_fl_trainï¼‰
        # è¿™é‡Œè¿›è¡Œcom_roundè½®è”é‚¦è®­ç»ƒ
        self.logger.info(f"å¼€å§‹è”é‚¦åˆ†ç±»å™¨è®­ç»ƒ - {self.com_round} è½®")
        self._fl_train()
        
        # 7. ä»»åŠ¡å®Œæˆåå¤„ç†ï¼ˆå¯¹åº”ours.pyçš„after_taskï¼‰
        self._after_task()
        
        self.logger.info(f"DDDRå¢é‡è®­ç»ƒå®Œæˆ - ä»»åŠ¡ {self.current_task}")
    
    def _update_task_state(self, data_manager):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€ - å¯¹åº”ours.pyä¸­incremental_trainçš„å‰å‡ è¡Œ"""
        # å¯¹åº” setup_seed(self.seed)
        if hasattr(self, 'seed'):
            import random
            import numpy as np
            import torch
            random.seed(self.seed)
            np.random.seed(self.seed)
            torch.manual_seed(self.seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(self.seed)
                torch.cuda.manual_seed_all(self.seed)
        
        # å¯¹åº” self._cur_task += 1 å’Œç±»åˆ«è®¡ç®—
        if data_manager:
            task_size = data_manager.get_task_size(self.current_task)
            self.total_classes_seen = self.known_classes + task_size
            
            self.logger.info(f"Learning on {self.known_classes}-{self.total_classes_seen}")
            
            # æ›´æ–°ç”Ÿæˆå™¨çš„ç±»åˆ«æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self._generator and hasattr(self._generator, 'embedding_manager'):
                # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…LDMå®ç°æ¥è°ƒæ•´
                pass
        else:
            self.logger.warning("æœªæä¾›data_managerï¼Œä½¿ç”¨é…ç½®ä¸­çš„ç±»åˆ«ä¿¡æ¯")
    
    def _after_task(self):
        """ä»»åŠ¡å®Œæˆåå¤„ç† - å¯¹åº”ours.pyçš„after_task"""
        self.known_classes = self.total_classes_seen
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜æ¨¡å‹ç­‰é€»è¾‘
        self.logger.info(f"ä»»åŠ¡ {self.current_task} å®Œæˆï¼Œå·²çŸ¥ç±»åˆ«æ•°: {self.known_classes}")
    
    @property
    def need_syn_imgs(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆåˆæˆå›¾åƒ"""
        return self.config.get('syn_image_path') is None
    
    def _class_inversion(self):
        """è”é‚¦ç±»åæ¼” - å®Œå…¨å¯¹é½ours.pyçš„_class_inversionæ–¹æ³•"""
        self.logger.info("ğŸ”„ å¼€å§‹ç±»åˆ«åæ¼”")
        
        # å°†ç”Ÿæˆå™¨ç§»åˆ°GPUå¹¶é‡ç½®åµŒå…¥åˆ°åˆå§‹çŠ¶æ€ï¼ˆä¸ours.py line 206-207ä¸€è‡´ï¼‰
        device = self.config.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self._generator = self._generator.to(device)
        
        if hasattr(self, 'generator_init_embedding'):
            self._generator.embedding_manager.load_state_dict(self.generator_init_embedding)
        
        # è·å–è¶…å‚æ•°ï¼ˆä¸ours.pyä¸€è‡´ï¼‰
        com_round_gen = self.com_round_gen
        frac = self.frac
        num_users = self.num_users
        g_sigma = self.g_sigma
        
        # è¿›åº¦æ¡ï¼ˆä¸ours.py line 208ä¸€è‡´ï¼‰
        prog_bar = tqdm(range(com_round_gen), desc='Class Inversion')
        
        for _ in prog_bar:
            local_weights = []
            
            # å®¢æˆ·ç«¯é€‰æ‹©ï¼ˆä¸ours.py line 211-212ä¸€è‡´ï¼‰
            m = max(int(frac * num_users), 1)
            idxs_users = np.random.choice(range(min(num_users, len(self._learner_proxies))), m, replace=False)
            client_ids = list(self._learner_proxies.keys())
            
            # å®¢æˆ·ç«¯æœ¬åœ°ç”Ÿæˆå™¨è®­ç»ƒï¼ˆä¸ours.py line 213-218ä¸€è‡´ï¼‰
            for idx in idxs_users:
                if idx >= len(client_ids):
                    continue
                    
                client_id = client_ids[idx]
                try:
                    proxy = self._learner_proxies[client_id]
                    # å¯¹åº”ours.pyçš„_local_update_gè°ƒç”¨ï¼Œè¿”å›embedding_manager.state_dict()
                    w = proxy.call_method("train_generator_embeddings")
                    if w is not None:
                        local_weights.append(deepcopy(w))
                except Exception as e:
                    self.logger.warning(f"å®¢æˆ·ç«¯ {client_id} ç”Ÿæˆå™¨è®­ç»ƒå¤±è´¥: {e}")
            
            # æƒé‡èšåˆï¼ˆä¸ours.py line 219-220ä¸€è‡´ï¼‰
            if local_weights:
                global_weights = self._average_embedding_weights(local_weights, g_sigma=g_sigma)
                self._generator.embedding_manager.load_state_dict(global_weights)
        
        # å¯¼å‡ºæœ€ç»ˆçš„ç±»åˆ«åµŒå…¥ï¼ˆä¸ours.py line 221ä¸€è‡´ï¼‰
        inv_text_embeds = deepcopy(self._generator.embedding_manager.string_to_param_dict)
        
        # ä¿å­˜ç±»åˆ«åµŒå…¥ï¼ˆä¸ours.py line 222ä¸€è‡´ï¼‰
        if self.save_cls_embeds:
            self._save_class_embeddings(inv_text_embeds)
        
        self.logger.info("âœ… ç±»åˆ«åæ¼”å®Œæˆ")
        return inv_text_embeds
    
    def _fl_train(self):
        """
        è”é‚¦è®­ç»ƒ - å®Œå…¨å¯¹é½ours.pyçš„_fl_trainæ–¹æ³•
        
        åŒºåˆ†é¦–ä»»åŠ¡å’Œå¢é‡ä»»åŠ¡ï¼š
        - é¦–ä»»åŠ¡ï¼šä½¿ç”¨local_update
        - å¢é‡ä»»åŠ¡ï¼šä½¿ç”¨local_finetuneï¼ˆåŒ…å«å›æ”¾å’ŒçŸ¥è¯†è’¸é¦ï¼‰
        """
        self.logger.info(f"ğŸ“ å¼€å§‹è”é‚¦è®­ç»ƒ - Task {self.current_task}")
        
        # å°†ç½‘ç»œç§»åˆ°GPUï¼ˆä¸ours.py line 123ä¸€è‡´ï¼‰
        device = self.config.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        
        # è¿›åº¦æ¡ï¼ˆä¸ours.py line 124ä¸€è‡´ï¼‰
        prog_bar = tqdm(range(self.com_round))
        
        for com in prog_bar:
            local_weights = []
            
            # å®¢æˆ·ç«¯é€‰æ‹©ï¼ˆä¸ours.py line 127-128ä¸€è‡´ï¼‰
            m = max(int(self.frac * self.num_users), 1)
            idxs_users = np.random.choice(range(min(self.num_users, len(self._learner_proxies))), m, replace=False)
            client_ids = list(self._learner_proxies.keys())
            
            # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒï¼ˆä¸ours.py line 129-135ä¸€è‡´ï¼‰
            for idx in idxs_users:
                if idx >= len(client_ids):
                    continue
                    
                client_id = client_ids[idx]
                proxy = self._learner_proxies[client_id]
                
                try:
                    if self.current_task == 0:
                        # é¦–ä»»åŠ¡ï¼šä½¿ç”¨local_updateï¼ˆå¯¹åº”ours.py line 131ï¼‰
                        w = proxy.call_method("local_update")
                    else:
                        # å¢é‡ä»»åŠ¡ï¼šä½¿ç”¨local_finetuneï¼ˆå¯¹åº”ours.py line 133-134ï¼‰
                        w = proxy.call_method("local_finetune")
                    
                    if w is not None:
                        local_weights.append(deepcopy(w))
                        
                except Exception as e:
                    self.logger.error(f"å®¢æˆ·ç«¯ {client_id} è®­ç»ƒå¤±è´¥: {e}")
            
            # èšåˆæƒé‡ï¼ˆä¸ours.py line 137ä¸€è‡´ï¼‰
            if local_weights:
                global_weights = self._average_weights(local_weights, dp_si=self.classifer_dp)
                
                # æ›´æ–°å…¨å±€ç½‘ç»œï¼ˆä¸ours.py line 138ä¸€è‡´ï¼‰
                self._update_global_classifier(global_weights)
                
                # æµ‹è¯•ï¼ˆä¸ours.py line 140-143ä¸€è‡´ï¼‰
                test_acc = self._compute_test_accuracy()
                info = f"Task {self.current_task}, Epoch {com + 1}/{self.com_round} => Test_accy {test_acc:.2f}"
                prog_bar.set_description(info)
        
        self.logger.info("âœ… è”é‚¦è®­ç»ƒå®Œæˆ")
    
    def _init_syn_dataloader(self):
        """
        åˆå§‹åŒ–åˆæˆæ•°æ®åŠ è½½å™¨ - å¯¹åº”ours.pyçš„_init_syn_dataloader
        
        ä¸ºå„å®¢æˆ·ç«¯è®¾ç½®å½“å‰ä»»åŠ¡çš„åˆæˆæ•°æ®å’Œå†å²ä»»åŠ¡çš„åˆæˆæ•°æ®
        """
        if not self.need_syn_imgs:
            # å¦‚æœç”¨æˆ·æä¾›äº†syn_image_pathï¼Œè·³è¿‡åˆæˆæ•°æ®è®¾ç½®
            self.logger.info("ä½¿ç”¨ç”¨æˆ·æä¾›çš„åˆæˆå›¾åƒè·¯å¾„ï¼Œè·³è¿‡åˆæˆæ•°æ®åŠ è½½å™¨è®¾ç½®")
            return
        
        syn_imgs_dir = self.config.get("syn_imgs_dir", os.path.join(self.config.get("save_dir", "outputs"), "syn_imgs"))
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è®¾ç½®åˆæˆæ•°æ®
        for client_id, proxy in self._learner_proxies.items():
            try:
                # è®¾ç½®å½“å‰ä»»åŠ¡çš„åˆæˆæ•°æ®
                proxy.call_method("set_current_syn_data", {
                    "syn_imgs_dir": syn_imgs_dir,
                    "task_id": self.current_task,
                    "cur_size": self.cur_size
                })
                
                # è®¾ç½®å†å²ä»»åŠ¡çš„åˆæˆæ•°æ®ï¼ˆå¦‚æœä¸æ˜¯é¦–ä»»åŠ¡ï¼‰
                if self.current_task > 0:
                    proxy.call_method("set_replay_syn_data", {
                        "syn_imgs_dir": syn_imgs_dir,
                        "current_task": self.current_task,
                        "pre_size": self.pre_size
                    })
                
                self.logger.debug(f"âœ… å®¢æˆ·ç«¯ {client_id} åˆæˆæ•°æ®è®¾ç½®å®Œæˆ")
                
            except Exception as e:
                self.logger.error(f"å®¢æˆ·ç«¯ {client_id} åˆæˆæ•°æ®è®¾ç½®å¤±è´¥: {e}")
    
    def _init_text_embeddings(self):
        """åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥"""
        # ä¸ºå½“å‰ä»»åŠ¡çš„æ¯ä¸ªç±»åˆ«åˆ›å»ºæ–‡æœ¬åµŒå…¥
        task_classes = list(range(self.known_classes, self.total_classes_seen))
        
        inv_text_embeds = {}
        for class_id in task_classes:
            # åˆ›å»ºç±»åˆ«æ–‡æœ¬æç¤º
            text_prompt = f"a photo of class {class_id}"
            
            # è·å–BERTåµŒå…¥
            with torch.no_grad():
                text_embeds = self._generator.get_learned_conditioning([text_prompt])
                inv_text_embeds[f"class_{class_id}"] = text_embeds.clone()
        
        self.logger.info(f"âœ… åˆå§‹åŒ–äº† {len(inv_text_embeds)} ä¸ªç±»åˆ«çš„æ–‡æœ¬åµŒå…¥")
        return inv_text_embeds
    
    def _update_generator_embeddings(self, state_dict: Dict[str, Any]):
        """æ›´æ–°ç”Ÿæˆå™¨åµŒå…¥ï¼ˆç›´æ¥load_state_dictï¼‰"""
        try:
            self._generator.embedding_manager.load_state_dict(state_dict, strict=False)
        except Exception as e:
            self.logger.error(f"åŠ è½½ç”Ÿæˆå™¨åµŒå…¥å¤±è´¥: {e}")

    def _average_embedding_weights(self, weights_list: List[Dict[str, Any]], g_sigma: float = 0.0) -> Dict[str, Any]:
        """å¯¹embedding_manager.state_dictåšå…ƒç´ å‡å€¼ï¼ˆå®Œå…¨å¯¹é½ours.pyçš„average_weightsï¼‰"""
        if not weights_list:
            return {}
        # æ·±æ‹·è´ç¬¬ä¸€ä»½ç»“æ„ä½œä¸ºç´¯åŠ å™¨
        import copy, torch
        agg = copy.deepcopy(weights_list[0])
        # å¯¹å…¶ä½™æƒé‡é€å…ƒç´ ç›¸åŠ 
        for w in weights_list[1:]:
            for k in agg:
                if isinstance(agg[k], dict) and isinstance(w.get(k), dict):
                    # é€’å½’åˆ°ä¸‹ä¸€å±‚
                    for kk in agg[k]:
                        if isinstance(agg[k][kk], torch.Tensor) and isinstance(w[k].get(kk), torch.Tensor):
                            agg[k][kk] = agg[k][kk] + w[k][kk]
                elif isinstance(agg[k], torch.Tensor) and isinstance(w.get(k), torch.Tensor):
                    agg[k] = agg[k] + w[k]
        # æ±‚å¹³å‡
        num = float(len(weights_list))
        for k in agg:
            if isinstance(agg[k], dict):
                for kk in agg[k]:
                    if isinstance(agg[k][kk], torch.Tensor):
                        agg[k][kk] = agg[k][kk] / num
            elif isinstance(agg[k], torch.Tensor):
                agg[k] = agg[k] / num
        # å¯é€‰æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆå¯¹åº”ours.pyçš„g_sigmaå‚æ•°ï¼‰
        if g_sigma and g_sigma > 0:
            for k in agg:
                if isinstance(agg[k], dict):
                    for kk in agg[k]:
                        if isinstance(agg[k][kk], torch.Tensor):
                            agg[k][kk] = agg[k][kk] + torch.randn_like(agg[k][kk]) * g_sigma
                elif isinstance(agg[k], torch.Tensor):
                    agg[k] = agg[k] + torch.randn_like(agg[k]) * g_sigma
        return agg
    
    def _average_weights(self, weights_list: List[Dict[str, Any]], dp_si: float = 0.0) -> Dict[str, Any]:
        """å¯¹åˆ†ç±»å™¨æƒé‡åšFedAvgèšåˆï¼ˆå¯¹é½ours.pyçš„average_weightsï¼Œæ”¯æŒå·®åˆ†éšç§ï¼‰"""
        if not weights_list:
            return {}
        
        # ä½¿ç”¨FedAvgèšåˆå™¨
        aggregation_result = self.aggregator.aggregate([
            {"model_weights": w, "num_samples": 1} for w in weights_list
        ])
        
        aggregated_weights = aggregation_result["aggregated_weights"]
        
        # æ·»åŠ å·®åˆ†éšç§å™ªå£°ï¼ˆå¯¹åº”ours.pyçš„dp_siå‚æ•°ï¼‰
        if dp_si > 0:
            aggregated_weights = self._add_classifier_noise(aggregated_weights, dp_si)
        
        return aggregated_weights
    
    def _synthesis_imgs(self, inv_text_embeds: Dict[str, torch.Tensor]):
        """ç”Ÿæˆåˆæˆå›¾åƒï¼ˆå¯¹é½DDDR ours.pyçš„_synthesis_imgsï¼‰"""
        self.logger.info("ğŸ¨ å¼€å§‹ç”Ÿæˆåˆæˆå›¾åƒ")

        # å°†ç±»åµŒå…¥å­—å…¸è®¾ç½®åˆ°ç”Ÿæˆå™¨
        try:
            self._generator.embedding_manager.string_to_param_dict = inv_text_embeds
        except Exception as e:
            self.logger.warning(f"è®¾ç½®ç±»åµŒå…¥å¤±è´¥: {e}")

        # é‡‡æ ·å™¨
        try:
            from ...models.ldm import DDIMSampler
        except Exception as e:
            self.logger.error(f"æ— æ³•å¯¼å…¥DDIMSampler: {e}")
            return

        sampler = DDIMSampler(self._generator)

        # è¾“å‡ºç›®å½•ï¼šsyn_imgs_dir/task_{t}/class_id/*.jpg
        syn_root = self.config.get("syn_imgs_dir", os.path.join(self.config.get("save_dir", "outputs"), "syn_imgs"))
        outdir = os.path.join(syn_root, f"task_{self.current_task}")
        os.makedirs(outdir, exist_ok=True)

        # ç”Ÿæˆå‚æ•°ï¼ˆä¸ours.pyä¿æŒä¸€è‡´ï¼‰
        prompt = "a photo of *"
        n_samples = int(self.config.get("n_samples", 40))
        scale = float(self.config.get("scale", 10.0))
        ddim_steps = int(self.config.get("ddim_steps", 50))
        ddim_eta = float(self.config.get("ddim_eta", 0.0))
        H = int(self.config.get("img_h", 256))
        W = int(self.config.get("img_w", 256))
        num_iter = int(self.config.get("n_iter", self.n_iter if hasattr(self, 'n_iter') else 2))

        # è®¡ç®—æ¯ç±»ç”Ÿæˆæ•°é‡
        if len(inv_text_embeds) == 0:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„ç±»åµŒå…¥ï¼Œè·³è¿‡å›¾åƒç”Ÿæˆ")
            return
        num_images_per_class = max(1, int(self.pre_size // len(inv_text_embeds))) if hasattr(self, 'pre_size') else 40

        # è·å–ç±»åˆ«IDé›†åˆä¸æœ€å°ç±»åˆ«IDï¼ˆç›¸å¯¹IDè®¡ç®—ï¼‰
        try:
            class_ids = [int(name.split('_')[-1]) for name in inv_text_embeds.keys()]
        except Exception:
            class_ids = list(range(len(inv_text_embeds)))
        min_class_id = min(class_ids) if class_ids else 0

        device = self.config.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self._generator = self._generator.to(device)

        with torch.no_grad():
            for tmp_cls in class_ids:
                base_count = 0
                class_dir = os.path.join(outdir, str(tmp_cls))
                os.makedirs(class_dir, exist_ok=True)

                with self._generator.ema_scope():
                    uc = None
                    tmp_cls_tensor = torch.LongTensor([tmp_cls - min_class_id] * n_samples).to(device)
                    if scale != 1.0:
                        uc = self._generator.get_learned_conditioning(n_samples * [""], tmp_cls_tensor)

                    for _ in trange(num_iter, desc=f"Sampling {tmp_cls}"):
                        c = self._generator.get_learned_conditioning(n_samples * [prompt], tmp_cls_tensor)
                        shape = [4, H // 8, W // 8]
                        samples_ddim, _ = sampler.sample(
                            S=ddim_steps,
                            conditioning=c,
                            batch_size=n_samples,
                            shape=shape,
                            verbose=False,
                            unconditional_guidance_scale=scale,
                            unconditional_conditioning=uc,
                            eta=ddim_eta,
                        )
                        x_samples_ddim = self._generator.decode_first_stage(samples_ddim)
                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)

                        for x_sample in x_samples_ddim:
                            x_sample = 255.0 * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
                            img_path = os.path.join(class_dir, f"{tmp_cls}-{base_count}.jpg")
                            try:
                                from PIL import Image
                                Image.fromarray(x_sample.astype(np.uint8)).save(img_path)
                            except Exception as e:
                                self.logger.warning(f"ä¿å­˜å›¾åƒå¤±è´¥ {img_path}: {e}")
                            base_count += 1

        self.logger.info("âœ… åˆæˆå›¾åƒç”Ÿæˆå®Œæˆ")
    
    def _federated_learning_train(self):
        """è”é‚¦å­¦ä¹ è®­ç»ƒ"""
        self.logger.info(f"ğŸ“ å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ - {self.com_rounds} è½®æ¬¡")
        
        # æ‰§è¡Œæ ‡å‡†çš„è”é‚¦è®­ç»ƒæµç¨‹
        for round_idx in range(self.com_rounds):
            self.logger.info(f"  è½®æ¬¡ {round_idx + 1}/{self.com_rounds}")
            
            # é€‰æ‹©å‚ä¸çš„å®¢æˆ·ç«¯
            num_participants = max(int(0.5 * self.num_clients), 1)
            participant_ids = np.random.choice(range(self.num_clients), num_participants, replace=False)
            
            # æ‰§è¡Œå®¢æˆ·ç«¯è®­ç»ƒ
            client_results = self.execute_client_round(round_idx, participant_ids)
            
            # æ‰§è¡ŒæœåŠ¡ç«¯èšåˆ
            aggregation_result = self.execute_server_aggregation(client_results)
            
            self.logger.debug(f"è½®æ¬¡ {round_idx + 1}/{self.com_rounds} å®Œæˆ")
        
        self.logger.info("âœ… è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆ")
    
    def train(self, num_rounds: int, **kwargs) -> dict:
        """
        æ‰§è¡ŒDDDRè”é‚¦è®­ç»ƒ - æŒç»­å­¦ä¹ ä»»åŠ¡
        
        å®ç°å®Œæ•´çš„DDDRæŒç»­å­¦ä¹ æµç¨‹ï¼š
        1. å¯åŠ¨æœåŠ¡ç«¯é€šä¿¡
        2. ç­‰å¾…å®¢æˆ·ç«¯æ³¨å†Œ
        3. æ‰§è¡Œå¤šä¸ªä»»åŠ¡çš„å¢é‡è®­ç»ƒ
        4. æ¯ä¸ªä»»åŠ¡åŒ…å«ï¼šç±»åæ¼” â†’ å›¾åƒç”Ÿæˆ â†’ è”é‚¦åˆ†ç±»å™¨è®­ç»ƒ
        """
        import time
        start_time = time.time()
        
        self.logger.info(f" å¼€å§‹DDDRè”é‚¦æŒç»­å­¦ä¹ è®­ç»ƒ - {self.num_tasks} ä¸ªä»»åŠ¡")
        
        try:
            # ğŸ†• æœåŠ¡ç«¯å·²åœ¨åˆå§‹åŒ–é˜¶æ®µå¯åŠ¨ï¼Œå®¢æˆ·ç«¯å·²æ³¨å†Œ
            # æ‰§è¡Œå¤šä¸ªä»»åŠ¡çš„å¢é‡è®­ç»ƒ
            for task_idx in range(self.num_tasks):
                self.logger.info(f"ğŸ“‹ å¼€å§‹ä»»åŠ¡ {task_idx + 1}/{self.num_tasks}")
                
                # æ‰§è¡Œå•ä¸ªä»»åŠ¡çš„å¢é‡è®­ç»ƒ
                self.incremental_train(task_id=task_idx)
                
                # ä»»åŠ¡é—´è¯„ä¼°
                if task_idx < self.num_tasks - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªä»»åŠ¡
                    test_acc = self._compute_test_accuracy()
                    self.logger.info(f"âœ… ä»»åŠ¡ {task_idx + 1} å®Œæˆï¼Œå‡†ç¡®ç‡: {test_acc:.2f}%")
            
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            training_time = time.time() - start_time
            
            # æ„å»ºå¹¶è¿”å›è®­ç»ƒç»“æœ
            result = self.build_training_result(
                num_rounds=self.num_tasks,  # ä½¿ç”¨ä»»åŠ¡æ•°ä½œä¸ºè½®æ•°
                training_time=training_time,
                execution_mode="pseudo_federation"
            )
            
            self.logger.info(f"âœ… DDDRè”é‚¦æŒç»­å­¦ä¹ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ DDDRè”é‚¦è®­ç»ƒå¤±è´¥: {e}")
            raise
        finally:
            # åœæ­¢æœåŠ¡ç«¯é€šä¿¡
            self.stop_server()
    

    
    def evaluate(self, test_data: Optional[Any] = None, **kwargs) -> EvaluationResult:
        """æ‰§è¡Œè”é‚¦æ¨¡å‹è¯„ä¼°"""
        self.logger.info("ğŸ” å¼€å§‹DDDRè”é‚¦è¯„ä¼°")
        
        if not self._learner_proxies:
            self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„å®¢æˆ·ç«¯ä»£ç†ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return EvaluationResult(
                metrics={"accuracy": 0.0, "loss": 1.0},
                task_metrics={},
                evaluation_time=0.0,
                metadata={"error": "no_clients"}
            )
        
        import time
        start_time = time.time()
        
        # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯çš„è¯„ä¼°ç»“æœ
        client_evaluations = []
        for client_id, proxy in self._learner_proxies.items():
            try:
                result = proxy.call_method("evaluate", {
                    "test_loader": test_data,
                    "metrics": ["accuracy", "loss"]
                })
                client_evaluations.append(result)
                
            except Exception as e:
                self.logger.error(f"å®¢æˆ·ç«¯ {client_id} è¯„ä¼°å¤±è´¥: {e}")
        
        # èšåˆè¯„ä¼°ç»“æœ
        if client_evaluations:
            total_samples = sum(r.get("num_samples", 0) for r in client_evaluations)
            if total_samples > 0:
                weighted_accuracy = sum(
                    r.get("accuracy", 0) * r.get("num_samples", 0) 
                    for r in client_evaluations
                ) / total_samples
                
                weighted_loss = sum(
                    r.get("loss", 0) * r.get("num_samples", 0) 
                    for r in client_evaluations
                ) / total_samples
            else:
                weighted_accuracy = 0.0
                weighted_loss = 1.0
        else:
            weighted_accuracy = 0.0
            weighted_loss = 1.0
        
        evaluation_time = time.time() - start_time
        
        metrics = {
            "accuracy": weighted_accuracy,
            "loss": weighted_loss,
            "num_clients": len(client_evaluations),
            "total_samples": total_samples if client_evaluations else 0
        }
        
        task_metrics = {
            f"task_{self.current_task}": {
                "accuracy": weighted_accuracy,
                "loss": weighted_loss
            }
        }
        
        self.logger.info(
            f"âœ… è”é‚¦è¯„ä¼°å®Œæˆ - å‡†ç¡®ç‡: {weighted_accuracy:.4f}, "
            f"æŸå¤±: {weighted_loss:.4f}, å®¢æˆ·ç«¯æ•°: {len(client_evaluations)}"
        )
        
        return EvaluationResult(
            metrics=metrics,
            task_metrics=task_metrics,
            evaluation_time=evaluation_time,
            metadata={"current_task": self.current_task}
        )
    
    # ============ è¾…åŠ©æ–¹æ³• ============
    
    def _add_classifier_noise(self, weights: Dict[str, Any], noise_sigma: float) -> Dict[str, Any]:
        """ä¸ºåˆ†ç±»å™¨æƒé‡æ·»åŠ å·®åˆ†éšç§å™ªå£°"""
        import torch
        noisy_weights = {}
        for k, v in weights.items():
            if isinstance(v, torch.Tensor):
                noise = torch.randn_like(v) * noise_sigma
                noisy_weights[k] = v + noise
            else:
                noisy_weights[k] = v
        return noisy_weights
    
    def _update_global_classifier(self, weights: Dict[str, Any]):
        """æ›´æ–°å…¨å±€åˆ†ç±»å™¨å¹¶å¹¿æ’­ç»™æ‰€æœ‰å®¢æˆ·ç«¯ - å¯¹åº”ours.pyçš„self._network.load_state_dict"""
        # åœ¨FedCLä¸­ï¼Œtrainerä¸ç›´æ¥æŒæœ‰åˆ†ç±»å™¨ï¼Œåªè´Ÿè´£å¹¿æ’­æƒé‡ç»™learner
        # å¯¹åº”ours.pyçš„: self._network.load_state_dict(global_weights)
        
        # å¹¿æ’­ç»™æ‰€æœ‰å®¢æˆ·ç«¯
        for client_id, proxy in self._learner_proxies.items():
            try:
                proxy.call_method("set_model_weights", {"weights": weights})
            except Exception as e:
                self.logger.error(f"æ›´æ–°å®¢æˆ·ç«¯ {client_id} æ¨¡å‹å¤±è´¥: {e}")
        
        self.logger.debug(f"âœ… å…¨å±€åˆ†ç±»å™¨æƒé‡å·²å¹¿æ’­ç»™ {len(self._learner_proxies)} ä¸ªå®¢æˆ·ç«¯")
    
    def _compute_test_accuracy(self) -> float:
        """è®¡ç®—æµ‹è¯•å‡†ç¡®ç‡ - å¯¹åº”ours.pyçš„_compute_accuracyæ–¹æ³•"""
        try:
            # åœ¨FedCLä¸­ï¼Œtraineré€šè¿‡learnerä»£ç†æ¥è·å–è¯„ä¼°ç»“æœ
            # å¯¹åº”ours.pyçš„: test_acc = self._compute_accuracy(self._network, self.test_loader)
            
            client_accuracies = []
            for client_id, proxy in self._learner_proxies.items():
                try:
                    # è°ƒç”¨learnerçš„evaluateæ–¹æ³•
                    result = proxy.call_method("evaluate")
                    if result and "accuracy" in result:
                        client_accuracies.append(result["accuracy"] * 100)
                except Exception as e:
                    self.logger.debug(f"å®¢æˆ·ç«¯ {client_id} è¯„ä¼°å¤±è´¥: {e}")
            
            if client_accuracies:
                # è¿”å›æ‰€æœ‰å®¢æˆ·ç«¯çš„å¹³å‡å‡†ç¡®ç‡
                avg_accuracy = sum(client_accuracies) / len(client_accuracies)
                return avg_accuracy
            else:
                return 0.0
                
        except Exception as e:
            self.logger.warning(f"æµ‹è¯•å‡†ç¡®ç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _save_class_embeddings(self, inv_text_embeds: Dict[str, Any]):
        """ä¿å­˜ç±»åµŒå…¥ - å¯¹åº”ours.pyçš„save_cls_embeds"""
        try:
            save_dir = self.config.get("save_dir", "outputs")
            cls_embeds_dir = os.path.join(save_dir, "cls_embeds_ckpt")
            os.makedirs(cls_embeds_dir, exist_ok=True)
            
            # è·å–ç±»åˆ«èŒƒå›´
            min_class_id = self.known_classes
            max_class_id = self.total_classes_seen - 1
            
            # ä¿å­˜åµŒå…¥æƒé‡
            embed_path = os.path.join(
                cls_embeds_dir, 
                f"{min_class_id}-{max_class_id}_embedding_manager.pt"
            )
            
            if self._generator and hasattr(self._generator, 'embedding_manager'):
                torch.save(self._generator.embedding_manager.state_dict(), embed_path)
                self.logger.info(f"âœ… ç±»åµŒå…¥å·²ä¿å­˜: {embed_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç±»åµŒå…¥å¤±è´¥: {e}")
