#!/usr/bin/env python3
"""
å¤šå®¢æˆ·ç«¯è¯„ä¼°æµ‹è¯•è„šæœ¬
åŸºäºFedCLExperimentæ ‡å‡†æµç¨‹ï¼Œæµ‹è¯•è¯„ä¼°å™¨å’Œæµ‹è¯•æ•°æ®é›†åŠŸèƒ½
"""

import pytest
import time
import threading
import signal
import os
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import torch
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
import sys
sys.path.insert(0, os.path.abspath('.'))

from fedcl.experiment.experiment import FedCLExperiment
from fedcl.federation.coordinators.federated_client import MultiLearnerClient
from fedcl.config.config_manager import DictConfig

# ç¡®ä¿å®ç°æ¨¡å—è¢«åŠ è½½ï¼Œè§¦å‘ç»„ä»¶æ³¨å†Œ
import fedcl.implementations

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EvaluationLogMonitor:
    """è¯„ä¼°æ—¥å¿—ç›‘æ§å™¨ - ä¸“é—¨ç›‘æ§è¯„ä¼°ç›¸å…³çš„æ—¥å¿—"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.evaluation_events = []
        self.errors = []
        self.warnings = []
        self.monitoring = False
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§æ—¥å¿—"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_logs)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§æ—¥å¿—"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=5)
            
    def _monitor_logs(self):
        """ç›‘æ§æ—¥å¿—æ–‡ä»¶"""
        processed_files = set()
        
        while self.monitoring:
            try:
                # æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
                log_files = list(self.log_dir.glob("*.log"))
                if not log_files:
                    time.sleep(1)
                    continue
                    
                # å¤„ç†æ–°çš„æ—¥å¿—æ–‡ä»¶
                for log_file in log_files:
                    if log_file not in processed_files:
                        self._process_log_file(log_file)
                        processed_files.add(log_file)
                        
                time.sleep(1)
                
            except Exception as e:
                logger.warning(f"æ—¥å¿—ç›‘æ§é”™è¯¯: {e}")
                time.sleep(1)
                
    def _process_log_file(self, log_file: Path):
        """å¤„ç†æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æŸ¥è¯„ä¼°ç›¸å…³äº‹ä»¶
                if any(keyword in line.lower() for keyword in [
                    'evaluation', 'evaluator', 'test_data', 'accuracy', 'precision', 'recall', 'f1', 'loss'
                ]):
                    self.evaluation_events.append(line)
                    logger.info(f"ğŸ“Š EVALUATION: {line}")
                    
                # æ£€æŸ¥é”™è¯¯
                if 'ERROR' in line:
                    self.errors.append(line)
                    logger.error(f"ğŸ”´ ERROR: {line}")
                    
                # æ£€æŸ¥è­¦å‘Š
                elif 'WARNING' in line or 'WARN' in line:
                    self.warnings.append(line)
                    logger.warning(f"ğŸŸ¡ WARNING: {line}")
                    
        except Exception as e:
            logger.warning(f"å¤„ç†æ—¥å¿—æ–‡ä»¶é”™è¯¯ {log_file}: {e}")
            
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æ‘˜è¦"""
        return {
            'evaluation_events': self.evaluation_events,
            'errors': self.errors,
            'warnings': self.warnings,
            'evaluation_count': len(self.evaluation_events),
            'error_count': len(self.errors),
            'warning_count': len(self.warnings)
        }


class TestEvaluationConfiguration:
    """è¯„ä¼°é…ç½®æµ‹è¯•"""
    
    @pytest.fixture
    def evaluation_config_dir(self):
        """è¯„ä¼°é…ç½®ç›®å½•"""
        return Path("tests/configs/mnist_real_test")
    
    def test_evaluation_config_validation(self, evaluation_config_dir):
        """æµ‹è¯•è¯„ä¼°é…ç½®éªŒè¯"""
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        assert (evaluation_config_dir / "experiment_config.yaml").exists()
        assert (evaluation_config_dir / "server_config.yaml").exists()
        
        # æ£€æŸ¥å®¢æˆ·ç«¯é…ç½®æ–‡ä»¶
        client_configs = []
        for i in range(1, 4):
            client_config_path = evaluation_config_dir / f"client_{i}_config.yaml"
            assert client_config_path.exists(), f"å®¢æˆ·ç«¯{i}é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
            
            with open(client_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                client_configs.append(config)
        
        # éªŒè¯æ¯ä¸ªå®¢æˆ·ç«¯éƒ½æœ‰è¯„ä¼°ç›¸å…³é…ç½®
        for i, config in enumerate(client_configs, 1):
            logger.info(f"éªŒè¯å®¢æˆ·ç«¯{i}çš„è¯„ä¼°é…ç½®...")
            
            # æ£€æŸ¥è¯„ä¼°å™¨é…ç½®
            assert 'evaluators' in config, f"å®¢æˆ·ç«¯{i}ç¼ºå°‘evaluatorsé…ç½®"
            evaluators = config['evaluators']
            assert len(evaluators) > 0, f"å®¢æˆ·ç«¯{i}è¯„ä¼°å™¨é…ç½®ä¸ºç©º"
            
            logger.info(f"å®¢æˆ·ç«¯{i}è¯„ä¼°å™¨æ•°é‡: {len(evaluators)}")
            for eval_name, eval_config in evaluators.items():
                assert 'class' in eval_config, f"å®¢æˆ·ç«¯{i}è¯„ä¼°å™¨{eval_name}ç¼ºå°‘classé…ç½®"
                logger.info(f"  - {eval_name}: {eval_config['class']}")
            
            # æ£€æŸ¥æµ‹è¯•æ•°æ®é›†é…ç½®
            assert 'test_datas' in config, f"å®¢æˆ·ç«¯{i}ç¼ºå°‘test_datasé…ç½®"
            test_datas = config['test_datas']
            assert len(test_datas) > 0, f"å®¢æˆ·ç«¯{i}æµ‹è¯•æ•°æ®é›†é…ç½®ä¸ºç©º"
            
            logger.info(f"å®¢æˆ·ç«¯{i}æµ‹è¯•æ•°æ®é›†æ•°é‡: {len(test_datas)}")
            for data_name, data_config in test_datas.items():
                assert 'dataset_config' in data_config, f"å®¢æˆ·ç«¯{i}æµ‹è¯•æ•°æ®é›†{data_name}ç¼ºå°‘dataset_config"
                logger.info(f"  - {data_name}: {data_config['dataset_config'].get('name', 'unknown')}")
            
            # æ£€æŸ¥è¯„ä¼°ä»»åŠ¡é…ç½®
            assert 'evaluation' in config, f"å®¢æˆ·ç«¯{i}ç¼ºå°‘evaluationé…ç½®"
            evaluation = config['evaluation']
            assert 'tasks' in evaluation, f"å®¢æˆ·ç«¯{i}è¯„ä¼°é…ç½®ç¼ºå°‘tasks"
            
            tasks = evaluation['tasks']
            assert len(tasks) > 0, f"å®¢æˆ·ç«¯{i}è¯„ä¼°ä»»åŠ¡é…ç½®ä¸ºç©º"
            
            logger.info(f"å®¢æˆ·ç«¯{i}è¯„ä¼°ä»»åŠ¡æ•°é‡: {len(tasks)}")
            for j, task in enumerate(tasks):
                assert 'learner' in task, f"å®¢æˆ·ç«¯{i}è¯„ä¼°ä»»åŠ¡{j}ç¼ºå°‘learner"
                assert 'evaluator' in task, f"å®¢æˆ·ç«¯{i}è¯„ä¼°ä»»åŠ¡{j}ç¼ºå°‘evaluator"
                assert 'test_data' in task, f"å®¢æˆ·ç«¯{i}è¯„ä¼°ä»»åŠ¡{j}ç¼ºå°‘test_data"
                logger.info(f"  - ä»»åŠ¡{j+1}: {task['learner']} -> {task['evaluator']} on {task['test_data']}")


class TestSingleClientEvaluation:
    """å•å®¢æˆ·ç«¯è¯„ä¼°æµ‹è¯•"""
    
    @pytest.fixture
    def single_client_with_evaluation_config(self):
        """å¸¦è¯„ä¼°åŠŸèƒ½çš„å•å®¢æˆ·ç«¯é…ç½®"""
        config = {
            'client': {
                'id': 'eval_test_client',
                'type': 'multi_learner'
            },
            'learners': {
                'default_learner': {
                    'class': 'default',
                    'model': {
                        'type': 'SimpleMLP',
                        'input_size': 784,
                        'hidden_sizes': [128, 64],
                        'num_classes': 10,
                        'dropout_rate': 0.2
                    },
                    'optimizer': {
                        'type': 'SGD',
                        'lr': 0.01,
                        'momentum': 0.9
                    },
                    'dataloader': 'mnist_data',
                    'priority': 0,
                    'enabled': True
                }
            },
            'dataloaders': {
                'mnist_data': {
                    'batch_size': 32,
                    'shuffle': True,
                    'num_workers': 0,
                    'dataset_config': {
                        'name': 'MNIST',
                        'path': 'data/MNIST',
                        'split': 'train',
                        'download': True
                    }
                }
            },
            'test_datas': {
                'mnist_test': {
                    'batch_size': 64,
                    'shuffle': False,
                    'num_workers': 0,
                    'dataset_config': {
                        'name': 'MNIST',
                        'path': 'data/MNIST',
                        'split': 'test',
                        'download': True
                    }
                }
            },
            'evaluators': {
                'accuracy_evaluator': {
                    'class': 'accuracy',
                    'metrics': ['accuracy', 'precision', 'recall', 'f1'],
                    'test_data': 'mnist_test'
                },
                'loss_evaluator': {
                    'class': 'loss',
                    'metrics': ['loss', 'cross_entropy'],
                    'test_data': 'mnist_test'
                }
            },
            'evaluation': {
                'frequency': 1,
                'tasks': [
                    {
                        'learner': 'default_learner',
                        'evaluator': 'accuracy_evaluator',
                        'test_data': 'mnist_test'
                    },
                    {
                        'learner': 'default_learner',
                        'evaluator': 'loss_evaluator',
                        'test_data': 'mnist_test'
                    }
                ]
            },
            'training_plan': {
                'total_epochs': 2,
                'execution_strategy': 'sequential',
                'phases': [
                    {
                        'name': 'default_training',
                        'epochs': [1, 2],
                        'learner': 'default_learner'
                    }
                ]
            },
            'system': {
                'device': 'cpu',
                'random_seed': 42
            }
        }
        return DictConfig(config)
    
    def test_single_client_evaluation_initialization(self, single_client_with_evaluation_config):
        """æµ‹è¯•å•å®¢æˆ·ç«¯è¯„ä¼°åˆå§‹åŒ–"""
        client = MultiLearnerClient.create_from_config(single_client_with_evaluation_config)
        
        assert client.client_id == 'eval_test_client'
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°ç›¸å…³é…ç½®
        config_dict = single_client_with_evaluation_config.to_dict()
        assert 'evaluators' in config_dict
        assert 'test_datas' in config_dict
        assert 'evaluation' in config_dict
        
        logger.info("âœ… å•å®¢æˆ·ç«¯è¯„ä¼°é…ç½®åˆå§‹åŒ–æˆåŠŸ")
    
    def test_single_client_evaluation_data_loading(self, single_client_with_evaluation_config):
        """æµ‹è¯•å•å®¢æˆ·ç«¯è¯„ä¼°æ•°æ®åŠ è½½"""
        client = MultiLearnerClient.create_from_config(single_client_with_evaluation_config)
        
        try:
            # åŠ è½½è®­ç»ƒæ•°æ®
            client._load_multi_learner_data()
            
            # éªŒè¯è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ
            assert len(client.dataloaders) > 0
            assert 'mnist_data' in client.dataloaders
            
            # æ£€æŸ¥æ˜¯å¦èƒ½å¤Ÿè®¿é—®æµ‹è¯•æ•°æ®é…ç½®
            config_dict = single_client_with_evaluation_config.to_dict()
            test_datas = config_dict.get('test_datas', {})
            
            assert 'mnist_test' in test_datas
            logger.info("âœ… å•å®¢æˆ·ç«¯è¯„ä¼°æ•°æ®é…ç½®éªŒè¯æˆåŠŸ")
            
        except Exception as e:
            pytest.skip(f"æ•°æ®åŠ è½½å¤±è´¥ï¼Œå¯èƒ½MNISTæ•°æ®ä¸å­˜åœ¨: {e}")


class TestMultiClientEvaluationExperiment:
    """å¤šå®¢æˆ·ç«¯è¯„ä¼°å®éªŒæµ‹è¯•"""
    
    @pytest.fixture
    def evaluation_experiment_config_dir(self):
        """è¯„ä¼°å®éªŒé…ç½®ç›®å½•"""
        return Path("tests/configs/mnist_real_test")
    
    @pytest.fixture
    def evaluation_log_monitor(self):
        """è¯„ä¼°æ—¥å¿—ç›‘æ§å™¨"""
        log_dir = Path("tests/test_outputs/mnist_evaluation_test/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        monitor = EvaluationLogMonitor(str(log_dir))
        yield monitor
        monitor.stop_monitoring()
    
    def test_evaluation_data_integrity_check(self):
        """æµ‹è¯•è¯„ä¼°æ•°æ®å®Œæ•´æ€§æ£€æŸ¥"""
        import torchvision.datasets as datasets
        
        try:
            # æ£€æŸ¥MNISTæ•°æ®æ˜¯å¦å­˜åœ¨
            train_dataset = datasets.MNIST('data/MNIST', train=True, download=False)
            test_dataset = datasets.MNIST('data/MNIST', train=False, download=False)
            
            assert len(train_dataset) == 60000
            assert len(test_dataset) == 10000
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            sample_data, sample_label = train_dataset[0]
            assert 0 <= sample_label <= 9
            
            logger.info("âœ… è¯„ä¼°æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            
        except Exception as e:
            pytest.skip(f"MNISTæ•°æ®ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯: {e}")
    
    @pytest.mark.slow
    def test_multi_client_evaluation_experiment(self, evaluation_experiment_config_dir, evaluation_log_monitor):
        """æµ‹è¯•å¤šå®¢æˆ·ç«¯è¯„ä¼°å®éªŒæ‰§è¡Œ"""
        # æ¸…ç†ä¹‹å‰çš„è¾“å‡º
        output_dir = Path("tests/test_outputs/mnist_evaluation_test")
        if output_dir.exists():
            import shutil
            shutil.rmtree(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¼€å§‹æ—¥å¿—ç›‘æ§
        evaluation_log_monitor.start_monitoring()
        
        experiment_success = False
        experiment_error = None
        
        try:
            # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
            with FedCLExperiment(str(evaluation_experiment_config_dir)) as experiment:
                logger.info(f"ğŸš€ å¼€å§‹å¤šå®¢æˆ·ç«¯è¯„ä¼°å®éªŒ: {experiment.experiment_id}")
                
                # è®¾ç½®è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰
                def timeout_handler(signum, frame):
                    raise TimeoutError("è¯„ä¼°å®éªŒæ‰§è¡Œè¶…æ—¶")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(300)  # 5åˆ†é’Ÿè¶…æ—¶
                
                try:
                    # æ‰§è¡Œå®éªŒ
                    results = experiment.run()
                    experiment_success = True
                    
                    # éªŒè¯ç»“æœ
                    assert results is not None
                    logger.info(f"âœ… è¯„ä¼°å®éªŒå®Œæˆï¼Œç»“æœ: {results}")
                    
                except TimeoutError:
                    logger.warning("â° è¯„ä¼°å®éªŒæ‰§è¡Œè¶…æ—¶")
                    experiment_error = "timeout"
                    
                finally:
                    signal.alarm(0)  # æ¸…é™¤è¶…æ—¶
                    
        except Exception as e:
            experiment_error = str(e)
            logger.error(f"âŒ è¯„ä¼°å®éªŒæ‰§è¡Œå¤±è´¥: {e}")
        
        # ç­‰å¾…æ—¥å¿—å¤„ç†å®Œæˆ
        time.sleep(2)
        evaluation_log_monitor.stop_monitoring()
        
        # åˆ†æè¯„ä¼°æ—¥å¿—
        log_summary = evaluation_log_monitor.get_summary()
        
        logger.info(f"\nğŸ“Š è¯„ä¼°å®éªŒæ‰§è¡Œæ‘˜è¦:")
        logger.info(f"  - è¯„ä¼°äº‹ä»¶æ•°é‡: {log_summary['evaluation_count']}")
        logger.info(f"  - é”™è¯¯æ•°é‡: {log_summary['error_count']}")
        logger.info(f"  - è­¦å‘Šæ•°é‡: {log_summary['warning_count']}")
        
        # è¾“å‡ºè¯„ä¼°äº‹ä»¶
        if log_summary['evaluation_events']:
            logger.info(f"\nğŸ“Š è¯„ä¼°äº‹ä»¶åˆ—è¡¨:")
            for event in log_summary['evaluation_events'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.info(f"  {event}")
        
        # è¾“å‡ºé”™è¯¯å’Œè­¦å‘Š
        if log_summary['errors']:
            logger.info(f"\nğŸ”´ é”™è¯¯åˆ—è¡¨:")
            for error in log_summary['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"  {error}")
                
        if log_summary['warnings']:
            logger.info(f"\nğŸŸ¡ è­¦å‘Šåˆ—è¡¨:")
            for warning in log_summary['warnings'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"  {warning}")
        
        # æ£€æŸ¥å®éªŒæ˜¯å¦æˆåŠŸ
        if experiment_success:
            logger.info(f"âœ… å¤šå®¢æˆ·ç«¯è¯„ä¼°å®éªŒæˆåŠŸå®Œæˆ")
            
            # éªŒè¯è¯„ä¼°æ˜¯å¦æ­£å¸¸æ‰§è¡Œ
            if log_summary['evaluation_count'] > 0:
                logger.info(f"ğŸ‰ æ£€æµ‹åˆ° {log_summary['evaluation_count']} ä¸ªè¯„ä¼°äº‹ä»¶")
            else:
                logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°è¯„ä¼°äº‹ä»¶ï¼Œå¯èƒ½è¯„ä¼°æœªæ­£å¸¸æ‰§è¡Œ")
                
        else:
            logger.error(f"âŒ å¤šå®¢æˆ·ç«¯è¯„ä¼°å®éªŒæœªèƒ½å®Œæˆ: {experiment_error}")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        output_files = list(output_dir.rglob("*"))
        logger.info(f"\nğŸ“ ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶ ({len(output_files)} ä¸ª):")
        for file in output_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"  {file.relative_to(output_dir)}")
        
        # éªŒè¯åŸºæœ¬è¦æ±‚
        assert log_summary['error_count'] < 10, f"é”™è¯¯è¿‡å¤š: {log_summary['error_count']}"
        
        # å¦‚æœå®éªŒæˆåŠŸï¼ŒéªŒè¯æ˜¯å¦æœ‰è¯„ä¼°ç»“æœ
        if experiment_success:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„è¯„ä¼°ç»“æœéªŒè¯
            pass
    
    def test_evaluation_output_validation(self, evaluation_experiment_config_dir):
        """æµ‹è¯•è¯„ä¼°è¾“å‡ºéªŒè¯"""
        output_dir = Path("tests/test_outputs/mnist_evaluation_test")
        
        if not output_dir.exists():
            pytest.skip("è¯„ä¼°å®éªŒå°šæœªè¿è¡Œï¼Œè·³è¿‡è¾“å‡ºéªŒè¯")
            
        # æ£€æŸ¥åŸºæœ¬è¾“å‡ºç»“æ„
        expected_dirs = ['logs']
        for dir_name in expected_dirs:
            dir_path = output_dir / dir_name
            if dir_path.exists():
                logger.info(f"âœ… æ‰¾åˆ°ç›®å½•: {dir_name}")
            else:
                logger.warning(f"âš ï¸  ç¼ºå°‘ç›®å½•: {dir_name}")
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
        log_files = list((output_dir / "logs").glob("*.log")) if (output_dir / "logs").exists() else []
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶æ•°é‡: {len(log_files)}")
        
        for log_file in log_files:
            logger.info(f"  {log_file.name} ({log_file.stat().st_size} bytes)")
            
            # åˆ†ææ—¥å¿—å†…å®¹ä¸­çš„è¯„ä¼°ä¿¡æ¯
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
                evaluation_mentions = content.lower().count('evaluation')
                accuracy_mentions = content.lower().count('accuracy')
                loss_mentions = content.lower().count('loss')
                
                logger.info(f"    - è¯„ä¼°ç›¸å…³æåŠ: {evaluation_mentions}")
                logger.info(f"    - å‡†ç¡®ç‡ç›¸å…³æåŠ: {accuracy_mentions}")
                logger.info(f"    - æŸå¤±ç›¸å…³æåŠ: {loss_mentions}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®å·¥ä½œç›®å½•
    os.chdir(Path(__file__).parent)
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹å¤šå®¢æˆ·ç«¯è¯„ä¼°æµ‹è¯•")
    logger.info("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-m", "not slow"  # é»˜è®¤ä¸è¿è¡Œè€—æ—¶æµ‹è¯•
    ])


if __name__ == "__main__":
    main()
