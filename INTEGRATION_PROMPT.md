# è”é‚¦å­¦ä¹ ç®—æ³•é›†æˆå®Œæ•´æŒ‡å— - LLM Prompt

## ä»»åŠ¡æ¦‚è¿°

ä½ æ˜¯ä¸€ä½è”é‚¦å­¦ä¹ /æŒç»­å­¦ä¹ /é—å¿˜å­¦ä¹ ç®—æ³•ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç»å…¸ç®—æ³•**å®Œæ•´åœ°**é›†æˆåˆ°æœ¬å¼€æºæ¡†æ¶ä¸­ï¼Œå¹¶ç²¾ç¡®å¤ç°è®ºæ–‡ä¸­çš„å®éªŒç»“æœã€‚

**é‡è¦**ï¼šæœ¬æ¡†æ¶æ”¯æŒä¸‰å¤§ç±»ç®—æ³•ï¼š
- **fl.xxx**: è”é‚¦å­¦ä¹ ç®—æ³• (Federated Learning)
- **cl.xxx**: æŒç»­å­¦ä¹ ç®—æ³• (Continual Learning)
- **ul.xxx**: é—å¿˜å­¦ä¹ ç®—æ³• (Unlearning)

æ‰€æœ‰ç®—æ³•å…±äº«ç»Ÿä¸€çš„å®éªŒç®¡ç†æ•°æ®åº“å’Œè¯„ä¼°ä½“ç³»ã€‚

---

## æ¡†æ¶æ ¸å¿ƒè®¾è®¡ç†å¿µ

### 1. è£…é¥°å™¨é©±åŠ¨çš„ç»„ä»¶æ³¨å†Œç³»ç»Ÿ â­

æœ¬æ¡†æ¶çš„**æ ¸å¿ƒç‰¹æ€§**æ˜¯åŸºäºè£…é¥°å™¨çš„è‡ªåŠ¨æ³¨å†Œç³»ç»Ÿï¼Œæ‰€æœ‰ç»„ä»¶ï¼ˆLearnerã€Modelã€Datasetã€Trainerã€Aggregatorã€Metricï¼‰éƒ½é€šè¿‡è£…é¥°å™¨æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨ä¸­ã€‚

**ä¸ºä»€ä¹ˆä½¿ç”¨è£…é¥°å™¨ï¼Ÿ**
- âœ… **è‡ªåŠ¨å‘ç°**: æ¡†æ¶å¯åŠ¨æ—¶è‡ªåŠ¨æ‰«æå¹¶æ³¨å†Œæ‰€æœ‰ç»„ä»¶ï¼Œæ— éœ€æ‰‹åŠ¨ç»´æŠ¤åˆ—è¡¨
- âœ… **è§£è€¦**: ç®—æ³•å®ç°ä¸æ¡†æ¶æ ¸å¿ƒå®Œå…¨è§£è€¦ï¼Œæ˜“äºæ‰©å±•
- âœ… **ç±»å‹å®‰å…¨**: è£…é¥°å™¨æä¾›å…ƒæ•°æ®éªŒè¯å’Œç±»å‹æ£€æŸ¥
- âœ… **é…ç½®é©±åŠ¨**: é€šè¿‡YAMLé…ç½®æ–‡ä»¶å³å¯åˆ‡æ¢ç®—æ³•ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
- âœ… **ç»Ÿä¸€ç®¡ç†**: æ‰€æœ‰ç®—æ³•ç±»å‹ï¼ˆfl/cl/ulï¼‰ä½¿ç”¨ç›¸åŒçš„æ³¨å†Œæœºåˆ¶

**å…³é”®è£…é¥°å™¨**:
```python
from fedcl.methods.learners._decorators import learner
from fedcl.api.decorators import model, dataset, trainer, aggregator

@learner(namespace='fl', name='FedAvg', description='...')  # æ³¨å†Œå­¦ä¹ å™¨ (å¿…é¡»)
@model(name='ResNet18', ...)         # æ³¨å†Œæ¨¡å‹ (å¯é€‰)
@dataset(name='CIFAR10', ...)        # æ³¨å†Œæ•°æ®é›† (å¯é€‰)
@trainer(name='FLTrainer', ...)      # æ³¨å†Œè®­ç»ƒå™¨ (å¯é€‰)
@aggregator(name='FedAvg', ...)      # æ³¨å†Œèšåˆå™¨ (å¯é€‰)
```

### 2. ä¸šåŠ¡å±‚é€šä¿¡åè®® â­

è”é‚¦å­¦ä¹ çš„æ ¸å¿ƒæ˜¯åˆ†å¸ƒå¼é€šä¿¡ã€‚æ¡†æ¶æä¾›äº†å®Œæ•´çš„ä¸šåŠ¡å±‚é€šä¿¡æŠ½è±¡ï¼š

**é€šä¿¡æµç¨‹**:
```
Server                           Client
  â”‚                                 â”‚
  â”œâ”€â–º broadcast(global_model) â”€â”€â”€â”€â”€â–ºâ”‚  # æœåŠ¡å™¨å¹¿æ’­å…¨å±€æ¨¡å‹
  â”‚                                 â”‚
  â”‚â—„â”€â”€â”€â”€ upload(local_update) â”€â”€â”€â”€â”€â”€â”¤  # å®¢æˆ·ç«¯ä¸Šä¼ æœ¬åœ°æ›´æ–°
  â”‚                                 â”‚
  â”œâ”€â–º aggregate(updates) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  # æœåŠ¡å™¨èšåˆ
  â”‚                                 â”‚
  â””â”€â–º broadcast(new_global_model) â”€â–ºâ”‚  # æ–°ä¸€è½®é€šä¿¡
```

**éœ€è¦å®ç°çš„é€šä¿¡æ¥å£**:
- `get_local_model()` / `get_model()`: å®šä¹‰ä¸Šä¼ å“ªäº›å‚æ•°ï¼ˆå…¨éƒ¨/éƒ¨åˆ†/ç‰¹å¾ï¼‰
- `set_local_model()` / `set_model()`: å®šä¹‰å¦‚ä½•æ¥æ”¶å¹¶æ›´æ–°å‚æ•°
- `aggregate()`: è‡ªå®šä¹‰èšåˆé€»è¾‘ï¼ˆåœ¨Aggregatorä¸­å®ç°ï¼‰

æ³¨ï¼šBaseLearnerå®šä¹‰äº†æŠ½è±¡æ–¹æ³• `get_local_model()`/`set_local_model()`ï¼Œå­ç±»é€šå¸¸å®ç° `get_model()`/`set_model()` å¹¶å§”æ‰˜è°ƒç”¨ã€‚

### 3. å®Œæ•´çš„ç»„ä»¶ç”Ÿæ€ç³»ç»Ÿ

æ¯ä¸ªç®—æ³•ä¸ä»…ä»…æ˜¯ä¸€ä¸ªLearnerï¼Œè€Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼š

```
ç®—æ³•ç³»ç»Ÿ = Learner + Trainer + Model + Aggregator + Dataset + Metrics
         â†“        â†“        â†“         â†“          â†“         â†“
      æ ¸å¿ƒé€»è¾‘  è®­ç»ƒæµç¨‹  ç½‘ç»œç»“æ„  èšåˆç­–ç•¥   æ•°æ®åˆ†åŒº  è¯„ä¼°æŒ‡æ ‡
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆå¤šç»„ä»¶ï¼Ÿ**
- ä¸åŒç®—æ³•çš„è®­ç»ƒæµç¨‹å¯èƒ½å®Œå…¨ä¸åŒï¼ˆåŒæ­¥/å¼‚æ­¥/åŠç›‘ç£ï¼‰
- æ¨¡å‹æ¶æ„å¯èƒ½æœ‰ç‰¹æ®Šè®¾è®¡ï¼ˆä¸ªæ€§åŒ–å±‚/å…±äº«å±‚/åŸå‹ç½‘ç»œï¼‰
- èšåˆç­–ç•¥å„å¼‚ï¼ˆåŠ æƒå¹³å‡/ä¸­ä½æ•°/è‡ªé€‚åº”æƒé‡ï¼‰
- è¯„ä¼°æŒ‡æ ‡éœ€è¦å®šåˆ¶ï¼ˆå‡†ç¡®ç‡/é—å¿˜ç‡/å…¬å¹³æ€§ï¼‰

---

## æ¡†æ¶ç›®å½•ç»“æ„

```
MOE-FedCL/
â”œâ”€â”€ fedcl/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ decorators.py              # ğŸ”‘ æ ¸å¿ƒè£…é¥°å™¨å®šä¹‰
â”‚   â”‚
â”‚   â”œâ”€â”€ methods/
â”‚   â”‚   â”œâ”€â”€ learners/                  # å­¦ä¹ ç®—æ³•å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ _registry.py           # å…¨å±€æ³¨å†Œè¡¨
â”‚   â”‚   â”‚   â”œâ”€â”€ _decorators.py         # @learnerè£…é¥°å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ fl/                    # è”é‚¦å­¦ä¹ : fl.xxx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fedavg.py          # ç¤ºä¾‹: fl.FedAvg
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fedprox.py         # ç¤ºä¾‹: fl.FedProx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ [your_algorithm].py
â”‚   â”‚   â”‚   â”œâ”€â”€ cl/                    # æŒç»­å­¦ä¹ : cl.xxx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ewc.py             # ç¤ºä¾‹: cl.EWC
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ [your_algorithm].py
â”‚   â”‚   â”‚   â””â”€â”€ ul/                    # é—å¿˜å­¦ä¹ : ul.xxx
â”‚   â”‚   â”‚       â”œâ”€â”€ retrain.py         # ç¤ºä¾‹: ul.Retrain
â”‚   â”‚   â”‚       â””â”€â”€ [your_algorithm].py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                    # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ lenet.py               # LeNet-5
â”‚   â”‚   â”‚   â”œâ”€â”€ resnet.py              # ResNetç³»åˆ—
â”‚   â”‚   â”‚   â”œâ”€â”€ vgg.py                 # VGGç³»åˆ—
â”‚   â”‚   â”‚   â””â”€â”€ [algorithm_name]_net.py # ç®—æ³•ç‰¹å®šæ¨¡å‹
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ datasets/                  # æ•°æ®é›†å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ mnist.py               # @datasetè£…é¥°
â”‚   â”‚   â”‚   â”œâ”€â”€ cifar10.py
â”‚   â”‚   â”‚   â””â”€â”€ [dataset_name].py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ trainers/                  # è®­ç»ƒå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ generic.py             # é€šç”¨FLè®­ç»ƒå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ continual.py           # æŒç»­å­¦ä¹ è®­ç»ƒå™¨
â”‚   â”‚   â”‚   â””â”€â”€ [custom_trainer].py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ aggregators/               # èšåˆå™¨ â­ æ–°å¢
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fedavg.py              # åŠ æƒå¹³å‡èšåˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ fedopt.py              # æœåŠ¡å™¨ä¼˜åŒ–å™¨èšåˆ
â”‚   â”‚   â”‚   â””â”€â”€ [custom_aggregator].py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ metrics/                   # è¯„ä¼°æŒ‡æ ‡ â­ æ–°å¢
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ accuracy.py            # å‡†ç¡®ç‡
â”‚   â”‚       â”œâ”€â”€ forgetting.py          # é—å¿˜ç‡ï¼ˆCLä¸“ç”¨ï¼‰
â”‚   â”‚       â””â”€â”€ fairness.py            # å…¬å¹³æ€§ï¼ˆFLä¸“ç”¨ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ federation/                    # è”é‚¦åŸºç¡€è®¾æ–½
â”‚   â”‚   â”œâ”€â”€ server.py                  # æœåŠ¡å™¨é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ client.py                  # å®¢æˆ·ç«¯é€»è¾‘
â”‚   â”‚   â””â”€â”€ communication.py           # é€šä¿¡åè®®
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ distributed/
â”‚       â”œâ”€â”€ base/
â”‚       â”‚   â”œâ”€â”€ server_base.yaml       # æœåŠ¡å™¨åŸºç¡€é…ç½®
â”‚       â”‚   â””â”€â”€ client_base.yaml       # å®¢æˆ·ç«¯åŸºç¡€é…ç½®
â”‚       â””â”€â”€ experiments/
â”‚
â”œâ”€â”€ papers/                            # ğŸ”‘ æ‰€æœ‰è®ºæ–‡å¤ç°è„šæœ¬
â”‚   â”œâ”€â”€ fedavg_mcmahan2017/
â”‚   â”‚   â”œâ”€â”€ reproduce.py               # å¤ç°è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ configs/                   # å®éªŒé…ç½®
â”‚   â”‚   â””â”€â”€ README.md                  # è®ºæ–‡ä¿¡æ¯å’Œç»“æœ
â”‚   â”œâ”€â”€ fedprox_li2020/
â”‚   â”œâ”€â”€ moon_li2021/
â”‚   â””â”€â”€ [paper_name_author_year]/     # ç»Ÿä¸€å‘½åè§„èŒƒ
â”‚
â””â”€â”€ examples/
    â””â”€â”€ smart_batch_runner.py          # æ‰¹é‡å®éªŒè°ƒåº¦å™¨
```

---

## å®Œæ•´é›†æˆæµç¨‹ï¼ˆ8å¤§æ­¥éª¤ï¼‰

### æ­¥éª¤1: æ·±å…¥ç†è§£è®ºæ–‡ç®—æ³•

**å¿…é¡»æå–çš„ä¿¡æ¯**ï¼š

#### 1.1 ç®—æ³•åˆ†ç±»
```
ç®—æ³•ç±»å‹: [ ] fl.xxx (è”é‚¦å­¦ä¹ )
         [ ] cl.xxx (æŒç»­å­¦ä¹ )
         [ ] ul.xxx (é—å¿˜å­¦ä¹ )
```

#### 1.2 æ ¸å¿ƒåˆ›æ–°ç‚¹
- **ä¸€å¥è¯æ€»ç»“**: [ç”¨1å¥è¯æ¦‚æ‹¬ç®—æ³•æ ¸å¿ƒæ€æƒ³]
- **ä¸åŸºçº¿çš„å·®å¼‚**:
  - æ¨¡å‹ç»“æ„: [æ˜¯å¦æœ‰å˜åŒ–ï¼Ÿå¦‚ä½•å˜åŒ–ï¼Ÿ]
  - è®­ç»ƒè¿‡ç¨‹: [æ˜¯å¦æœ‰ç‰¹æ®ŠæŸå¤±/æ­£åˆ™åŒ–/çº¦æŸï¼Ÿ]
  - é€šä¿¡å†…å®¹: [ä¸Šä¼ ä»€ä¹ˆï¼Ÿä¸‹è½½ä»€ä¹ˆï¼Ÿä¸FedAvgæœ‰ä½•ä¸åŒï¼Ÿ]
  - èšåˆç­–ç•¥: [å¦‚ä½•èšåˆï¼Ÿæ˜¯å¦æœ‰è‡ªé€‚åº”æƒé‡ï¼Ÿ]

#### 1.3 å…³é”®ç»„ä»¶éœ€æ±‚åˆ†æ
```python
éœ€è¦è‡ªå®šä¹‰çš„ç»„ä»¶:
[ ] Learner      - æ ¸å¿ƒç®—æ³•é€»è¾‘
[ ] Trainer      - æ˜¯å¦éœ€è¦ç‰¹æ®Šè®­ç»ƒæµç¨‹ï¼Ÿ
[ ] Model        - æ˜¯å¦éœ€è¦ç‰¹æ®Šç½‘ç»œç»“æ„ï¼Ÿ
[ ] Aggregator   - æ˜¯å¦éœ€è¦è‡ªå®šä¹‰èšåˆï¼Ÿ
[ ] Dataset      - æ˜¯å¦éœ€è¦ç‰¹æ®Šæ•°æ®å¤„ç†ï¼Ÿ
[ ] Metric       - æ˜¯å¦éœ€è¦æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Ÿ
```

#### 1.4 è¶…å‚æ•°æå–
```yaml
# ä»è®ºæ–‡Table/Appendixæå–æ‰€æœ‰è¶…å‚æ•°
ç®—æ³•ç‰¹æœ‰å‚æ•°:
  - param_name_1: [é»˜è®¤å€¼] # [è¯´æ˜]
  - param_name_2: [é»˜è®¤å€¼] # [è¯´æ˜]

æ ‡å‡†å‚æ•°:
  - learning_rate: [å€¼]
  - batch_size: [å€¼]
  - local_epochs: [å€¼]
  - communication_rounds: [å€¼]
```

#### 1.5 å®éªŒè®¾ç½®
```yaml
æ•°æ®é›†: [MNIST, CIFAR10, ...]
Non-IIDè®¾ç½®:
  - [Dirichlet(Î±=0.5)]
  - [Pathological(#C=2)]
  - [...]
å®¢æˆ·ç«¯æ•°é‡: [10, 100, ...]
å‚ä¸ç‡: [1.0, 0.1, ...]
```

---

### æ­¥éª¤2: åˆ†æå¼€æºä»£ç ï¼ˆå¦‚æœæœ‰ï¼‰

**å…³é”®ä»£ç ä½ç½®è¯†åˆ«**ï¼š

```python
# 1. å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒé€»è¾‘
def local_train(self, ...):
    # æ‰¾åˆ°è¿™ä¸ªå‡½æ•°ï¼Œåˆ†æ:
    # - æŸå¤±å‡½æ•°çš„ç»„æˆ
    # - æ˜¯å¦æœ‰ç‰¹æ®Šçš„æ­£åˆ™åŒ–é¡¹
    # - æ˜¯å¦ä½¿ç”¨äº†è¾…åŠ©æ¨¡å‹/æ•°æ®ç»“æ„
    pass

# 2. å‚æ•°ä¸Šä¼ é€»è¾‘
def get_params_to_upload(self):
    # ä¸Šä¼ å…¨éƒ¨å‚æ•°ï¼Ÿéƒ¨åˆ†å‚æ•°ï¼Ÿè¿˜æ˜¯å…¶ä»–ä¿¡æ¯ï¼Ÿ
    pass

# 3. å‚æ•°æ›´æ–°é€»è¾‘
def update_from_server(self, params):
    # å¦‚ä½•å¤„ç†æœåŠ¡å™¨ä¸‹å‘çš„å‚æ•°ï¼Ÿ
    # æ˜¯è¦†ç›–ï¼Ÿèåˆï¼Ÿè¿˜æ˜¯åªæ›´æ–°éƒ¨åˆ†å±‚ï¼Ÿ
    pass

# 4. æœåŠ¡å™¨èšåˆé€»è¾‘
def aggregate(self, client_updates):
    # åŠ æƒå¹³å‡ï¼Ÿä¸­ä½æ•°ï¼Ÿè¿˜æ˜¯æ›´å¤æ‚çš„ç­–ç•¥ï¼Ÿ
    pass
```

---

### æ­¥éª¤3: å®ç°Learnerï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰

#### 3.1 åˆ›å»ºæ–‡ä»¶
```bash
# æ ¹æ®ç®—æ³•ç±»å‹é€‰æ‹©ç›®å½•
fedcl/methods/learners/fl/[algorithm_name].py   # è”é‚¦å­¦ä¹ 
fedcl/methods/learners/cl/[algorithm_name].py   # æŒç»­å­¦ä¹ 
fedcl/methods/learners/ul/[algorithm_name].py   # é—å¿˜å­¦ä¹ 
```

#### 3.2 å®Œæ•´Learneræ¨¡æ¿

```python
"""
[ç®—æ³•å…¨å] ([ç®—æ³•ç®€ç§°])

è®ºæ–‡: [æ ‡é¢˜]
ä½œè€…: [ä½œè€…åˆ—è¡¨]
ä¼šè®®/æœŸåˆŠ: [venue, year]
é“¾æ¥: [arXiv/DOI]

æ ¸å¿ƒæ€æƒ³:
    [2-3å¥è¯æè¿°ç®—æ³•çš„æ ¸å¿ƒåˆ›æ–°ç‚¹]

å…³é”®ç‰¹æ€§:
    1. [ç‰¹æ€§1]
    2. [ç‰¹æ€§2]
    3. [ç‰¹æ€§3]

ä¸FedAvgçš„ä¸»è¦åŒºåˆ«:
    - [åŒºåˆ«1]
    - [åŒºåˆ«2]
"""
from typing import Dict, Any, Optional, Tuple, List
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import copy

from fedcl.methods.learners._decorators import learner
from fedcl.learner.base_learner import BaseLearner  # åŸºç±»


@learner(
    namespace='[fl|cl|ul]',  # å‘½åç©ºé—´: fl(è”é‚¦å­¦ä¹ ), cl(æŒç»­å­¦ä¹ ), ul(é—å¿˜å­¦ä¹ )
    name='[ç®—æ³•å]',          # ä¾‹å¦‚: 'FedAvg', 'MOON', 'TARGET'
    description='[ä¸€å¥è¯æè¿°ï¼ˆå¯é€‰ï¼‰]'  # ä¾‹å¦‚: 'FedAvg: Federated Averaging'
)
class [AlgorithmName]Learner(BaseLearner):  # ç»§æ‰¿BaseLearneråŸºç±»
    """
    [ç®—æ³•åç§°] Learnerå®ç°

    å‚æ•°è¯´æ˜:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        device: è®­ç»ƒè®¾å¤‡
        learning_rate: å­¦ä¹ ç‡
        local_epochs: æœ¬åœ°è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°

        # ç®—æ³•ç‰¹æœ‰å‚æ•°ï¼ˆä»¥MOONä¸ºä¾‹ï¼‰
        mu: å¯¹æ¯”æŸå¤±æƒé‡ï¼ˆé»˜è®¤: 1.0ï¼‰
        temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦ï¼ˆé»˜è®¤: 0.5ï¼‰

    é€šä¿¡åè®®:
        ä¸Šä¼ : [æè¿°ä¸Šä¼ ä»€ä¹ˆï¼Œå¦‚: æ¨¡å‹å‚æ•°, ç‰¹å¾åŸå‹, ç»Ÿè®¡ä¿¡æ¯ç­‰]
        ä¸‹è½½: [æè¿°ä¸‹è½½ä»€ä¹ˆï¼Œå¦‚: å…¨å±€æ¨¡å‹, èšåˆç‰¹å¾ç­‰]

    ç¤ºä¾‹:
        >>> learner = [AlgorithmName]Learner(
        ...     model=model,
        ...     device='cuda',
        ...     learning_rate=0.01,
        ...     mu=1.0
        ... )
        >>> results = learner.local_train(train_loader)
    """

    def __init__(self, client_id: str, config: Dict[str, Any] = None, lazy_init: bool = True):
        """
        åˆå§‹åŒ–å­¦ä¹ å™¨

        Args:
            client_id: å®¢æˆ·ç«¯å”¯ä¸€æ ‡è¯†
            config: ç»„ä»¶é…ç½®å­—å…¸ï¼ˆç”±æ¡†æ¶ä¼ å…¥ï¼‰
            lazy_init: æ˜¯å¦å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶ï¼ˆé»˜è®¤Trueï¼‰
        """
        # æå–learneré…ç½®
        learner_params = (config or {}).get('learner', {}).get('params', {})

        # æå–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°é…ç½®
        self._model_cfg = learner_params.get('model', {})
        self._optimizer_cfg = learner_params.get('optimizer', {
            'type': 'SGD',
            'lr': learner_params.get('learning_rate', 0.01),
            'momentum': 0.9
        })
        self._loss_cfg = learner_params.get('loss', 'CrossEntropyLoss')

        # æ ‡å‡†è®­ç»ƒå‚æ•°
        self._lr = learner_params.get('learning_rate', 0.01)
        self._bs = learner_params.get('batch_size', 32)
        self._epochs = learner_params.get('local_epochs', 5)

        # ç®—æ³•ç‰¹æœ‰å‚æ•°ï¼ˆä»learner_paramsä¸­æå–ï¼‰
        self.special_param_1 = learner_params.get('special_param_1', 1.0)
        self.special_param_2 = learner_params.get('special_param_2', 10)

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(client_id, config, lazy_init)

        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç»„ä»¶å ä½ç¬¦ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._model = None
        self._optimizer = None
        self._criterion = None
        self._train_loader = None

        # åˆå§‹åŒ–ç®—æ³•ç‰¹æœ‰çš„æ•°æ®ç»“æ„
        self.global_model = None      # ç”¨äºä¿å­˜å…¨å±€æ¨¡å‹å‰¯æœ¬
        self.prev_model = None         # ç”¨äºä¿å­˜ä¸Šä¸€è½®æ¨¡å‹
        self.prototypes = None         # ç±»åŸå‹ï¼ˆå¦‚æœéœ€è¦ï¼‰

        self.logger.info(
            f"{self.__class__.__name__} {client_id} åˆå§‹åŒ–å®Œæˆ: "
            f"model={self._model_cfg.get('name')}, "
            f"special_param_1={self.special_param_1}"
        )

    # ä½¿ç”¨@propertyå®ç°å»¶è¿ŸåŠ è½½
    @property
    def model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model is None:
            from fedcl.api.registry import registry
            model_name = self._model_cfg['name']
            model_params = self._model_cfg.get('params', {})
            model_class = registry.get_model(model_name)
            self._model = model_class(**model_params).to(self.device)
            self.logger.debug(f"æ¨¡å‹ {model_name} åˆ›å»ºå®Œæˆ")
        return self._model

    @property
    def optimizer(self):
        """å»¶è¿ŸåŠ è½½ä¼˜åŒ–å™¨"""
        if self._optimizer is None:
            opt_type = self._optimizer_cfg.get('type', 'SGD').upper()
            lr = self._optimizer_cfg.get('lr', self._lr)

            if opt_type == 'SGD':
                self._optimizer = optim.SGD(
                    self.model.parameters(),
                    lr=lr,
                    momentum=self._optimizer_cfg.get('momentum', 0.9)
                )
            elif opt_type == 'ADAM':
                self._optimizer = optim.Adam(
                    self.model.parameters(),
                    lr=lr
                )
            self.logger.debug(f"ä¼˜åŒ–å™¨ {opt_type} åˆ›å»ºå®Œæˆ")
        return self._optimizer

    @property
    def criterion(self):
        """å»¶è¿ŸåŠ è½½æŸå¤±å‡½æ•°"""
        if self._criterion is None:
            if isinstance(self._loss_cfg, str):
                if self._loss_cfg == 'CrossEntropyLoss':
                    self._criterion = nn.CrossEntropyLoss()
                elif self._loss_cfg == 'MSELoss':
                    self._criterion = nn.MSELoss()
            self.logger.debug("æŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
        return self._criterion

    @property
    def train_loader(self):
        """å»¶è¿ŸåŠ è½½æ•°æ®åŠ è½½å™¨"""
        if self._train_loader is None:
            dataset = self.dataset  # ä»BaseLearnerç»§æ‰¿
            self._train_loader = DataLoader(
                dataset,
                batch_size=self._bs,
                shuffle=True
            )
            self.logger.debug(f"DataLoaderåˆ›å»ºå®Œæˆ: batch_size={self._bs}")
        return self._train_loader

    def local_train(
        self,
        train_loader: DataLoader,
        current_round: int = 0,
        **kwargs
    ) -> Dict[str, Any]:
        """
        æœ¬åœ°è®­ç»ƒå‡½æ•° - ç®—æ³•æ ¸å¿ƒå®ç°

        æ³¨æ„ï¼šå®é™…æ¡†æ¶ä¸­ä½¿ç”¨ async def train()ï¼Œè¿™é‡Œä¸ºäº†ç®€åŒ–ç¤ºä¾‹ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            current_round: å½“å‰é€šä¿¡è½®æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            è®­ç»ƒç»“æœå­—å…¸:
                - loss: å¹³å‡æŸå¤±
                - accuracy: è®­ç»ƒå‡†ç¡®ç‡
                - num_samples: è®­ç»ƒæ ·æœ¬æ•°
                - [å…¶ä»–è‡ªå®šä¹‰æŒ‡æ ‡]
        """
        self.model.train()

        # ç»Ÿè®¡ä¿¡æ¯
        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        # ç®—æ³•ç‰¹æœ‰çš„ç»Ÿè®¡ï¼ˆä¾‹å¦‚ï¼šå¯¹æ¯”æŸå¤±ã€æ­£åˆ™åŒ–æŸå¤±ç­‰ï¼‰
        total_ce_loss = 0.0
        total_special_loss = 0.0

        for epoch in range(self.local_epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                data = data.to(self.device)
                target = target.to(self.device)

                self.optimizer.zero_grad()

                # ====== æ ¸å¿ƒï¼šç®—æ³•ç‰¹æœ‰çš„å‰å‘ä¼ æ’­ ======
                output = self.model(data)

                # åŸºç¡€åˆ†ç±»æŸå¤±
                ce_loss = self.criterion(output, target)

                # ç®—æ³•ç‰¹æœ‰çš„æŸå¤±é¡¹ï¼ˆæ ¹æ®ç®—æ³•æ·»åŠ ï¼‰
                special_loss = self._compute_special_loss(
                    output=output,
                    data=data,
                    target=target,
                    # ä¼ å…¥éœ€è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                )

                # æ€»æŸå¤±
                loss = ce_loss + self.special_param_1 * special_loss

                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()

                # ç»Ÿè®¡
                total_loss += loss.item() * data.size(0)
                total_ce_loss += ce_loss.item() * data.size(0)
                total_special_loss += special_loss.item() * data.size(0)

                pred = output.argmax(dim=1, keepdim=True)
                total_correct += pred.eq(target.view_as(pred)).sum().item()
                total_samples += data.size(0)

        # è¿”å›è¯¦ç»†çš„è®­ç»ƒç»“æœ
        return {
            'loss': total_loss / total_samples,
            'ce_loss': total_ce_loss / total_samples,
            'special_loss': total_special_loss / total_samples,
            'accuracy': total_correct / total_samples,
            'num_samples': total_samples,
            'current_round': current_round,
        }

    def _compute_special_loss(
        self,
        output: torch.Tensor,
        data: torch.Tensor,
        target: torch.Tensor,
    ) -> torch.Tensor:
        """
        è®¡ç®—ç®—æ³•ç‰¹æœ‰çš„æŸå¤±é¡¹

        ç¤ºä¾‹ï¼ˆæ ¹æ®ç®—æ³•ç±»å‹é€‰æ‹©ï¼‰:
            - FedProx: proximal term
            - MOON: contrastive loss
            - EWC: Fisher regularization
            - ...

        Args:
            output: æ¨¡å‹è¾“å‡º
            data: è¾“å…¥æ•°æ®
            target: æ ‡ç­¾

        Returns:
            æŸå¤±å¼ é‡
        """
        # ç¤ºä¾‹ï¼šå¯¹æ¯”æŸå¤±ï¼ˆMOONç±»ç®—æ³•ï¼‰
        if self.global_model is not None and self.prev_model is not None:
            # æå–ç‰¹å¾
            with torch.no_grad():
                global_features = self.global_model.get_features(data)
                prev_features = self.prev_model.get_features(data)

            current_features = self.model.get_features(data)

            # è®¡ç®—å¯¹æ¯”æŸå¤±
            # ...ï¼ˆå…·ä½“å®ç°ï¼‰

            return contrastive_loss

        return torch.tensor(0.0, device=self.device)

    # ====== é€šä¿¡åè®®å®ç° ======

    async def get_model(self) -> Dict[str, Any]:
        """
        è·å–éœ€è¦ä¸Šä¼ åˆ°æœåŠ¡å™¨çš„æ¨¡å‹æ•°æ®

        ä¸åŒç®—æ³•ä¸Šä¼ ä¸åŒå†…å®¹:
            - FedAvg: æ‰€æœ‰æ¨¡å‹å‚æ•°
            - FedPer: åªä¸Šä¼ baseå±‚å‚æ•°
            - FedProto: ä¸Šä¼ ç±»åŸå‹
            - ...

        Returns:
            åŒ…å«æ¨¡å‹å‚æ•°å’Œå…ƒæ•°æ®çš„å­—å…¸:
            {
                'model_type': str,
                'parameters': {'weights': Dict[str, torch.Tensor]},
                'metadata': {...}
            }
        """
        # ç¤ºä¾‹ï¼šä¸Šä¼ æ‰€æœ‰å‚æ•°
        weights = {
            name: param.detach().cpu().clone()
            for name, param in self.model.named_parameters()
        }

        # å¦‚æœåªä¸Šä¼ éƒ¨åˆ†å‚æ•°ï¼ˆä¾‹å¦‚FedPerï¼‰
        # weights = {
        #     name: param.detach().cpu().clone()
        #     for name, param in self.model.named_parameters()
        #     if 'base' in name  # åªä¸Šä¼ baseå±‚
        # }

        return {
            'model_type': self._model_cfg['name'],
            'parameters': {'weights': weights},
            'metadata': {
                'client_id': self.client_id,
                'samples': len(self.dataset),
            }
        }

    async def set_model(self, model_data: Dict[str, Any]) -> bool:
        """
        æ¥æ”¶å¹¶è®¾ç½®æœåŠ¡å™¨ä¸‹å‘çš„æ¨¡å‹å‚æ•°

        Args:
            model_data: æœåŠ¡å™¨èšåˆåçš„æ¨¡å‹æ•°æ®

        Returns:
            bool: è®¾ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            if 'parameters' in model_data and 'weights' in model_data['parameters']:
                weights = model_data['parameters']['weights']

                # æ›´æ–°æ¨¡å‹å‚æ•°
                state_dict = self.model.state_dict()
                for name, value in weights.items():
                    if name in state_dict:
                        if not isinstance(value, torch.Tensor):
                            value = torch.from_numpy(value)
                        state_dict[name] = value.to(self.device)

                self.model.load_state_dict(state_dict, strict=True)

                # å¦‚æœåªæ›´æ–°éƒ¨åˆ†å±‚ï¼ˆä¾‹å¦‚FedPerï¼‰
                # for name, value in weights.items():
                #     if name in state_dict and 'base' in name:
                #         state_dict[name] = value.to(self.device)

                return True
        except Exception as e:
            self.logger.exception(f"Failed to set model: {e}")
        return False

    async def get_local_model(self) -> Dict[str, Any]:
        """BaseLearneræŠ½è±¡æ–¹æ³• - å§”æ‰˜ç»™get_model()"""
        return await self.get_model()

    async def set_local_model(self, model_data: Dict[str, Any]) -> bool:
        """BaseLearneræŠ½è±¡æ–¹æ³• - å§”æ‰˜ç»™set_model()"""
        return await self.set_model(model_data)

    # ====== è¯„ä¼°ç›¸å…³ ======

    async def evaluate(
        self,
        evaluation_params: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        æœ¬åœ°æµ‹è¯•/è¯„ä¼°ï¼ˆBaseLearneræŠ½è±¡æ–¹æ³•ï¼‰

        æ³¨æ„ï¼šå®é™…æ¡†æ¶ä¸­ä½¿ç”¨ async def evaluate()ï¼Œ
        é€šå¸¸ä½¿ç”¨ self.train_loader æˆ–ä» evaluation_params è·å–æµ‹è¯•æ•°æ®

        Args:
            evaluation_params: è¯„ä¼°å‚æ•°ï¼Œå¯èƒ½åŒ…å«æµ‹è¯•æ¨¡å‹ç­‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.model.eval()

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        # ç”¨äºè®¡ç®—æ›´å¤šæŒ‡æ ‡
        all_preds = []
        all_targets = []

        with torch.no_grad():
            # é€šå¸¸ä½¿ç”¨ self.train_loader è¿›è¡Œè¯„ä¼°ï¼ˆæˆ–ä¸“é—¨çš„test_loaderï¼‰
            for data, target in self.train_loader:
                data = data.to(self.device)
                target = target.to(self.device)

                output = self.model(data)
                loss = self.criterion(output, target)

                total_loss += loss.item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                total_correct += pred.eq(target.view_as(pred)).sum().item()
                total_samples += data.size(0)

                all_preds.extend(pred.cpu().numpy())
                all_targets.extend(target.cpu().numpy())

        return {
            'loss': total_loss / total_samples,
            'accuracy': total_correct / total_samples,
            'samples': total_samples,
        }

    # ====== è¾…åŠ©æ–¹æ³• ======

    def save_checkpoint(self, path: str) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'special_param_1': self.special_param_1,
            'special_param_2': self.special_param_2,
            # ä¿å­˜ç®—æ³•ç‰¹æœ‰çš„çŠ¶æ€
        }
        torch.save(checkpoint, path)

    def load_checkpoint(self, path: str) -> None:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.special_param_1 = checkpoint['special_param_1']
        self.special_param_2 = checkpoint['special_param_2']
```

---

### æ­¥éª¤4: å®ç°Trainerï¼ˆè®­ç»ƒæµç¨‹æ§åˆ¶å™¨ï¼‰

**ä½•æ—¶éœ€è¦è‡ªå®šä¹‰Trainerï¼Ÿ**
- è®­ç»ƒæµç¨‹ä¸æ ‡å‡†FLæµç¨‹ä¸åŒï¼ˆå¦‚åŠç›‘ç£ã€å…ƒå­¦ä¹ ã€è¯¾ç¨‹å­¦ä¹ ï¼‰
- éœ€è¦ç‰¹æ®Šçš„é€šä¿¡æ¨¡å¼ï¼ˆå¦‚å¼‚æ­¥ã€åˆ†å±‚ï¼‰
- éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ç­–ç•¥

#### 4.1 åˆ›å»ºTrainer

```python
# fedcl/methods/trainers/[algorithm_name]_trainer.py

"""
[ç®—æ³•å] ä¸“ç”¨è®­ç»ƒå™¨

ç”¨äºå¤„ç†ç‰¹æ®Šçš„è®­ç»ƒæµç¨‹ï¼Œä¾‹å¦‚:
    - å¼‚æ­¥æ›´æ–°
    - åŠç›‘ç£å­¦ä¹ 
    - å…ƒå­¦ä¹ è®­ç»ƒå¾ªç¯
"""
from typing import Dict, List, Any
from fedcl.api.decorators import trainer
from fedcl.methods.trainers.generic import GenericFLTrainer


@trainer(
    name='[AlgorithmName]Trainer',
    trainer_type='[federated|continual|unlearning]',
    description='[æè¿°ç‰¹æ®Šçš„è®­ç»ƒæµç¨‹]'
)
class [AlgorithmName]Trainer(GenericFLTrainer):
    """
    [ç®—æ³•å] è®­ç»ƒå™¨

    ç‰¹æ®ŠåŠŸèƒ½:
        - [åŠŸèƒ½1]
        - [åŠŸèƒ½2]
    """

    def train_round(
        self,
        round_idx: int,
        selected_clients: List[int],
        **kwargs
    ) -> Dict[str, Any]:
        """
        å•è½®è®­ç»ƒæµç¨‹

        å¯ä»¥è¦†ç›–è¿™ä¸ªæ–¹æ³•æ¥å®ç°ç‰¹æ®Šçš„è®­ç»ƒé€»è¾‘
        """
        # è‡ªå®šä¹‰è®­ç»ƒæµç¨‹
        # ...

        return results
```

---

### æ­¥éª¤5: å®ç°Modelï¼ˆç½‘ç»œç»“æ„ï¼‰

**ä½•æ—¶éœ€è¦è‡ªå®šä¹‰Modelï¼Ÿ**
- æ¨¡å‹æœ‰ç‰¹æ®Šç»“æ„ï¼ˆå¦‚åˆ†ç¦»çš„å…¨å±€å±‚/ä¸ªæ€§åŒ–å±‚ï¼‰
- éœ€è¦è¿”å›ä¸­é—´ç‰¹å¾ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ã€è’¸é¦ç­‰ï¼‰
- éœ€è¦ç‰¹æ®Šçš„åˆå§‹åŒ–æ–¹å¼

#### 5.1 åˆ›å»ºModel

```python
# fedcl/methods/models/[model_name].py

"""
[æ¨¡å‹åç§°]

è®ºæ–‡: [å¦‚æœæ˜¯è®ºæ–‡ç‰¹å®šæ¨¡å‹ï¼Œæ³¨æ˜è®ºæ–‡]
ç”¨é€”: [æè¿°æ¨¡å‹çš„ç”¨é€”å’Œç‰¹ç‚¹]
"""
import torch
import torch.nn as nn
from fedcl.api.decorators import model


@model(
    name='[ModelName]',
    model_type='[cnn|mlp|transformer|...]',
    description='[æ¨¡å‹æè¿°]',
    input_shape=(3, 32, 32),  # ç¤ºä¾‹
    num_classes=10,            # ç¤ºä¾‹
)
class [ModelName](nn.Module):
    """
    [æ¨¡å‹åç§°]

    å‚æ•°:
        num_classes: åˆ†ç±»æ•°é‡
        feature_dim: ç‰¹å¾ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰

    Forward:
        æ”¯æŒè¿”å›ä¸­é—´ç‰¹å¾ç”¨äºç‰¹æ®Šç®—æ³•
    """

    def __init__(
        self,
        num_classes: int = 10,
        feature_dim: int = 128,
    ):
        super().__init__()

        # å¦‚æœæ˜¯ä¸ªæ€§åŒ–ç®—æ³•ï¼Œæ˜ç¡®åŒºåˆ†å…¨å±€å±‚å’Œä¸ªæ€§åŒ–å±‚
        self.base = nn.Sequential(
            # å…¨å±€å…±äº«å±‚
            # ...
        )

        self.head = nn.Sequential(
            # ä¸ªæ€§åŒ–å±‚
            # ...
        )

    def forward(
        self,
        x: torch.Tensor,
        return_features: bool = False
    ):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡
            return_features: æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾

        Returns:
            å¦‚æœreturn_features=False: åˆ†ç±»logits
            å¦‚æœreturn_features=True: (logits, features)
        """
        features = self.base(x)
        logits = self.head(features)

        if return_features:
            return logits, features
        return logits

    def get_base_params(self):
        """è·å–å…¨å±€å±‚å‚æ•°ï¼ˆç”¨äºéƒ¨åˆ†å‚æ•°èšåˆï¼‰"""
        return self.base.parameters()

    def get_head_params(self):
        """è·å–ä¸ªæ€§åŒ–å±‚å‚æ•°"""
        return self.head.parameters()
```

---

### æ­¥éª¤6: å®ç°Aggregatorï¼ˆèšåˆç­–ç•¥ï¼‰

**ä½•æ—¶éœ€è¦è‡ªå®šä¹‰Aggregatorï¼Ÿ**
- èšåˆç­–ç•¥ä¸æ˜¯ç®€å•çš„åŠ æƒå¹³å‡ï¼ˆå¦‚ä¸­ä½æ•°ã€ä¿®å‰ªå‡å€¼ã€è‡ªé€‚åº”æƒé‡ï¼‰
- éœ€è¦ä½¿ç”¨æœåŠ¡å™¨ç«¯ä¼˜åŒ–å™¨ï¼ˆå¦‚FedOptã€FedAdamï¼‰
- éœ€è¦è¿‡æ»¤æˆ–è°ƒæ•´å®¢æˆ·ç«¯ä¸Šä¼ çš„å†…å®¹

#### 6.1 åˆ›å»ºAggregator

```python
# fedcl/methods/aggregators/[aggregator_name].py

"""
[èšåˆå™¨åç§°]

ç”¨äº: [æè¿°èšåˆç­–ç•¥]
è®ºæ–‡: [å¦‚æœæ¥è‡ªç‰¹å®šè®ºæ–‡]
"""
from typing import Dict, List
import torch
from fedcl.api.decorators import aggregator


@aggregator(
    name='[AggregatorName]',
    aggregator_type='[weighted_avg|median|adaptive|...]',
    description='[æè¿°]'
)
class [AggregatorName]:
    """
    [èšåˆå™¨åç§°]

    èšåˆç­–ç•¥:
        [æè¿°å…·ä½“çš„èšåˆé€»è¾‘]

    å‚æ•°:
        ç‰¹å®šèšåˆå™¨çš„å‚æ•°
    """

    def __init__(
        self,
        # èšåˆå™¨ç‰¹æœ‰å‚æ•°
        **kwargs
    ):
        self.kwargs = kwargs

    def aggregate(
        self,
        client_params_list: List[Dict[str, torch.Tensor]],
        client_weights: List[float],
        **kwargs
    ) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œèšåˆ

        Args:
            client_params_list: å®¢æˆ·ç«¯å‚æ•°åˆ—è¡¨
            client_weights: å®¢æˆ·ç«¯æƒé‡ï¼ˆé€šå¸¸åŸºäºæ•°æ®é‡ï¼‰

        Returns:
            èšåˆåçš„å…¨å±€å‚æ•°
        """
        # å®ç°èšåˆé€»è¾‘
        # ...

        return aggregated_params
```

---

### æ­¥éª¤7: å®ç°Datasetï¼ˆæ•°æ®é›†ï¼‰

**ä½•æ—¶éœ€è¦è‡ªå®šä¹‰Datasetï¼Ÿ**
- ä½¿ç”¨æ–°çš„æ•°æ®é›†ï¼ˆæ¡†æ¶ä¸­ä¸å­˜åœ¨ï¼‰
- éœ€è¦ç‰¹æ®Šçš„æ•°æ®é¢„å¤„ç†
- éœ€è¦ç‰¹æ®Šçš„æ•°æ®åˆ†åŒºç­–ç•¥

#### 7.1 åˆ›å»ºDataset

```python
# fedcl/methods/datasets/[dataset_name].py

"""
[æ•°æ®é›†åç§°]

æ•°æ®æ¥æº: [URLæˆ–æè¿°]
ä»»åŠ¡ç±»å‹: [åˆ†ç±»/å›å½’/...]
"""
from typing import Dict, Any
import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from fedcl.api.decorators import dataset
from fedcl.methods.datasets.base import FederatedDataset


@dataset(
    name='[DatasetName]',
    dataset_type='[image_classification|text_classification|...]',
    num_classes=10,
    input_shape=(3, 32, 32),
    download_url='[å¦‚æœå¯ä»¥è‡ªåŠ¨ä¸‹è½½]'
)
class [DatasetName]FederatedDataset(FederatedDataset):
    """
    [æ•°æ®é›†åç§°]

    ç»Ÿè®¡ä¿¡æ¯:
        - è®­ç»ƒé›†å¤§å°: [æ•°é‡]
        - æµ‹è¯•é›†å¤§å°: [æ•°é‡]
        - ç±»åˆ«æ•°: [æ•°é‡]
        - è¾“å…¥å½¢çŠ¶: [å½¢çŠ¶]

    æ•°æ®åˆ†åŒº:
        æ”¯æŒçš„Non-IIDè®¾ç½®:
            - iid: ç‹¬ç«‹åŒåˆ†å¸ƒ
            - dirichlet: Dirichletåˆ†å¸ƒ
            - pathological: ç—…ç†æ€§Non-IID
    """

    def __init__(
        self,
        root: str = './data',
        train: bool = True,
        download: bool = True,
        **kwargs
    ):
        super().__init__(root, train, download)

        # æ•°æ®è½¬æ¢
        if train:
            transform = transforms.Compose([
                # è®­ç»ƒé›†æ•°æ®å¢å¼º
                transforms.RandomCrop(32, padding=4),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((...), (...)),
            ])
        else:
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((...), (...)),
            ])

        # åŠ è½½æ•°æ®
        self.dataset = self._load_dataset(root, train, transform, download)

        # è®¾ç½®å±æ€§
        self.num_classes = 10
        self.input_shape = (3, 32, 32)

    def _load_dataset(self, root, train, transform, download):
        """åŠ è½½åŸå§‹æ•°æ®é›†"""
        # å®ç°æ•°æ®åŠ è½½é€»è¾‘
        # ...
        pass

    def get_statistics(self) -> Dict[str, Any]:
        """è¿”å›æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'dataset_name': '[DatasetName]',
            'num_samples': len(self.dataset),
            'num_classes': self.num_classes,
            'input_shape': self.input_shape,
            'train': self.train,
        }
```

---

### æ­¥éª¤8: å®ç°Metricsï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰

**ä½•æ—¶éœ€è¦è‡ªå®šä¹‰Metricï¼Ÿ**
- éœ€è¦ç®—æ³•ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚é—å¿˜ç‡ã€å…¬å¹³æ€§ï¼‰
- éœ€è¦å¤šä»»åŠ¡è¯„ä¼°
- éœ€è¦å¤æ‚çš„ç»Ÿè®¡åˆ†æ

#### 8.1 åˆ›å»ºMetric

```python
# fedcl/methods/metrics/[metric_name].py

"""
[æŒ‡æ ‡åç§°]

ç”¨äº: [æè¿°æŒ‡æ ‡çš„ç”¨é€”]
è®¡ç®—æ–¹å¼: [æè¿°è®¡ç®—æ–¹æ³•]
"""
from typing import List, Dict, Any
import numpy as np
from fedcl.api.decorators import metric


@metric(
    name='[MetricName]',
    metric_type='[classification|fairness|forgetting|...]',
    description='[æè¿°]'
)
class [MetricName]:
    """
    [æŒ‡æ ‡åç§°]

    è®¡ç®—æ–¹æ³•:
        [è¯¦ç»†æè¿°è®¡ç®—å…¬å¼]

    ç”¨ä¾‹:
        >>> metric = [MetricName]()
        >>> score = metric.compute(predictions, targets)
    """

    def __init__(self, **kwargs):
        self.kwargs = kwargs

    def compute(
        self,
        predictions: List,
        targets: List,
        **kwargs
    ) -> Dict[str, float]:
        """
        è®¡ç®—æŒ‡æ ‡

        Args:
            predictions: é¢„æµ‹ç»“æœ
            targets: çœŸå®æ ‡ç­¾

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        # å®ç°æŒ‡æ ‡è®¡ç®—
        # ...

        return {
            '[metric_name]': score
        }
```

---

### æ­¥éª¤9: æ³¨å†Œæ‰€æœ‰ç»„ä»¶

#### 9.1 æ›´æ–° `__init__.py`

```python
# fedcl/methods/learners/__init__.py
from fedcl.methods.learners.fl.[algorithm_name] import [AlgorithmName]Learner

__all__ = [
    # ... ç°æœ‰çš„
    '[AlgorithmName]Learner',
]

# fedcl/methods/models/__init__.py
from fedcl.methods.models.[model_name] import [ModelName]

__all__ = [
    # ... ç°æœ‰çš„
    '[ModelName]',
]

# ç±»ä¼¼åœ°æ›´æ–°å…¶ä»–ç»„ä»¶çš„ __init__.py
```

---

### æ­¥éª¤10: åˆ›å»ºå®éªŒé…ç½®

#### 10.1 ç›®å½•ç»“æ„
```
configs/distributed/experiments/[algorithm_name]/
â”œâ”€â”€ server.yaml          # æœåŠ¡å™¨é…ç½®
â”œâ”€â”€ client_0.yaml        # å®¢æˆ·ç«¯0é…ç½®
â”œâ”€â”€ client_1.yaml        # å®¢æˆ·ç«¯1é…ç½®
â”œâ”€â”€ ...
â””â”€â”€ client_9.yaml        # å®¢æˆ·ç«¯9é…ç½®
```

#### 10.2 æœåŠ¡å™¨é…ç½®æ¨¡æ¿

```yaml
# configs/distributed/experiments/[algorithm_name]/server.yaml

extends: "../../base/server_base.yaml"

# è”é‚¦é…ç½®
federation:
  aggregation:
    method: "[AggregatorName]"  # ä½¿ç”¨æ³¨å†Œçš„èšåˆå™¨åç§°
    params:
      # èšåˆå™¨ç‰¹æœ‰å‚æ•°
      adaptive_weight: true

# è®­ç»ƒé…ç½®
training:
  rounds: 100              # é€šä¿¡è½®æ•°ï¼ˆä»è®ºæ–‡è·å–ï¼‰
  sample_ratio: 1.0        # æ¯è½®å‚ä¸ç‡

  # å¦‚æœéœ€è¦è‡ªå®šä¹‰è®­ç»ƒå™¨
  trainer:
    name: "[AlgorithmName]Trainer"
    params:
      # è®­ç»ƒå™¨ç‰¹æœ‰å‚æ•°

# æ—¥å¿—å’Œæ£€æŸ¥ç‚¹
logging:
  log_level: "INFO"
  save_checkpoints: true
  checkpoint_freq: 10
```

#### 10.3 å®¢æˆ·ç«¯é…ç½®æ¨¡æ¿

```yaml
# configs/distributed/experiments/[algorithm_name]/client_0.yaml

extends: "../../base/client_base.yaml"

node_id: "client_0"

# è®­ç»ƒé…ç½®
training:
  learner:
    name: "[namespace].[ç®—æ³•å]"  # ä¾‹å¦‚: "fl.FedAvg", "cl.TARGET", "ul.SISA"
    params:
      client_index: 0

      # æ ‡å‡†å‚æ•°
      batch_size: 32
      local_epochs: 5
      learning_rate: 0.01
      momentum: 0.9
      weight_decay: 0.0001

      # ç®—æ³•ç‰¹æœ‰å‚æ•°ï¼ˆä»è®ºæ–‡è·å–ï¼‰
      special_param_1: 1.0
      special_param_2: 10

  # æ¨¡å‹é…ç½®
  model:
    name: "[ModelName]"
    params:
      num_classes: 10
      # æ¨¡å‹ç‰¹æœ‰å‚æ•°

  # æ•°æ®é›†é…ç½®
  dataset:
    name: "CIFAR10"
    partition:
      method: "dirichlet"   # iid | dirichlet | pathological
      num_clients: 10
      alpha: 0.5            # Dirichletå‚æ•°
      seed: 42

# è¯„ä¼°é…ç½®
evaluation:
  metrics:
    - name: "Accuracy"
    - name: "[CustomMetricName]"  # å¦‚æœæœ‰è‡ªå®šä¹‰æŒ‡æ ‡
      params:
        # æŒ‡æ ‡å‚æ•°
```

---

### æ­¥éª¤11: ç¼–å†™è®ºæ–‡å¤ç°è„šæœ¬ â­

**é‡è¦**: æ‰€æœ‰å¤ç°è„šæœ¬ç»Ÿä¸€æ”¾åœ¨ `papers/` ç›®å½•ä¸‹ï¼

#### 11.1 åˆ›å»ºè®ºæ–‡ç›®å½•

```bash
papers/
â””â”€â”€ [method name]/
    â”œâ”€â”€ reproduce.py              # å¤ç°è„šæœ¬
    â”œâ”€â”€ configs/                  # å®éªŒé…ç½®ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æ”¾åœ¨configs/distributed/ï¼‰
    â”‚   â”œâ”€â”€ server.yaml
    â”‚   â””â”€â”€ client_*.yaml
    â”œâ”€â”€ README.md                 # è®ºæ–‡ä¿¡æ¯å’Œå¤ç°è¯´æ˜
    â””â”€â”€ results/                  # ç»“æœå­˜å‚¨
        â”œâ”€â”€ results.csv
        â””â”€â”€ figures/

# ç¤ºä¾‹:
papers/fedavg_mcmahan2017/
papers/fedprox_li2020/
papers/moon_li2021/
papers/fedper_arivazhagan2019/
```

#### 11.2 å¤ç°è„šæœ¬æ¨¡æ¿

```python
# papers/[method name]/reproduce.py

"""
è®ºæ–‡å¤ç°: [è®ºæ–‡æ ‡é¢˜]

è®ºæ–‡ä¿¡æ¯:
    æ ‡é¢˜: [å®Œæ•´æ ‡é¢˜]
    ä½œè€…: [ä½œè€…åˆ—è¡¨]
    ä¼šè®®/æœŸåˆŠ: [venue, year]
    é“¾æ¥: [arXiv/DOIé“¾æ¥]

å®éªŒè®¾ç½®:
    æ•°æ®é›†: [åˆ—è¡¨]
    Non-IID: [è®¾ç½®åˆ—è¡¨]
    ç®—æ³•: [ç®—æ³•å]
    å®¢æˆ·ç«¯æ•°: [æ•°é‡]
    é€šä¿¡è½®æ•°: [æ•°é‡]
    é‡å¤æ¬¡æ•°: [æ¬¡æ•°]

é¢„æœŸç»“æœ:
    [ä»è®ºæ–‡Table/Figureä¸­æå–çš„ç»“æœ]

è¿è¡Œå‘½ä»¤:
    python papers/[paper_name]_[author]_[year]/reproduce.py \\
        --dataset CIFAR10 \\
        --noniid dirichlet \\
        --alpha 0.5

æ•°æ®åº“:
    æ‰€æœ‰å®éªŒç»“æœç»Ÿä¸€å†™å…¥: experiments/unified_results.db
    è¡¨ç»“æ„:
        - experiment_name: å®éªŒåç§°
        - algorithm_type: fl | cl | ul
        - algorithm_name: å…·ä½“ç®—æ³•å
        - dataset: æ•°æ®é›†å
        - accuracy: å‡†ç¡®ç‡
        - ...
"""
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import argparse
import pandas as pd
from typing import List, Dict

from examples.smart_batch_runner import SmartBatchRunner, ExperimentConfig


# ==================== è®ºæ–‡é…ç½® ====================

PAPER_INFO = {
    'title': '[è®ºæ–‡æ ‡é¢˜]',
    'authors': '[ä½œè€…]',
    'venue': '[ä¼šè®®/æœŸåˆŠ, å¹´ä»½]',
    'arxiv': '[é“¾æ¥]',
    'algorithm_type': 'fl',  # å‘½åç©ºé—´: fl | cl | ul
    'algorithm_name': '[AlgorithmName]',
}

# å®éªŒè¶…å‚æ•°ï¼ˆä»è®ºæ–‡è·å–ï¼‰
HYPERPARAMETERS = {
    'learning_rate': 0.01,
    'batch_size': 32,
    'local_epochs': 5,
    'communication_rounds': 100,
    'num_clients': 10,
    'sample_ratio': 1.0,

    # ç®—æ³•ç‰¹æœ‰å‚æ•°
    'special_param_1': 1.0,
    'special_param_2': 10,
}

# å®éªŒè®¾ç½®ï¼ˆå¤ç°è®ºæ–‡çš„Table/Figureï¼‰
EXPERIMENTS = {
    'table1': {
        'description': 'å¤ç°è®ºæ–‡Table 1: ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”',
        'datasets': ['MNIST', 'FMNIST', 'CIFAR10', 'CIFAR100'],
        'noniid_settings': [
            {'method': 'iid', 'name': 'IID'},
        ],
    },
    'table2': {
        'description': 'å¤ç°è®ºæ–‡Table 2: ä¸åŒNon-IIDç¨‹åº¦çš„å½±å“',
        'datasets': ['CIFAR10'],
        'noniid_settings': [
            {'method': 'dirichlet', 'alpha': 0.1, 'name': 'Dir(0.1)'},
            {'method': 'dirichlet', 'alpha': 0.5, 'name': 'Dir(0.5)'},
            {'method': 'dirichlet', 'alpha': 1.0, 'name': 'Dir(1.0)'},
        ],
    },
}


# ==================== å®éªŒé…ç½®ç”Ÿæˆ ====================

def create_experiment_configs(
    experiment_name: str = 'table1',
    datasets: List[str] = None,
    noniid_settings: List[Dict] = None,
) -> List[ExperimentConfig]:
    """
    åˆ›å»ºå®éªŒé…ç½®åˆ—è¡¨

    Args:
        experiment_name: å®éªŒåç§°ï¼ˆå¯¹åº”è®ºæ–‡çš„Table/Figureï¼‰
        datasets: æ•°æ®é›†åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨EXPERIMENTSä¸­çš„é…ç½®ï¼‰
        noniid_settings: Non-IIDè®¾ç½®åˆ—è¡¨

    Returns:
        å®éªŒé…ç½®åˆ—è¡¨
    """
    if experiment_name in EXPERIMENTS:
        exp_config = EXPERIMENTS[experiment_name]
        datasets = datasets or exp_config['datasets']
        noniid_settings = noniid_settings or exp_config['noniid_settings']
        print(f"\nå¤ç°å®éªŒ: {exp_config['description']}")

    configs = []
    algo_name = PAPER_INFO['algorithm_name']

    for dataset in datasets:
        for noniid in noniid_settings:
            exp_name = f"{dataset}_{noniid['name']}_{algo_name}"

            # æ„å»ºé…ç½®è¦†ç›–
            config_overrides = {
                # Learneré…ç½®
                'training.learner.name': f"{PAPER_INFO['algorithm_type']}.{algo_name}",  # ä¾‹å¦‚: "fl.FedAvg"
                'training.learner.params.learning_rate': HYPERPARAMETERS['learning_rate'],
                'training.learner.params.batch_size': HYPERPARAMETERS['batch_size'],
                'training.learner.params.local_epochs': HYPERPARAMETERS['local_epochs'],
                'training.learner.params.special_param_1': HYPERPARAMETERS['special_param_1'],
                'training.learner.params.special_param_2': HYPERPARAMETERS['special_param_2'],

                # Dataseté…ç½®
                'training.dataset.name': dataset,
                'training.dataset.partition.method': noniid['method'],
                'training.dataset.partition.num_clients': HYPERPARAMETERS['num_clients'],

                # Serveré…ç½®
                'training.rounds': HYPERPARAMETERS['communication_rounds'],
                'training.sample_ratio': HYPERPARAMETERS['sample_ratio'],
            }

            # æ·»åŠ Non-IIDç‰¹å®šå‚æ•°
            if 'alpha' in noniid:
                config_overrides['training.dataset.partition.alpha'] = noniid['alpha']
            if 'num_classes' in noniid:
                config_overrides['training.dataset.partition.num_classes'] = noniid['num_classes']

            configs.append(ExperimentConfig(
                name=exp_name,
                dataset=dataset,
                algorithm=algo_name,
                algorithm_type=PAPER_INFO['algorithm_type'],  # fl | cl | ul
                noniid_type=noniid['name'],
                config_overrides=config_overrides,
                metadata={
                    'paper': PAPER_INFO['title'],
                    'experiment': experiment_name,
                }
            ))

    return configs


# ==================== ç»“æœåˆ†æ ====================

def analyze_results(results_csv: str, paper_results: Dict = None):
    """
    åˆ†æå®éªŒç»“æœå¹¶ä¸è®ºæ–‡å¯¹æ¯”

    Args:
        results_csv: ç»“æœCSVæ–‡ä»¶è·¯å¾„
        paper_results: è®ºæ–‡ä¸­æŠ¥å‘Šçš„ç»“æœï¼ˆå¯é€‰ï¼‰
    """
    df = pd.read_csv(results_csv)

    print("\n" + "="*80)
    print("å®éªŒç»“æœæ±‡æ€»")
    print("="*80)

    # æŒ‰æ•°æ®é›†å’ŒNon-IIDåˆ†ç»„ç»Ÿè®¡
    summary = df.groupby(['dataset', 'noniid_type']).agg({
        'accuracy': ['mean', 'std', 'min', 'max'],
        'loss': ['mean', 'std'],
    }).round(4)

    print(summary)

    # å¦‚æœæä¾›äº†è®ºæ–‡ç»“æœï¼Œè¿›è¡Œå¯¹æ¯”
    if paper_results:
        print("\n" + "="*80)
        print("ä¸è®ºæ–‡ç»“æœå¯¹æ¯”")
        print("="*80)

        comparison = []
        for (dataset, noniid), paper_acc in paper_results.items():
            our_acc = df[
                (df['dataset'] == dataset) &
                (df['noniid_type'] == noniid)
            ]['accuracy'].mean()

            gap = our_acc - paper_acc

            comparison.append({
                'Dataset': dataset,
                'Non-IID': noniid,
                'Paper': f"{paper_acc:.2%}",
                'Ours': f"{our_acc:.2%}",
                'Gap': f"{gap:+.2%}"
            })

        comparison_df = pd.DataFrame(comparison)
        print(comparison_df.to_string(index=False))

        # åˆ†æå·®è·
        avg_gap = abs(comparison_df['Gap'].str.rstrip('%').astype(float).mean())
        print(f"\nå¹³å‡å·®è·: {avg_gap:.2f}%")

        if avg_gap < 2.0:
            print("âœ… å¤ç°æˆåŠŸï¼ç»“æœä¸è®ºæ–‡åŸºæœ¬ä¸€è‡´ï¼ˆå·®è·<2%ï¼‰")
        elif avg_gap < 5.0:
            print("âš ï¸  ç»“æœå¯æ¥å—ï¼Œä½†å­˜åœ¨ä¸€å®šå·®è·ï¼ˆ2-5%ï¼‰")
        else:
            print("âŒ ç»“æœå·®è·è¾ƒå¤§ï¼ˆ>5%ï¼‰ï¼Œéœ€è¦æ’æŸ¥åŸå› ")


# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(
        description=f"å¤ç°è®ºæ–‡: {PAPER_INFO['title']}"
    )

    # å®éªŒé€‰æ‹©
    parser.add_argument(
        '--experiment',
        type=str,
        default='table1',
        choices=list(EXPERIMENTS.keys()),
        help='é€‰æ‹©è¦å¤ç°çš„å®éªŒï¼ˆå¯¹åº”è®ºæ–‡çš„Table/Figureï¼‰'
    )

    # è¿è¡Œé€‰é¡¹
    parser.add_argument(
        '--mode',
        type=str,
        default='full',
        choices=['test', 'full', 'resume'],
        help='è¿è¡Œæ¨¡å¼: test(æµ‹è¯•å•ä¸ª), full(å®Œæ•´è¿è¡Œ), resume(æ–­ç‚¹ç»­è·‘)'
    )

    parser.add_argument(
        '--repetitions',
        type=int,
        default=3,
        help='æ¯ä¸ªå®éªŒé‡å¤æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰'
    )

    parser.add_argument(
        '--max-concurrent',
        type=int,
        default=5,
        help='æœ€å¤§å¹¶å‘å®éªŒæ•°'
    )

    # æ•°æ®åº“é…ç½®
    parser.add_argument(
        '--db-path',
        type=str,
        default='experiments/unified_results.db',
        help='ç»Ÿä¸€çš„å®éªŒç»“æœæ•°æ®åº“è·¯å¾„'
    )

    # ç»“æœåˆ†æ
    parser.add_argument(
        '--analyze-only',
        action='store_true',
        help='åªåˆ†æå·²æœ‰ç»“æœï¼Œä¸è¿è¡Œå®éªŒ'
    )

    args = parser.parse_args()

    # è®¾ç½®è·¯å¾„
    paper_dir = Path(__file__).parent
    project_root = paper_dir.parent.parent
    db_path = project_root / args.db_path
    results_csv = paper_dir / 'results' / f'{args.experiment}_results.csv'

    # åªåˆ†æç»“æœ
    if args.analyze_only:
        if results_csv.exists():
            analyze_results(str(results_csv))
        else:
            print(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_csv}")
        return

    # åˆ›å»ºå®éªŒé…ç½®
    exp_configs = create_experiment_configs(experiment_name=args.experiment)

    print("\n" + "="*80)
    print(f"è®ºæ–‡: {PAPER_INFO['title']}")
    print(f"ç®—æ³•: {PAPER_INFO['algorithm_type']}.{PAPER_INFO['algorithm_name']}")
    print(f"å®éªŒ: {args.experiment}")
    print("="*80)
    print(f"æ€»å®éªŒé…ç½®æ•°: {len(exp_configs)}")
    print(f"æ¯ä¸ªé…ç½®é‡å¤: {args.repetitions} æ¬¡")
    print(f"æ€»è¿è¡Œæ•°: {len(exp_configs) * args.repetitions}")
    print(f"æ•°æ®åº“: {db_path}")
    print("="*80)

    # æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œç¬¬ä¸€ä¸ªé…ç½®
    if args.mode == 'test':
        print("\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œç¬¬ä¸€ä¸ªé…ç½®")
        exp_configs = exp_configs[:1]
        args.repetitions = 1

    # é…ç½®åŸºç¡€ç›®å½•
    config_base_dir = str(project_root / 'configs' / 'distributed' / 'experiments' /
                         f"{PAPER_INFO['algorithm_name'].lower()}")

    # åˆ›å»ºæ™ºèƒ½è°ƒåº¦å™¨
    runner = SmartBatchRunner(
        config_base_dir=config_base_dir,
        experiments=exp_configs,
        max_repetitions=args.repetitions,
        db_path=str(db_path),
        log_dir=str(paper_dir / 'logs'),
        enable_gpu_scheduling=True,
        max_concurrent_experiments=args.max_concurrent,

        # æ•°æ®é›†ç‰¹å®šå¹¶å‘é™åˆ¶
        dataset_concurrent_limits={
            'MNIST': 10,
            'FMNIST': 10,
            'CIFAR10': 5,
            'CIFAR100': 3,
        }
    )

    # è¿è¡Œå®éªŒ
    print("\nğŸš€ å¼€å§‹è¿è¡Œå®éªŒ...")
    runner.run_multiprocess()

    # å¯¼å‡ºç»“æœ
    print("\nğŸ“Š å¯¼å‡ºç»“æœ...")
    results_csv.parent.mkdir(parents=True, exist_ok=True)
    runner.export_results_to_csv(str(results_csv))

    # åˆ†æç»“æœ
    print("\nğŸ“ˆ åˆ†æç»“æœ...")

    # å¦‚æœæœ‰è®ºæ–‡æŠ¥å‘Šçš„ç»“æœï¼Œè¿›è¡Œå¯¹æ¯”
    # paper_results = {
    #     ('CIFAR10', 'IID'): 0.85,
    #     ('CIFAR10', 'Dir(0.5)'): 0.78,
    #     # ...
    # }
    # analyze_results(str(results_csv), paper_results)

    analyze_results(str(results_csv))

    print(f"\nâœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_csv}")


if __name__ == '__main__':
    main()
```

#### 11.3 README.mdæ¨¡æ¿

```markdown
# [è®ºæ–‡æ ‡é¢˜] - å¤ç°

## è®ºæ–‡ä¿¡æ¯

- **æ ‡é¢˜**: [å®Œæ•´æ ‡é¢˜]
- **ä½œè€…**: [ä½œè€…åˆ—è¡¨]
- **ä¼šè®®/æœŸåˆŠ**: [venue, year]
- **è®ºæ–‡é“¾æ¥**: [arXiv/DOI]
- **å¼€æºä»£ç **: [GitHubé“¾æ¥ï¼ˆå¦‚æœ‰ï¼‰]

## ç®—æ³•ç®€ä»‹

[2-3æ®µæè¿°ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³å’Œåˆ›æ–°ç‚¹]

## å®éªŒè®¾ç½®

### Table 1: [æè¿°]

| æ•°æ®é›† | Non-IID | è®ºæ–‡ç»“æœ | å¤ç°ç»“æœ | å·®è· |
|--------|---------|----------|----------|------|
| MNIST  | IID     | 98.5%    | 98.3%    | -0.2%|
| CIFAR10| Dir(0.5)| 78.2%    | 78.0%    | -0.2%|

### Table 2: [æè¿°]

...

## è¿è¡Œæ–¹æ³•

### å¤ç°æ‰€æœ‰å®éªŒ

```bash
# å¤ç°Table 1
python papers/[paper_name]_[author]_[year]/reproduce.py \
    --experiment table1 \
    --repetitions 3

# å¤ç°Table 2
python papers/[paper_name]_[author]_[year]/reproduce.py \
    --experiment table2 \
    --repetitions 3
```

### æµ‹è¯•å•ä¸ªå®éªŒ

```bash
python papers/[paper_name]_[author]_[year]/reproduce.py \
    --mode test \
    --experiment table1
```

### åªåˆ†æå·²æœ‰ç»“æœ

```bash
python papers/[paper_name]_[author]_[year]/reproduce.py \
    --analyze-only \
    --experiment table1
```

## ç»“æœåˆ†æ

[åˆ†æå¤ç°ç»“æœä¸è®ºæ–‡çš„å¯¹æ¯”]

### æˆåŠŸå¤ç° âœ…

- [åˆ—å‡ºæˆåŠŸå¤ç°çš„å®éªŒ]

### å­˜åœ¨å·®è· âš ï¸

- [åˆ—å‡ºæœ‰å·®è·çš„å®éªŒ]
- å¯èƒ½åŸå› : [åˆ†æ]

## ä¾èµ–å’Œç¯å¢ƒ

```bash
# Pythonç‰ˆæœ¬
python >= 3.8

# ä¾èµ–åŒ…
torch >= 1.10
torchvision
numpy
pandas
pyyaml
```

## æ–‡ä»¶ç»“æ„

```
papers/[paper_name]_[author]_[year]/
â”œâ”€â”€ reproduce.py          # å¤ç°è„šæœ¬
â”œâ”€â”€ README.md             # æœ¬æ–‡ä»¶
â”œâ”€â”€ configs/              # å®éªŒé…ç½®ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ logs/                 # è¿è¡Œæ—¥å¿—
â””â”€â”€ results/              # ç»“æœæ–‡ä»¶
    â”œâ”€â”€ table1_results.csv
    â”œâ”€â”€ table2_results.csv
    â””â”€â”€ figures/
```

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬å¤ç°ä»£ç ï¼Œè¯·å¼•ç”¨åŸè®ºæ–‡:

```bibtex
@inproceedings{...}
```
```

---

## ç»Ÿä¸€æ•°æ®åº“ç®¡ç† â­

### æ•°æ®åº“è®¾è®¡

æ‰€æœ‰ç®—æ³•ç±»å‹ï¼ˆfl.xxx, cl.xxx, ul.xxxï¼‰çš„å®éªŒç»“æœç»Ÿä¸€å­˜å‚¨åœ¨ä¸€ä¸ªæ•°æ®åº“ä¸­ï¼š

```sql
-- æ•°æ®åº“: experiments/unified_results.db

-- å®éªŒé…ç½®è¡¨
CREATE TABLE experiments (
    config_hash TEXT PRIMARY KEY,
    exp_name TEXT NOT NULL,
    algorithm_type TEXT NOT NULL,  -- 'fl' | 'cl' | 'ul'
    algorithm_name TEXT NOT NULL,   -- å…·ä½“ç®—æ³•å
    dataset TEXT NOT NULL,
    noniid_type TEXT,
    config_json TEXT,
    paper_name TEXT,                -- æ¥è‡ªå“ªç¯‡è®ºæ–‡
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å®éªŒè¿è¡Œè¡¨
CREATE TABLE experiment_runs (
    run_id INTEGER PRIMARY KEY AUTOINCREMENT,
    config_hash TEXT NOT NULL,
    run_number INTEGER NOT NULL,
    status TEXT NOT NULL,  -- 'pending' | 'running' | 'success' | 'failed'

    -- ç»“æœæŒ‡æ ‡
    accuracy REAL,
    loss REAL,
    rounds INTEGER,
    duration_sec REAL,

    -- ç®—æ³•ç‰¹å®šæŒ‡æ ‡
    forgetting_rate REAL,      -- é—å¿˜å­¦ä¹ ä¸“ç”¨
    fairness_score REAL,       -- å…¬å¹³æ€§æŒ‡æ ‡
    custom_metric_1 REAL,
    custom_metric_2 REAL,

    -- å…ƒä¿¡æ¯
    error_msg TEXT,
    log_file TEXT,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,

    FOREIGN KEY (config_hash) REFERENCES experiments(config_hash),
    UNIQUE(config_hash, run_number)
);

-- ç´¢å¼•
CREATE INDEX idx_algorithm_type ON experiments(algorithm_type);
CREATE INDEX idx_algorithm_name ON experiments(algorithm_name);
CREATE INDEX idx_dataset ON experiments(dataset);
CREATE INDEX idx_paper ON experiments(paper_name);
CREATE INDEX idx_status ON experiment_runs(status);
```

### æŸ¥è¯¢ç¤ºä¾‹

```python
import sqlite3
import pandas as pd

# è¿æ¥æ•°æ®åº“
conn = sqlite3.connect('experiments/unified_results.db')

# æŸ¥è¯¢æ‰€æœ‰è”é‚¦å­¦ä¹ ç®—æ³•çš„ç»“æœ
df = pd.read_sql_query("""
    SELECT
        e.algorithm_name,
        e.dataset,
        e.noniid_type,
        AVG(r.accuracy) as avg_accuracy,
        AVG(r.loss) as avg_loss
    FROM experiments e
    JOIN experiment_runs r ON e.config_hash = r.config_hash
    WHERE e.algorithm_type = 'fl' AND r.status = 'success'
    GROUP BY e.algorithm_name, e.dataset, e.noniid_type
""", conn)

print(df)

# æŸ¥è¯¢ç‰¹å®šè®ºæ–‡çš„æ‰€æœ‰å®éªŒ
df = pd.read_sql_query("""
    SELECT * FROM experiments e
    JOIN experiment_runs r ON e.config_hash = r.config_hash
    WHERE e.paper_name = 'fedavg_mcmahan2017'
    AND r.status = 'success'
""", conn)

# å¯¹æ¯”ä¸åŒç®—æ³•ç±»å‹
df = pd.read_sql_query("""
    SELECT
        algorithm_type,
        algorithm_name,
        AVG(accuracy) as avg_accuracy
    FROM experiments e
    JOIN experiment_runs r ON e.config_hash = r.config_hash
    WHERE dataset = 'CIFAR10' AND status = 'success'
    GROUP BY algorithm_type, algorithm_name
    ORDER BY avg_accuracy DESC
""", conn)
```

---

## æ£€æŸ¥æ¸…å• âœ…

åœ¨æäº¤é›†æˆå‰ï¼Œç¡®è®¤ä»¥ä¸‹æ‰€æœ‰é¡¹ï¼š

### ä»£ç å®ç°
- [ ] Learnerå®ç°å®Œæ•´ï¼Œæ ¸å¿ƒç®—æ³•é€»è¾‘æ­£ç¡®
- [ ] æ‰€æœ‰éœ€è¦çš„ç»„ä»¶éƒ½å·²å®ç°ï¼ˆTrainerã€Modelã€Aggregatorã€Datasetã€Metricï¼‰
- [ ] è£…é¥°å™¨æ­£ç¡®ä½¿ç”¨ï¼Œæ‰€æœ‰ç»„ä»¶å·²æ³¨å†Œ
- [ ] é€šä¿¡åè®®æ­£ç¡®å®ç°ï¼ˆget_local_model, set_local_model, get_model, set_modelï¼‰
- [ ] ä»£ç æœ‰å……åˆ†çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œæ³¨é‡Š
- [ ] å˜é‡å‘½åæ¸…æ™°ï¼Œç¬¦åˆPythonè§„èŒƒ

### é…ç½®æ–‡ä»¶
- [ ] å®éªŒé…ç½®ä¸è®ºæ–‡å®Œå…¨ä¸€è‡´
- [ ] æ‰€æœ‰è¶…å‚æ•°å¯ä»¥é€šè¿‡YAMLé…ç½®
- [ ] é…ç½®æ–‡ä»¶ç»“æ„æ¸…æ™°ï¼Œæœ‰æ³¨é‡Šè¯´æ˜

### å¤ç°è„šæœ¬
- [ ] å¤ç°è„šæœ¬æ”¾åœ¨ `papers/[paper_name]_[author]_[year]/`
- [ ] README.mdè¯¦ç»†è®°å½•è®ºæ–‡ä¿¡æ¯å’Œè¿è¡Œæ–¹æ³•
- [ ] è„šæœ¬æ”¯æŒä¸åŒè¿è¡Œæ¨¡å¼ï¼ˆtestã€fullã€resumeï¼‰
- [ ] ç»“æœè‡ªåŠ¨åˆ†æå’Œå¯¹æ¯”

### æµ‹è¯•éªŒè¯
- [ ] è‡³å°‘åœ¨1ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•é€šè¿‡
- [ ] ç»“æœä¸è®ºæ–‡å·®è·åœ¨åˆç†èŒƒå›´ï¼ˆÂ±2%ï¼‰
- [ ] ä»£ç åœ¨ä¸åŒç¯å¢ƒä¸‹å¯è¿è¡Œ
- [ ] æ— å†…å­˜æ³„æ¼æˆ–GPUæ˜¾å­˜æº¢å‡º

### æ•°æ®åº“
- [ ] å®éªŒç»“æœæ­£ç¡®å†™å…¥ç»Ÿä¸€æ•°æ®åº“
- [ ] algorithm_typeå­—æ®µæ­£ç¡®ï¼ˆfl/cl/ulï¼‰
- [ ] å¯ä»¥é€šè¿‡SQLæŸ¥è¯¢å’Œåˆ†æç»“æœ

### æ–‡æ¡£
- [ ] README.mdå®Œæ•´
- [ ] æ‰€æœ‰ä»£ç æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] ç‰¹æ®Šè®¾è®¡æœ‰è¯´æ˜æ³¨é‡Š

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: å¦‚ä½•é€‰æ‹©æ­£ç¡®çš„åŸºç±»ï¼Ÿ

**A**: æ‰€æœ‰Learneréƒ½ç»§æ‰¿è‡ª `BaseLearner`ï¼š

```python
from fedcl.learner.base_learner import BaseLearner
from fedcl.methods.learners._decorators import learner

# è”é‚¦å­¦ä¹ ç®—æ³•
@learner(namespace='fl', name='MyFLAlgo')
class MyFLLearner(BaseLearner):
    pass

# æŒç»­å­¦ä¹ ç®—æ³•
@learner(namespace='cl', name='MyCLAlgo')
class MyCLLearner(BaseLearner):
    pass

# é—å¿˜å­¦ä¹ ç®—æ³•
@learner(namespace='ul', name='MyULAlgo')
class MyULLearner(BaseLearner):
    pass
```

**æ ¸å¿ƒæ–¹æ³•éœ€è¦å®ç°**:
- `async def train()`: æœ¬åœ°è®­ç»ƒé€»è¾‘ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰
- `async def evaluate()`: è¯„ä¼°é€»è¾‘ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰
- `async def get_local_model()`: è¿”å›éœ€è¦ä¸Šä¼ çš„æ¨¡å‹å‚æ•°ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰
- `async def set_local_model()`: æ¥æ”¶æœåŠ¡å™¨ä¸‹å‘çš„æ¨¡å‹å‚æ•°ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰

**æ³¨æ„**ï¼šæ¡†æ¶ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹ï¼ˆasync/awaitï¼‰ï¼Œæ‰€æœ‰æ ¸å¿ƒæ–¹æ³•éƒ½æ˜¯å¼‚æ­¥çš„ã€‚

### Q2: è£…é¥°å™¨å‚æ•°æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ

**A**: Learnerè£…é¥°å™¨é‡‡ç”¨ç®€æ´çš„ä¸‰å‚æ•°è®¾è®¡ï¼š

```python
from fedcl.methods.learners._decorators import learner

@learner(
    namespace='fl',              # å‘½åç©ºé—´: fl(è”é‚¦å­¦ä¹ ), cl(æŒç»­å­¦ä¹ ), ul(é—å¿˜å­¦ä¹ )
    name='FedAvg',              # æ–¹æ³•åï¼Œç”¨äºé…ç½®æ–‡ä»¶å¼•ç”¨
    description='FedAvg: ...'   # å¯é€‰æè¿°ï¼Œç”¨äºæ–‡æ¡£
)
class FedAvgLearner(BaseLearner):
    pass
```

**æ³¨å†Œç»“æœ**:
- `namespace='fl', name='FedAvg'` â†’ æ³¨å†Œä¸º `'fl.FedAvg'`
- `namespace='cl', name='TARGET'` â†’ æ³¨å†Œä¸º `'cl.TARGET'`
- `namespace='ul', name='SISA'` â†’ æ³¨å†Œä¸º `'ul.SISA'`

**åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨**:
```yaml
training:
  learner:
    name: "fl.FedAvg"  # æˆ– "cl.TARGET", "ul.SISA"
```

### Q3: å¦‚ä½•å¤„ç†ç®—æ³•éœ€è¦çš„ç‰¹æ®Šæ¨¡å‹ç»“æ„ï¼Ÿ

**A**: åˆ›å»ºè‡ªå®šä¹‰Modelå¹¶ç”¨è£…é¥°å™¨æ³¨å†Œï¼š

```python
@model(name='MySpecialNet')
class MySpecialNet(nn.Module):
    def __init__(self, ...):
        # å®šä¹‰ç‰¹æ®Šç»“æ„
        pass
```

ç„¶ååœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šï¼š

```yaml
training:
  model:
    name: "MySpecialNet"
```

### Q4: é€šä¿¡åè®®ä¸­ä¸Šä¼ /ä¸‹è½½ä»€ä¹ˆï¼Ÿ

**A**: BaseLearnerå®šä¹‰äº†ä¸¤ä¸ªæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»éœ€è¦å®ç°ï¼š

```python
# æŠ½è±¡æ–¹æ³•ï¼ˆBaseLearnerä¸­å®šä¹‰ï¼‰
async def get_local_model(self) -> Dict[str, Any]:
    """è¿”å›è¦ä¸Šä¼ åˆ°æœåŠ¡å™¨çš„æ¨¡å‹æ•°æ®"""
    pass

async def set_local_model(self, model_data: Dict[str, Any]) -> bool:
    """æ¥æ”¶æœåŠ¡å™¨ä¸‹å‘çš„æ¨¡å‹æ•°æ®"""
    pass
```

**å®é™…å®ç°æ¨¡å¼**ï¼ˆæ¨èï¼‰ï¼š
```python
class MyLearner(BaseLearner):
    # å®ç°å…·ä½“é€»è¾‘
    async def get_model(self) -> Dict[str, Any]:
        weights = {...}  # è·å–éœ€è¦ä¸Šä¼ çš„å‚æ•°
        return {
            'model_type': 'MyModel',
            'parameters': {'weights': weights},
            'metadata': {'client_id': self.client_id, 'samples': 1000}
        }

    async def set_model(self, model_data: Dict[str, Any]) -> bool:
        # æ›´æ–°æœ¬åœ°æ¨¡å‹
        weights = model_data['parameters']['weights']
        # ...
        return True

    # å§”æ‰˜ç»™å…·ä½“å®ç°
    async def get_local_model(self):
        return await self.get_model()

    async def set_local_model(self, model_data):
        return await self.set_model(model_data)
```

**ä¸åŒç®—æ³•çš„ä¸Šä¼ ç­–ç•¥**ï¼š
```python
# æ ‡å‡†FL: ä¸Šä¼ å…¨éƒ¨å‚æ•°
async def get_model(self):
    weights = {name: param for name, param in self.model.named_parameters()}
    return {'parameters': {'weights': weights}, ...}

# ä¸ªæ€§åŒ–FL (FedPer): åªä¸Šä¼ å…±äº«å±‚
async def get_model(self):
    weights = {name: param for name, param in self.model.named_parameters()
               if 'base' in name}
    return {'parameters': {'weights': weights}, ...}

# åŸå‹FL (FedProto): åœ¨metadataä¸­ä¸Šä¼ ç±»åŸå‹
async def get_model(self):
    return {
        'parameters': {'weights': {}},
        'metadata': {'prototypes': self.compute_prototypes()}
    }
```

### Q5: å¦‚ä½•è°ƒè¯•å®éªŒï¼Ÿ

**A**: ä½¿ç”¨æµ‹è¯•æ¨¡å¼ï¼š

```bash
# åªè¿è¡Œä¸€ä¸ªé…ç½®ï¼Œå¿«é€Ÿæµ‹è¯•
python papers/xxx/reproduce.py --mode test --experiment table1

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
tail -f papers/xxx/logs/experiment.log

# æŸ¥çœ‹æ•°æ®åº“
sqlite3 experiments/unified_results.db "SELECT * FROM experiment_runs ORDER BY run_id DESC LIMIT 10"
```

### Q6: ç»“æœä¸è®ºæ–‡å·®è·å¤§æ€ä¹ˆåŠï¼Ÿ

**A**: ç³»ç»ŸåŒ–æ’æŸ¥ï¼š

1. **æ£€æŸ¥è¶…å‚æ•°**: ç¡®è®¤æ‰€æœ‰å‚æ•°ä¸è®ºæ–‡ä¸€è‡´
2. **æ£€æŸ¥æ•°æ®å¤„ç†**: å½’ä¸€åŒ–ã€æ•°æ®å¢å¼º
3. **æ£€æŸ¥éšæœºç§å­**: å›ºå®šç§å­å¢åŠ å¯å¤ç°æ€§
4. **æ£€æŸ¥ä¼˜åŒ–å™¨**: SGD vs Adamï¼Œå­¦ä¹ ç‡è°ƒåº¦
5. **æ£€æŸ¥è¯„ä¼°æ–¹å¼**: æµ‹è¯•é›†ã€è¯„ä¼°æŒ‡æ ‡
6. **æŸ¥çœ‹æ—¥å¿—**: åˆ†æè®­ç»ƒæ›²çº¿ï¼ŒæŸ¥æ‰¾å¼‚å¸¸
7. **å‚è€ƒå¼€æºä»£ç **: å¦‚æœæœ‰å®˜æ–¹å®ç°ï¼Œå¯¹æ¯”ç»†èŠ‚

### Q7: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼Ÿ

**A**: åˆ›å»ºMetricå¹¶æ³¨å†Œï¼š

```python
@metric(name='MyMetric')
class MyMetric:
    def compute(self, predictions, targets, **kwargs):
        # è®¡ç®—æŒ‡æ ‡
        return {'my_metric': score}
```

åœ¨Learnerä¸­ä½¿ç”¨ï¼š

```python
def local_test(self, test_loader, **kwargs):
    # ... æµ‹è¯•é€»è¾‘

    # ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡
    from fedcl.methods.metrics.my_metric import MyMetric
    metric = MyMetric()
    custom_scores = metric.compute(all_preds, all_targets)

    return {
        'accuracy': accuracy,
        **custom_scores
    }
```

---

## è¿›é˜¶æŠ€å·§

### 1. æ”¯æŒå¤šGPUè®­ç»ƒ

```python
class MyLearner(FedAvgLearner):
    def __init__(self, model, device, **kwargs):
        super().__init__(model, device, **kwargs)

        # è‡ªåŠ¨ä½¿ç”¨DataParallel
        if torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
```

### 2. åŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦

```python
class MyLearner(FedAvgLearner):
    def __init__(self, model, device, **kwargs):
        super().__init__(model, device, **kwargs)

        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100
        )

    def local_train(self, train_loader, current_round, **kwargs):
        results = super().local_train(train_loader, **kwargs)

        # æ¯è½®è°ƒæ•´å­¦ä¹ ç‡
        self.scheduler.step()

        return results
```

### 3. æ—©åœæœºåˆ¶

```python
class MyLearner(FedAvgLearner):
    def __init__(self, model, device, patience=10, **kwargs):
        super().__init__(model, device, **kwargs)
        self.patience = patience
        self.best_loss = float('inf')
        self.patience_counter = 0

    def local_train(self, train_loader, **kwargs):
        results = super().local_train(train_loader, **kwargs)

        # æ—©åœæ£€æŸ¥
        if results['loss'] < self.best_loss:
            self.best_loss = results['loss']
            self.patience_counter = 0
        else:
            self.patience_counter += 1

        if self.patience_counter >= self.patience:
            results['early_stopped'] = True

        return results
```

---

## å‚è€ƒèµ„æº

### æ¡†æ¶æ–‡æ¡£
- `README.md` - æ¡†æ¶æ€»è§ˆ
- `ARCHITECTURE.md` - æ¶æ„è®¾è®¡
- `fedcl/api/decorators.py` - è£…é¥°å™¨æºç å’Œæ–‡æ¡£

### ç¤ºä¾‹ä»£ç 
- `fedcl/methods/learners/fl/fedper.py` - ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ç¤ºä¾‹
  ```python
  @learner('fl', 'FedPer', description='FedPer: Federated Learning with Personalization Layers')
  class FedPerLearner(BaseLearner):
      pass
  ```
- `fedcl/methods/learners/fl/moon.py` - å¯¹æ¯”å­¦ä¹ ç¤ºä¾‹
  ```python
  @learner('fl', 'MOON', description='MOON: Model-Contrastive Federated Learning')
  class MOONLearner(BaseLearner):
      pass
  ```
- `fedcl/methods/learners/cl/target.py` - æŒç»­å­¦ä¹ ç¤ºä¾‹
  ```python
  @learner('cl', 'TARGET', description='TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation (ICCV 2023)')
  class TARGETLearner(BaseLearner):
      pass
  ```

### å¤–éƒ¨èµ„æº
- [FedML](https://github.com/FedML-AI/FedML) - å‚è€ƒå®ç°
- [Flower](https://github.com/adap/flower) - å¦ä¸€ä¸ªFLæ¡†æ¶
- [PFL-Non-IID](https://github.com/TsingZ0/PFL-Non-IID) - ä¸ªæ€§åŒ–FLç®—æ³•é›†åˆ

---

## æ€»ç»“

é›†æˆæ–°ç®—æ³•åˆ°æœ¬æ¡†æ¶éœ€è¦ï¼š

1. âœ… **ç†è§£è£…é¥°å™¨ç³»ç»Ÿ** - æ‰€æœ‰ç»„ä»¶é€šè¿‡è£…é¥°å™¨æ³¨å†Œ
2. âœ… **å®ç°å®Œæ•´ç»„ä»¶** - Learner + Trainer + Model + Aggregator + Dataset + Metric
3. âœ… **è®¾è®¡é€šä¿¡åè®®** - æ˜ç¡®ä¸Šä¼ /ä¸‹è½½å†…å®¹
4. âœ… **ç¼–å†™å¤ç°è„šæœ¬** - æ”¾åœ¨ `papers/` ç›®å½•
5. âœ… **ç»Ÿä¸€æ•°æ®åº“ç®¡ç†** - æ‰€æœ‰ç»“æœå†™å…¥ `experiments/unified_results.db`
6. âœ… **å……åˆ†æµ‹è¯•éªŒè¯** - ç¡®ä¿ç»“æœæ­£ç¡®

**è®°ä½**: æ¡†æ¶çš„æ ¸å¿ƒæ˜¯**è£…é¥°å™¨é©±åŠ¨çš„ç»„ä»¶æ³¨å†Œ**å’Œ**ç»Ÿä¸€çš„å®éªŒç®¡ç†**ã€‚æ‰€æœ‰ç®—æ³•ï¼ˆfl/cl/ulï¼‰å…±äº«ç›¸åŒçš„åŸºç¡€è®¾æ–½ï¼Œä½†å¯ä»¥æœ‰å„è‡ªçš„ç‰¹æ®Šå®ç°ã€‚

ç¥ä½ æˆåŠŸå¤ç°è®ºæ–‡ç»“æœï¼ğŸš€
