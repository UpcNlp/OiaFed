# =============================================================================
# FedDistill Algorithm Preset
# =============================================================================
# Paper: Federated Knowledge Distillation
#
# Description:
#   FedDistill applies knowledge distillation in federated learning by having
#   clients train their local models while distilling knowledge from the global
#   model. The distillation loss helps transfer soft labels and feature
#   representations from the global model to local models.
#
# Key Features:
#   - Knowledge distillation from global model to local models
#   - Soft label transfer using temperature scaling
#   - Helps preserve global knowledge during local training
#   - Reduces catastrophic forgetting
#
# Components:
#   - Trainer: default
#   - Learner: fl.feddistill (distillation-based training)
#   - Aggregator: fedavg
#
# Hyperparameters:
#   - distill_temperature: Temperature for softmax in distillation (default: 2.0)
#   - distill_weight: Weight of distillation loss (default: 0.5)
#
# Usage:
#   extend: presets/algorithms/feddistill.yaml
#
# =============================================================================

extend: ../common/base.yaml

# Trainer configuration
trainer:
  type: default
  args:
    max_rounds: 100
    local_epochs: 5
    client_fraction: 0.1
    min_available_clients: 2

    fit_config:
      epochs: 5
      evaluate_after_fit: true

    eval_interval: 5
    evaluate_after_aggregation: false

# Aggregator configuration
aggregator:
  type: fedavg
  args:
    weighted: true

# Learner configuration (FedDistill with knowledge distillation)
learner:
  type: fl.feddistill
  args:
    learning_rate: 0.01
    batch_size: 32
    optimizer: SGD
    momentum: 0.9

    # FedDistill-specific configuration
    distill_temperature: 2.0  # Temperature scaling for soft labels
    distill_weight: 0.5       # Weight balancing between CE and distillation loss
